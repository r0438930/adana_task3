{"title_page": "Distributional semantics", "text_new": "'''Distributional semantics''' is a research area that develops and studies theories and methods for quantifying and categorizing semantic similarities between linguistic items based on their distributional properties in large samples of language data. The basic idea of distributional semantics can be summed up in the so-called [[Distributionalism|Distributional]] hypothesis: ''linguistic items with similar distributions have similar meanings.''\n\n==Distributional hypothesis==\nThe '''distributional hypothesis''' in [[linguistics]] is derived from the [[semantic theory]] of language usage, i.e. words that are used and occur in the same [[context (language use)|context]]s tend to purport similar meanings.<ref>{{harvnb|Harris|1954}}</ref> \n\nThe underlying idea that \"a word is characterized by the company it keeps\" was popularized by [[J. R. Firth|Firth]] in the 1950s.<ref>{{harvnb|Firth|1957}}</ref>\n\nThe distributional hypothesis is the basis for [[statistical semantics]]. Although the Distributional Hypothesis originated in linguistics,<ref>{{harvnb|Sahlgren|2008}}</ref> it is now receiving attention in [[cognitive science]] especially regarding the context of word use.<ref>{{harvnb|McDonald|Ramscar|2001}}</ref>\n\nIn recent years, the distributional hypothesis has provided the basis for the theory of [[similarity-based generalization]] in language learning: the idea that children can figure out how to use words they've rarely encountered before by generalizing about their use from distributions of similar words.<ref>{{harvnb| Gleitman|2002}}</ref><ref>{{harvnb| Yarlett|2008}}</ref>\n\nThe distributional hypothesis suggests that the more semantically similar two words are, the more distributionally similar they will be in turn, and thus the more that they will tend to occur in similar linguistic contexts.\n\nWhether or not this suggestion holds has significant implications for both the [[data-sparsity]] problem in computational modeling,<ref>Wishart, Ryder, and Prokopis Prokopidis. \u201cTopic Modelling Experiments on Hellenistic Corpora.\u201d In ''Proceedings of the Workshop on Corpora in the Digital Humanities 17'', 39\u201347. Bloomington, IN: CEUR Workshop Proceedings, 2017, Online: https://pdfs.semanticscholar.org/bd71/ab40960e481006117bafd0ae952d3e8d1f66.pdf.</ref> and for the question of how children are able to learn language so rapidly given relatively impoverished input (this is also known as the problem of the [[poverty of the stimulus]]).\n\n==Distributional semantic modeling in vector spaces==\nDistributional semantics favor the use of linear algebra as computational tool and representational framework. The basic approach is to collect distributional information in high-dimensional vectors, and to define distributional/semantic similarity in terms of vector similarity.<ref>{{harvnb|Rieger |1991 }}</ref> Different kinds of similarities can be extracted depending on which type of distributional information is used to collect the vectors: '''topical''' similarities can be extracted by populating the vectors with information on which text regions the linguistic items occur in; '''paradigmatic''' similarities can be extracted by populating the vectors with information on which other linguistic items the items co-occur with. Note that the latter type of vectors can also be used to extract '''syntagmatic''' similarities by looking at the individual vector components.\n\nThe basic idea of a correlation between distributional and semantic similarity can be operationalized in many different ways. There is a rich variety of computational models implementing distributional semantics, including [[latent semantic analysis]] (LSA),<ref>{{harvnb|Deerwester|Dumais|Furnas|Landauer|1990}}</ref><ref>{{Cite journal|last=Landauer|first=Thomas K.|last2=Dumais|first2=Susan T.|date=1997|title=A solution to Plato's problem: The latent semantic analysis theory of acquisition, induction, and representation of knowledge.|journal=Psychological Review|volume=104|issue=2|pages=211\u2013240|doi=10.1037/0033-295x.104.2.211|issn=1939-1471}}</ref> [[Hyperspace Analogue to Language]] (HAL), syntax- or dependency-based models,<ref>{{harvnb|Pad\u00f3|Lapata|2007}}</ref> [[random indexing]], [[semantic folding]]<ref>{{cite arXiv|eprint=1511.08855|title=Semantic Folding Theory And its Application in Semantic Fingerprinting|last=De Sousa Webber|first=Francisco|date=2015|publisher=|access-date=|class=cs.AI}}</ref> and various variants of the [[topic model]].<ref>{{Cite journal|last=Jordan|first=Michael I.|last2=Ng|first2=Andrew Y.|last3=Blei|first3=David M.|date=2003|title=Latent Dirichlet Allocation|url=http://www.jmlr.org/papers/v3/blei03a.html|journal=Journal of Machine Learning Research|volume=3|issue=Jan|pages=993\u20131022|issn=1533-7928}}</ref>\n\nDistributional semantic models differ primarily with respect to the following parameters:\n\n* Context type (text regions vs. linguistic items)\n* Context window (size, extension, etc.)\n* Frequency weighting (e.g. [[entropy (information theory)|entropy]], [[pointwise mutual information]],<ref>{{Cite journal|last=Church|first=Kenneth Ward|last2=Hanks|first2=Patrick|date=1989|title=Word association norms, mutual information, and lexicography|journal=Proceedings of the 27th Annual Meeting on Association for Computational Linguistics -|pages=76\u201383|location=Morristown, NJ, USA|publisher=Association for Computational Linguistics|doi=10.3115/981623.981633}}</ref> etc.)\n* Dimension reduction (e.g. [[random indexing]], [[singular value decomposition]], etc.)\n* [[Similarity measure]] (e.g. [[cosine similarity]], [[Minkowski distance]], etc.)\n\nDistributional semantic models that use linguistic items as context have also been referred to as '''word space, or vector space models'''.<ref>{{harvnb|Sch\u00fctze|1993}}</ref><ref>{{harvnb|Sahlgren|2006}}</ref>\n\n==Beyond Lexical Semantics==\nWhile distributional semantics typically has been applied to lexical items -- words and multi-word terms -- with considerable success, not least due to its applicability as an input layer for neurally inspired deep learning models, lexical semantics, i.e. the meaning of words, will only carry part of the semantics of an entire utterance. The meaning of a clause, e.g. ''\"Tigers love rabbits.\"'', can only partially be understood from examining the meaning of the three lexical items it consists of. Distributional semantics can straightforwardly be extended to cover larger linguistic item such as constructions, with and without non-instantiated items, but some of the base assumptions of the model need to be adjusted somewhat. [[Construction grammar]] and its formulation of the lexical-syntactic continuum offers one approach for including more elaborate constructions in a distributional semantic model and some experiments have been implemented using the Random Indexing approach <ref>{{cite journal |last1=Karlgren|first1=Jussi|author-link1=Jussi Karlgren|last2=Kanerva|first2=Pentti|date=2019|title=High-dimensional distributed semantic spaces for utterances|url=https://www.cambridge.org/core/journals/natural-language-engineering/article/highdimensional-distributed-semantic-spaces-for-utterances/38A75548413F9A430D1B2B35CE34CE43 |journal= Natural Language Engineering|volume=25 |issue=4 |pages=503-517 |doi=10.1017/S1351324919000226 |access-date=2020-04-13}}</ref>. \n\n[[Compositional distributional semantics|Compositional distributional semantic]] models extend distributional semantic models by explicit semantic functions that use syntactically based rules to combine the semantics of participating lexical units into a ''compositional model'' to characterize the semantics of entire phrases or sentences. Different approaches to composition have been explored -- including neural models -- and are under discussion at established workshops such as [[SemEval]].<ref>{{cite web |url=http://alt.qcri.org/semeval2014/task1/|title=SemEval-2014, Task 1}}</ref>\n\n==Applications==\n\nDistributional semantic models have been applied successfully to the following tasks:\n* finding [[semantic similarity]] between words and multi-word expressions;\n* [[Keyword clustering|word clustering]] based on semantic similarity;\n* automatic creation of [[thesauri]] and bilingual dictionaries;\n* lexical ambiguity resolution;\n* expanding search requests using synonyms and associations;\n* defining the topic of a document;\n* [[document clustering]] for [[information retrieval]];\n* [[data mining]] and [[named entities recognition]];\n* creating [[semantic mapping (literacy)|semantic maps]] of different subject domains;\n* [[Paraphrasing (computational linguistics)|paraphrasing]];\n* [[sentiment analysis]];\n* modeling selectional preferences of words.\n\n==Software==\n* [https://github.com/fozziethebeat/S-Space/ S-Space]\n* [https://github.com/semanticvectors/semanticvectors/ SemanticVectors]\n* [http://radimrehurek.com/gensim/index.html Gensim]\n* [http://www.linguatools.de/disco/disco-builder.html DISCO Builder]\n* [https://github.com/Lambda-3/Indra Indra]\n\n==See also==\n*[[Conceptual space]]\n*[[Co-occurrence]]\n*[[Gensim]]\n*[[Phraseme]]\n*[[Random indexing]]\n*[[Sentence embedding]]\n*[[Statistical semantics]]\n*[[Word2vec]]\n*[[Word embedding]]\n\n===People===\n*[[Scott Deerwester]]\n*[[Susan Dumais]]\n*[[J. R. Firth]]\n*[[George Furnas]]\n*[[Zellig Harris]]\n*[[Richard Hirschman]]\n*[[Thomas Landauer]]\n*[[Magnus Sahlgren]]\n*[[Hinrich Sch\u00fctze]]\n\n==References==\n\n{{reflist}}\n\n===Sources===\n\n* {{cite journal | last = Harris | first = Z. | year = 1954 | title = Distributional structure | journal = Word | volume = 10 | issue = 23| pages = 146\u2013162 | ref = harv | doi = 10.1080/00437956.1954.11659520 }}\n* {{cite journal | last = Firth| first = J.R. | year = 1957 | title = A synopsis of linguistic theory 1930-1955 | journal = Studies in Linguistic Analysis | pages = 1\u201332 | ref = harv }} Reprinted in {{cite book | editor = F.R. Palmer | title = Selected Papers of J.R. Firth 1952-1959 | publisher = London: Longman | year = 1968}}\n* {{cite journal | last = Sahlgren | first = Magnus | year = 2008 | title = The Distributional Hypothesis | url = http://soda.swedish-ict.se/3941/1/sahlgren.distr-hypo.pdf | journal = Rivista di Linguistica | volume = 20 | issue = 1 | pages = 33\u201353 | ref = harv }}\n* {{cite conference | last1 = McDonald | first1 = S. | last2 = Ramscar | first2 = M. | year = 2001 | citeseerx = 10.1.1.104.7535 | title = Testing the distributional hypothesis: The influence of context on judgements of semantic similarity | booktitle = Proceedings of the 23rd Annual Conference of the Cognitive Science Society | pages = 611\u2013616 | ref = harv }}\n* {{cite book | last = Gleitman | first = Lila R. | year = 2002 | ref = harv  | volume = 1| pages = 209\u2013229| doi=10.1075/cilt.228.17gle| series = Current Issues in Linguistic Theory | isbn = 978-90-272-4736-0 | chapter = Verbs of a feather flock together II | title = The Legacy of Zellig Harris }} \n* {{cite thesis | last = Yarlett | first = D. | year = 2008 | title = Language Learning Through Similarity-Based Generalization | url = http://psych.stanford.edu/~michael/papers/Draft_Yarlett_Similarity.pdf | degree = PhD | publisher = Stanford University | ref = harv | accessdate = 2012-07-12 | archiveurl = https://web.archive.org/web/20140419012951/http://psych.stanford.edu/~michael/papers/Draft_Yarlett_Similarity.pdf | archivedate = 2014-04-19 | url-status = dead }}\n* {{cite report | last = Rieger |first= Burghard B. | year= 1991 | citeseerx = 10.1.1.37.7976 | title  =  On Distributed Representations in Word Semantics | url  =  http://ftp.icsi.berkeley.edu/ftp/pub/techreports/1991/tr-91-012.pdf | publisher = ICSI Berkeley 12-1991 | ref = harv}}\n* {{cite journal | last1 = Deerwester | first1 = Scott | last2 = Dumais | first2 = Susan T. | last3 = Furnas | first3 = George W. | last4 = Landauer | first4 = Thomas K. | last5 = Harshman | first5 = Richard | url = http://lsi.research.telcordia.com/lsi/papers/JASIS90.pdf | title = Indexing by Latent Semantic Analysis | journal = Journal of the American Society for Information Science | volume = 41 | issue = 6 | pages = 391\u2013407 | year = 1990 | doi = 10.1002/(SICI)1097-4571(199009)41:6<391::AID-ASI1>3.0.CO;2-9 | ref = harv | url-status = dead | archiveurl = https://web.archive.org/web/20120717020428/http://lsi.research.telcordia.com/lsi/papers/JASIS90.pdf | archivedate = 2012-07-17 | citeseerx = 10.1.1.33.2447 }}\n* {{cite journal | last1 = Pad\u00f3 | first1 = Sebastian | last2 = Lapata | first2 = Mirella | year = 2007 | title = Dependency-based construction of semantic space models | journal = Computational Linguistics | volume = 33 | issue = 2 | pages = 161\u2013199 | ref = harv | doi=10.1162/coli.2007.33.2.161}}\n* {{cite conference | last = Sch\u00fctze | first = Hinrich | year = 1993 | citeseerx = 10.1.1.41.8856 | title = Word Space | booktitle = Advances in Neural Information Processing Systems 5 | pages = 895\u2013902 | ref = harv}}\n* {{cite thesis | last = Sahlgren | first = Magnus | year = 2006 | title = The Word-Space Model | url = http://soda.swedish-ict.se/437/1/TheWordSpaceModel.pdf | degree = PhD | publisher = Stockholm University | ref = harv}}\n* {{cite web | url=http://www.welchco.com/02/14/01/60/96/02/2901.HTM | title=A Solution to Plato's Problem: The Latent Semantic Analysis Theory of Acquisition, Induction, and Representation of Knowledge |author1=Thomas Landauer |author2=Susan T. Dumais | accessdate=2007-07-02  | ref = harv }}\n* {{cite conference |author1=Kevin Lund |author2=Curt Burgess |author3=Ruth Ann Atchley | year = 1995 | title = Semantic and associative priming in a high-dimensional semantic space | conference = Cognitive Science Proceedings | pages = 660\u2013665 | ref = harv}}\n* {{cite journal |author1=Kevin Lund |author2=Curt Burgess | year = 1996 | title = Producing high-dimensional semantic spaces from lexical co-occurrence | journal = Behavior Research Methods, Instruments, and Computers | volume = 28 | issue = 2 | pages = 203\u2013208 | ref = harv | doi=10.3758/bf03204766}}\n\n==External links==\n*[http://zelligharris.org Zellig S. Harris]\n\n{{DEFAULTSORT:Distributional Hypothesis}}\n[[Category:Computational linguistics]]\n[[Category:Semantics]]\n[[Category:Language acquisition]]\n", "text_old": "'''Distributional semantics''' is a research area that develops and studies theories and methods for quantifying and categorizing semantic similarities between linguistic items based on their distributional properties in large samples of language data. The basic idea of distributional semantics can be summed up in the so-called [[Distributionalism|Distributional]] hypothesis: ''linguistic items with similar distributions have similar meanings.''\n\n==Distributional hypothesis==\nThe '''distributional hypothesis''' in [[linguistics]] is derived from the [[semantic theory]] of language usage, i.e. words that are used and occur in the same [[context (language use)|context]]s tend to purport similar meanings.<ref>{{harvnb|Harris|1954}}</ref> \n\nThe underlying idea that \"a word is characterized by the company it keeps\" was popularized by [[J. R. Firth|Firth]] in the 1950s.<ref>{{harvnb|Firth|1957}}</ref>\n\nThe distributional hypothesis is the basis for [[statistical semantics]]. Although the Distributional Hypothesis originated in linguistics,<ref>{{harvnb|Sahlgren|2008}}</ref> it is now receiving attention in [[cognitive science]] especially regarding the context of word use.<ref>{{harvnb|McDonald|Ramscar|2001}}</ref>\n\nIn recent years, the distributional hypothesis has provided the basis for the theory of [[similarity-based generalization]] in language learning: the idea that children can figure out how to use words they've rarely encountered before by generalizing about their use from distributions of similar words.<ref>{{harvnb| Gleitman|2002}}</ref><ref>{{harvnb| Yarlett|2008}}</ref>\n\nThe distributional hypothesis suggests that the more semantically similar two words are, the more distributionally similar they will be in turn, and thus the more that they will tend to occur in similar linguistic contexts.\n\nWhether or not this suggestion holds has significant implications for both the [[data-sparsity]] problem in computational modeling,<ref>Wishart, Ryder, and Prokopis Prokopidis. \u201cTopic Modelling Experiments on Hellenistic Corpora.\u201d In ''Proceedings of the Workshop on Corpora in the Digital Humanities 17'', 39\u201347. Bloomington, IN: CEUR Workshop Proceedings, 2017, Online: https://pdfs.semanticscholar.org/bd71/ab40960e481006117bafd0ae952d3e8d1f66.pdf.</ref> and for the question of how children are able to learn language so rapidly given relatively impoverished input (this is also known as the problem of the [[poverty of the stimulus]]).\n\n==Distributional semantic modeling==\nDistributional semantics favor the use of linear algebra as computational tool and representational framework. The basic approach is to collect distributional information in high-dimensional vectors, and to define distributional/semantic similarity in terms of vector similarity.<ref>{{harvnb|Rieger |1991 }}</ref> Different kinds of similarities can be extracted depending on which type of distributional information is used to collect the vectors: '''topical''' similarities can be extracted by populating the vectors with information on which text regions the linguistic items occur in; '''paradigmatic''' similarities can be extracted by populating the vectors with information on which other linguistic items the items co-occur with. Note that the latter type of vectors can also be used to extract '''syntagmatic''' similarities by looking at the individual vector components.\n\nThe basic idea of a correlation between distributional and semantic similarity can be operationalized in many different ways. There is a rich variety of computational models implementing distributional semantics, including [[latent semantic analysis]] (LSA),<ref>{{harvnb|Deerwester|Dumais|Furnas|Landauer|1990}}</ref><ref>{{Cite journal|last=Landauer|first=Thomas K.|last2=Dumais|first2=Susan T.|date=1997|title=A solution to Plato's problem: The latent semantic analysis theory of acquisition, induction, and representation of knowledge.|journal=Psychological Review|volume=104|issue=2|pages=211\u2013240|doi=10.1037/0033-295x.104.2.211|issn=1939-1471}}</ref> [[Hyperspace Analogue to Language]] (HAL), syntax- or dependency-based models,<ref>{{harvnb|Pad\u00f3|Lapata|2007}}</ref> [[random indexing]], [[semantic folding]]<ref>{{cite arXiv|eprint=1511.08855|title=Semantic Folding Theory And its Application in Semantic Fingerprinting|last=De Sousa Webber|first=Francisco|date=2015|publisher=|access-date=|class=cs.AI}}</ref> and various variants of the [[topic model]].<ref>{{Cite journal|last=Jordan|first=Michael I.|last2=Ng|first2=Andrew Y.|last3=Blei|first3=David M.|date=2003|title=Latent Dirichlet Allocation|url=http://www.jmlr.org/papers/v3/blei03a.html|journal=Journal of Machine Learning Research|volume=3|issue=Jan|pages=993\u20131022|issn=1533-7928}}</ref>\n\nDistributional semantic models differ primarily with respect to the following parameters:\n\n* Context type (text regions vs. linguistic items)\n* Context window (size, extension, etc.)\n* Frequency weighting (e.g. [[entropy (information theory)|entropy]], [[pointwise mutual information]],<ref>{{Cite journal|last=Church|first=Kenneth Ward|last2=Hanks|first2=Patrick|date=1989|title=Word association norms, mutual information, and lexicography|journal=Proceedings of the 27th Annual Meeting on Association for Computational Linguistics -|pages=76\u201383|location=Morristown, NJ, USA|publisher=Association for Computational Linguistics|doi=10.3115/981623.981633}}</ref> etc.)\n* Dimension reduction (e.g. [[random indexing]], [[singular value decomposition]], etc.)\n* [[Similarity measure]] (e.g. [[cosine similarity]], [[Minkowski distance]], etc.)\n\nDistributional semantic models that use linguistic items as context have also been referred to as '''word space, or vector space models'''.<ref>{{harvnb|Sch\u00fctze|1993}}</ref><ref>{{harvnb|Sahlgren|2006}}</ref>\n\n==Beyond Lexical Semantics==\nWhile distributional semantics typically has been applied to lexical items -- words and multi-word terms -- with considerable success, not least due to its applicability as an input layer for neurally inspired deep learning models, lexical semantics, i.e. the meaning of words, will only carry part of the semantics of an entire utterance. The meaning of a clause, e.g. ''\"Tigers love rabbits.\"'', can only partially be understood from examining the meaning of the three lexical items it consists of. Distributional semantics can straightforwardly be extended to cover larger linguistic item such as constructions, with and without non-instantiated items, but some of the base assumptions of the model need to be adjusted somewhat. [[Construction grammar]] and its formulation of the lexical-syntactic continuum offers one approach for including more elaborate constructions in a distributional semantic model and some experiments have been implemented using the Random Indexing approach <ref>{{cite journal |last1=Karlgren|first1=Jussi|author-link1=Jussi Karlgren|last2=Kanerva|first2=Pentti|date=2019|title=High-dimensional distributed semantic spaces for utterances|url=https://www.cambridge.org/core/journals/natural-language-engineering/article/highdimensional-distributed-semantic-spaces-for-utterances/38A75548413F9A430D1B2B35CE34CE43 |journal= Natural Language Engineering|volume=25 |issue=4 |pages=503-517 |doi=10.1017/S1351324919000226 |access-date=2020-04-13}}</ref>. \n\n[[Compositional distributional semantics|Compositional distributional semantic]] models extend distributional semantic models by explicit semantic functions that use syntactically based rules to combine the semantics of participating lexical units into a ''compositional model'' to characterize the semantics of entire phrases or sentences. Different approaches to composition have been explored -- including neural models -- and are under discussion at established workshops such as [[SemEval]].<ref>{{cite web |url=http://alt.qcri.org/semeval2014/task1/|title=SemEval-2014, Task 1}}</ref>\n\n==Applications==\n\nDistributional semantic models have been applied successfully to the following tasks:\n* finding [[semantic similarity]] between words and multi-word expressions;\n* [[Keyword clustering|word clustering]] based on semantic similarity;\n* automatic creation of [[thesauri]] and bilingual dictionaries;\n* lexical ambiguity resolution;\n* expanding search requests using synonyms and associations;\n* defining the topic of a document;\n* [[document clustering]] for [[information retrieval]];\n* [[data mining]] and [[named entities recognition]];\n* creating [[semantic mapping (literacy)|semantic maps]] of different subject domains;\n* [[Paraphrasing (computational linguistics)|paraphrasing]];\n* [[sentiment analysis]];\n* modeling selectional preferences of words.\n\n==Software==\n* [https://github.com/fozziethebeat/S-Space/ S-Space]\n* [https://github.com/semanticvectors/semanticvectors/ SemanticVectors]\n* [http://radimrehurek.com/gensim/index.html Gensim]\n* [http://www.linguatools.de/disco/disco-builder.html DISCO Builder]\n* [https://github.com/Lambda-3/Indra Indra]\n\n==See also==\n*[[Conceptual space]]\n*[[Co-occurrence]]\n*[[Gensim]]\n*[[Phraseme]]\n*[[Random indexing]]\n*[[Sentence embedding]]\n*[[Statistical semantics]]\n*[[Word2vec]]\n*[[Word embedding]]\n\n===People===\n*[[Scott Deerwester]]\n*[[Susan Dumais]]\n*[[J. R. Firth]]\n*[[George Furnas]]\n*[[Zellig Harris]]\n*[[Richard Hirschman]]\n*[[Thomas Landauer]]\n*[[Magnus Sahlgren]]\n*[[Hinrich Sch\u00fctze]]\n\n==References==\n\n{{reflist}}\n\n===Sources===\n\n* {{cite journal | last = Harris | first = Z. | year = 1954 | title = Distributional structure | journal = Word | volume = 10 | issue = 23| pages = 146\u2013162 | ref = harv | doi = 10.1080/00437956.1954.11659520 }}\n* {{cite journal | last = Firth| first = J.R. | year = 1957 | title = A synopsis of linguistic theory 1930-1955 | journal = Studies in Linguistic Analysis | pages = 1\u201332 | ref = harv }} Reprinted in {{cite book | editor = F.R. Palmer | title = Selected Papers of J.R. Firth 1952-1959 | publisher = London: Longman | year = 1968}}\n* {{cite journal | last = Sahlgren | first = Magnus | year = 2008 | title = The Distributional Hypothesis | url = http://soda.swedish-ict.se/3941/1/sahlgren.distr-hypo.pdf | journal = Rivista di Linguistica | volume = 20 | issue = 1 | pages = 33\u201353 | ref = harv }}\n* {{cite conference | last1 = McDonald | first1 = S. | last2 = Ramscar | first2 = M. | year = 2001 | citeseerx = 10.1.1.104.7535 | title = Testing the distributional hypothesis: The influence of context on judgements of semantic similarity | booktitle = Proceedings of the 23rd Annual Conference of the Cognitive Science Society | pages = 611\u2013616 | ref = harv }}\n* {{cite book | last = Gleitman | first = Lila R. | year = 2002 | ref = harv  | volume = 1| pages = 209\u2013229| doi=10.1075/cilt.228.17gle| series = Current Issues in Linguistic Theory | isbn = 978-90-272-4736-0 | chapter = Verbs of a feather flock together II | title = The Legacy of Zellig Harris }} \n* {{cite thesis | last = Yarlett | first = D. | year = 2008 | title = Language Learning Through Similarity-Based Generalization | url = http://psych.stanford.edu/~michael/papers/Draft_Yarlett_Similarity.pdf | degree = PhD | publisher = Stanford University | ref = harv | accessdate = 2012-07-12 | archiveurl = https://web.archive.org/web/20140419012951/http://psych.stanford.edu/~michael/papers/Draft_Yarlett_Similarity.pdf | archivedate = 2014-04-19 | url-status = dead }}\n* {{cite report | last = Rieger |first= Burghard B. | year= 1991 | citeseerx = 10.1.1.37.7976 | title  =  On Distributed Representations in Word Semantics | url  =  http://ftp.icsi.berkeley.edu/ftp/pub/techreports/1991/tr-91-012.pdf | publisher = ICSI Berkeley 12-1991 | ref = harv}}\n* {{cite journal | last1 = Deerwester | first1 = Scott | last2 = Dumais | first2 = Susan T. | last3 = Furnas | first3 = George W. | last4 = Landauer | first4 = Thomas K. | last5 = Harshman | first5 = Richard | url = http://lsi.research.telcordia.com/lsi/papers/JASIS90.pdf | title = Indexing by Latent Semantic Analysis | journal = Journal of the American Society for Information Science | volume = 41 | issue = 6 | pages = 391\u2013407 | year = 1990 | doi = 10.1002/(SICI)1097-4571(199009)41:6<391::AID-ASI1>3.0.CO;2-9 | ref = harv | url-status = dead | archiveurl = https://web.archive.org/web/20120717020428/http://lsi.research.telcordia.com/lsi/papers/JASIS90.pdf | archivedate = 2012-07-17 | citeseerx = 10.1.1.33.2447 }}\n* {{cite journal | last1 = Pad\u00f3 | first1 = Sebastian | last2 = Lapata | first2 = Mirella | year = 2007 | title = Dependency-based construction of semantic space models | journal = Computational Linguistics | volume = 33 | issue = 2 | pages = 161\u2013199 | ref = harv | doi=10.1162/coli.2007.33.2.161}}\n* {{cite conference | last = Sch\u00fctze | first = Hinrich | year = 1993 | citeseerx = 10.1.1.41.8856 | title = Word Space | booktitle = Advances in Neural Information Processing Systems 5 | pages = 895\u2013902 | ref = harv}}\n* {{cite thesis | last = Sahlgren | first = Magnus | year = 2006 | title = The Word-Space Model | url = http://soda.swedish-ict.se/437/1/TheWordSpaceModel.pdf | degree = PhD | publisher = Stockholm University | ref = harv}}\n* {{cite web | url=http://www.welchco.com/02/14/01/60/96/02/2901.HTM | title=A Solution to Plato's Problem: The Latent Semantic Analysis Theory of Acquisition, Induction, and Representation of Knowledge |author1=Thomas Landauer |author2=Susan T. Dumais | accessdate=2007-07-02  | ref = harv }}\n* {{cite conference |author1=Kevin Lund |author2=Curt Burgess |author3=Ruth Ann Atchley | year = 1995 | title = Semantic and associative priming in a high-dimensional semantic space | conference = Cognitive Science Proceedings | pages = 660\u2013665 | ref = harv}}\n* {{cite journal |author1=Kevin Lund |author2=Curt Burgess | year = 1996 | title = Producing high-dimensional semantic spaces from lexical co-occurrence | journal = Behavior Research Methods, Instruments, and Computers | volume = 28 | issue = 2 | pages = 203\u2013208 | ref = harv | doi=10.3758/bf03204766}}\n\n==External links==\n*[http://zelligharris.org Zellig S. Harris]\n\n{{DEFAULTSORT:Distributional Hypothesis}}\n[[Category:Computational linguistics]]\n[[Category:Semantics]]\n[[Category:Language acquisition]]\n", "name_user": "Jussi Karlgren", "label": "safe", "comment": "\u2192\u200eDistributional semantic modeling", "url_page": "//en.wikipedia.org/wiki/Distributional_semantics"}
