{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deleting the empty folders\n",
    "\n",
    "Before preprocessing the data and training a model, there are many directories or folder which do not contain 'part files'. It is easier to delete these folders since they are not significant."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "import numpy\n",
    "\n",
    "from pyspark.ml import Pipeline\n",
    "        \n",
    "from pyspark.ml.feature import Tokenizer, RegexTokenizer, StopWordsRemover, HashingTF, IDF, OneHotEncoder, StringIndexer, VectorAssembler\n",
    "\n",
    "from pyspark.sql.types import IntegerType, StringType, ArrayType\n",
    "from pyspark.streaming import StreamingContext\n",
    "from pyspark.sql import Row\n",
    "from pyspark.sql.functions import udf, struct, array, col, lit, monotonically_increasing_id\n",
    "\n",
    "from pyspark.sql.types import StructType\n",
    "from pyspark.sql.types import StructField\n",
    "\n",
    "from pyspark.ml.classification import NaiveBayes, NaiveBayesModel\n",
    "from pyspark.ml.linalg import Vectors\n",
    "from pyspark.mllib.regression import LabeledPoint\n",
    "from pyspark.mllib.evaluation import MultilabelMetrics\n",
    "from pyspark.mllib.util import MLUtils\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Users/Simon/Documents/GitHub/adana_task3/Spark_Cleaned'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Determine directories of the data and the location for the cleaned data\n",
    "directory = r'/Users/Simon/Documents/GitHub/adana_task3/Spark_Temp'\n",
    "new_directory = r'/Users/Simon/Documents/GitHub/adana_task3/Spark_Cleaned'\n",
    "\n",
    "# directory = r'/⁨Users⁩/Simon⁩/Documents/Stream_History'\n",
    "# new_directory = r'/⁨Users⁩/Simon⁩/Documents/Stream_New'\n",
    "filenames = os.listdir(directory) #a list of all the folder names\n",
    "\n",
    "shutil.rmtree(new_directory, ignore_errors=False, onerror=None) #Reset (delete) the new directory\n",
    "shutil.copytree(directory, new_directory) #Copy the data to a new directory to built in some redundancy \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Total Number of Original Folders is:  5009\n",
      "\n",
      "The Total Number of Removed Folders is:  3888\n",
      "\n",
      "Remaining folders:  1121\n"
     ]
    }
   ],
   "source": [
    "## Removing the folders which do not contain 'part files'. These folders do not contain any useful data.\n",
    "counter = 0\n",
    "removed_folders = []\n",
    "for i in range(len(filenames)):\n",
    "    remove = False\n",
    "    folder = new_directory + \"/\" + filenames[i]\n",
    "    \n",
    "    if filenames[i] == '.DS_Store':\n",
    "        continue\n",
    "        \n",
    "    if len(os.listdir(folder)) <= 1:\n",
    "        remove = True\n",
    "        removed_folders.append(filenames[i])\n",
    "        counter += 1\n",
    "        shutil.rmtree(folder, ignore_errors=False, onerror=None)\n",
    "        \n",
    "print(\"The Total Number of Original Folders is: \", len(filenames))\n",
    "print()\n",
    "\n",
    "print(\"The Total Number of Removed Folders is: \", counter)\n",
    "print()\n",
    "\n",
    "print(\"Remaining folders: \", (len(filenames)) - counter )\n",
    "# print(removed_folders)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of directories included:  5008\n",
      "Number of part files included:  2703\n"
     ]
    }
   ],
   "source": [
    "def load_rdd(base_directory):\n",
    "    # Get all the directory names of the saved myoutput folders\n",
    "    foldernames = os.listdir(base_directory)\n",
    "    \n",
    "    # Create list of full directories\n",
    "    full_directories = []\n",
    "    \n",
    "    for i in range(len(foldernames)):\n",
    "        \n",
    "        if foldernames[i] == '.DS_Store':\n",
    "            continue\n",
    "        \n",
    "        directory_temp = base_directory + \"/\" + foldernames[i]\n",
    "        full_directories.append(directory_temp)\n",
    "    \n",
    "    print(\"Number of directories included: \", len(full_directories))\n",
    "    \n",
    "    df = spark.read.format('json').load(path=full_directories)\n",
    "    \n",
    "    \n",
    "    \n",
    "#     # Create a dataframe from the rdds in the first folder of the list\n",
    "#     first_directory = base_directory + \"/\" + foldernames[0]\n",
    "#     rdd = sc.textFile(first_directory)\n",
    "#     df = spark.read.json(rdd).toPandas()\n",
    "    \n",
    "#     # Remove this folder from the list to prevent it from being added twice\n",
    "#     foldernames.remove(foldernames[0])\n",
    "    \n",
    "#     for i in range(len(foldernames)):\n",
    "        \n",
    "#         if foldernames[i] == '.DS_Store':\n",
    "#             continue\n",
    "        \n",
    "#         directory = base_directory + \"/\" + foldernames[i]\n",
    "#         rdd = sc.textFile(directory)\n",
    "#         df_temp = spark.read.json(rdd).toPandas()\n",
    "#         df = df.append(df_temp)\n",
    "\n",
    "    return df\n",
    "\n",
    "# base_directory = r'/Users/Simon/Documents/GitHub/adana_task3/Spark_Cleaned/myoutput-1586797640000/part-00003'\n",
    "# base_directory = r'/Users/Simon/Documents/GitHub/adana_task3/Spark_Cleaned/myoutput-1586797640000'\n",
    "# base_directory = r'/Users/Simon/Documents/GitHub/adana_task3/Spark_Data_Limited'\n",
    "base_directory = r'/Users/Simon/Documents/GitHub/adana_task3/Spark_Cleaned'\n",
    "\n",
    "df = load_rdd(base_directory) \n",
    "print(\"Number of part files included: \", len(df.toPandas()))\n",
    "\n",
    "# df.write.csv(r'/Users/Simon/Documents/GitHub/adana_task3/All_Data.csv')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "IDEA: Make a pandas dataframe and save this as csv! No spark!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
