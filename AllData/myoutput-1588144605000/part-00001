{"title_page": "Locality of reference", "text_new": "{{Infobox settlement\n|name = Shwe Pan Kone Village\n|pushpin_label_position = bottom\n|pushpin_map = Myanmar\n|pushpin_map_caption = Location in Myanmar\n|seat = [[Shwe Pan Kone]]\n|seat_type = Cottage Capital\n|settlement_type = [[village of Burma|Village]]\n|image_skyline = \n|image_map = \n|map_caption = \n|subdivision_type = [[List of sovereign states|Village]]\n|subdivision_name = {{flag|Myanmar}}\n|subdivision_type1 = [[Administrative divisions of Myanmar|Village]]\n|subdivision_name1 = [[Sagaing Region]]\n|subdivision_type2 = [[Districts of Myanmar|District]]\n|subdivision_name2 = [[Monywa District]]\n|area_total_km2 = \n|population = \n|population_as_of = \n|population_density_km2 = auto\n|coordinates = {{coord|22|17|N|95|27|E|Village:MM|display=inline,title}}\n|elevation_ft = \n|elevation_m = \n|timezone = [[Time in Burma|MST]]\n|utc_offset = +6.30\n|website =\n}}\n'''shwe pan Kone village ''' is a township in [[Monywa District]] in the [[Sagaing Division]] of [[Myanmar]]. Myanmar Information Management Unit (MIMU)</ref> The principal village is [[Shwe pan Kone]].{{Infobox settlement\n|settlement_type = Village\n|name = Shwe Pan Kone\n|native_name = \u101b\u103d\u103e\u1031\u1015\u1014\u103a\u1038\u1000\u102f\u1036\u1038\n|pushpin_label_position = right\n|pushpin_map = Burma\n|pushpin_map_caption = Location in Burma\n|image_skyline = \n|image_map = \n|map_caption = \n|subdivision_type = [[List of sovereign states|village]]\n|subdivision_name = {{flag|Myanmar}}\n|subdivision_type1 = [[Administrative divisions of Burma|Region]]\n|subdivision_name1 = {{flag|Sagaing Region}}\n|subdivision_type2 = [[Districts of Burma|District]]\n|subdivision_name2 = [[Shwebo District]]\n|subdivision_type3 = [[Village of Burma|village]]\n|subdivision_name3 = [[Shwe Pan Kone Village]]\n'''Shwe Pan Kone''' ({{lang-my|'''\u101b\u103d\u103e\u1031\u1015\u1014\u103a\u1038\u1000\u102f\u1036\u1038'''}}) is a town in [[Shwebo District]], [[Sagaing Region]] in [[Myanmar]].  It is the administrative seat for [[Shwe Pan Kone Village]].<ref>[http://www.myanmars.net/myanmar-map/sagaing-map.htm \"Map of Sagaing Division\"] Myanmar's NET</ref>\n\n== Types of locality ==\nThere are several different types of locality of reference:\n\n* '''Temporal locality''': If at one point a particular memory location is referenced, then it is likely that the same location will be referenced again in the near future. There is a temporal proximity between the adjacent references to the same memory location. In this case it is common to make efforts to store a copy of the referenced data in faster memory storage, to reduce the latency of subsequent references. Temporal locality is a special case of spatial locality (see below), namely when the prospective location is identical to the present location.\n* '''Spatial locality''': If a particular storage location is referenced at a particular time, then it is likely that nearby memory locations will be referenced in the near future. In this case it is common to attempt to guess the size and shape of the area around the current reference for which it is worthwhile to prepare faster access for subsequent reference.\n** '''Memory locality''' (or ''data locality''<ref name=\"NistBig1\"/>):  Spatial locality explicitly relating to [[computer memory|memory]].\n* '''[[Branch (computer science)|Branch]] locality''': If there are only a few possible alternatives for the prospective part of the path in the spatial-temporal coordinate space. This is the case when an instruction loop has a simple structure, or the possible outcome of a small system of conditional branching instructions is restricted to a small set of possibilities.  Branch locality is typically not a spatial locality since the few possibilities can be located far away from each other.\n* '''Equidistant locality''': It is halfway between the spatial locality and the branch locality.  Consider a loop accessing locations in an equidistant pattern, i.e., the path in the spatial-temporal coordinate space is a dotted line.  In this case, a simple linear function can predict which location will be accessed in the near future.\n\nIn order to benefit from the very frequently occurring temporal and spatial locality, most of the information storage systems are [[Computer data storage#Hierarchy of storage|hierarchical]]. The equidistant locality is usually supported by the diverse nontrivial increment instructions of the processors. For branch locality, the contemporary processors have sophisticated branch predictors, and on the basis of this prediction the memory manager of the processor tries to collect and preprocess the data of the plausible alternatives.\n\n==Relevance==\nThere are several reasons for locality.  These reasons are either goals to achieve or circumstances to accept, depending on the aspect.  The reasons below are not [[Disjoint sets|disjoint]]; in fact, the list below goes from the most general case to special cases:\n\n* '''Predictability''': Locality is merely one type of predictable behavior in computer systems.\n* '''Structure of the program''': Locality occurs often because of the way in which computer programs are created, for handling decidable problems.  Generally, related data is stored in nearby locations in storage.  One common pattern in computing involves the processing of several items, one at a time. This means that if a lot of processing is done, the single item will be accessed more than once, thus leading to temporal locality of reference.  Furthermore, moving to the next item implies that the next item will be read, hence spatial locality of reference, since memory locations are typically read in batches.\n* '''Linear data structures''': Locality often occurs because code contains loops that tend to reference arrays or other data structures by indices. Sequential locality, a special case of spatial locality, occurs when relevant data elements are arranged and accessed linearly. For example, the simple traversal of elements in a one-dimensional array, from the base address to the highest element would exploit the sequential locality of the array in memory.<ref>Aho, Lam, Sethi, and Ullman. \"Compilers: Principles, Techniques & Tools\" 2nd ed. Pearson Education, Inc. 2007</ref> Equidistant locality occurs when the linear traversal is over a longer area of adjacent [[data structure]]s with identical structure and size, accessing mutually corresponding elements of each structure rather than each entire structure.  This is the case when a [[Matrix (mathematics)|matrix]] is represented as a sequential matrix of rows and the requirement is to access a single column of the matrix.\n* '''Efficiency of memory hierarchy use''': Although [[random access memory]] presents the programmer with the ability to read or write anywhere at any time, in practice [[latency (engineering)|latency]] and throughput are affected by the efficiency of the [[Cache (computing)|cache]], which is improved by increasing the locality of reference. Poor locality of reference results in cache [[Thrashing (computer science)|thrashing]] and [[cache pollution]] and to avoid it, data elements with poor locality can be bypassed from cache.<ref>\"[https://www.academia.edu/24842555/A_Survey_of_Cache_Bypassing_Techniques A Survey Of Cache Bypassing Techniques]\", JLPEA, vol. 6, no. 2, 2016</ref>\n\n== General usage ==\nIf most of the time the substantial portion of the references aggregate into clusters, and if the shape of this system of clusters can be well predicted, then it can be used for performance optimization. There are several ways to benefit from locality using [[optimization (computer science)|optimization]] techniques. Common techniques are:\n\n* Increasing the locality of references (generally on the software side)\n* Exploiting the locality of references: Generally achieved on the hardware side, temporal and spatial locality can be capitalized by hierarchical storage hardware. The equidistant locality can be used by the appropriately specialized instructions of the processors, this possibility is not only the responsibility of hardware, but the software as well, whether its structure is suitable for compiling a binary program that calls the specialized instructions in question.  The branch locality is a more elaborate possibility, hence more developing effort is needed, but there is much larger reserve for future exploration in this kind of locality than in all the remaining ones.\n\n== Spatial and temporal locality usage ==\n\n=== Hierarchial memory ===\n{{main|Memory hierarchy}}\n\nHierarchical memory is a hardware optimization that takes the benefits of spatial and temporal locality and can be used on several levels of the memory hierarchy. [[Paging]] obviously benefits from temporal and spatial locality.  A cache is a simple example of exploiting temporal locality, because it is a specially designed, faster but smaller memory area, generally used to keep recently referenced data and data near recently referenced data, which can lead to potential performance increases.\n\nData elements in a cache do not necessarily correspond to data elements that are spatially close in the main memory; however, data elements are brought into cache one [[cache line]] at a time. This means that spatial locality is again important: if one element is referenced, a few neighboring elements will also be brought into cache. Finally, temporal locality plays a role on the lowest level, since results that are referenced very closely together can be kept in the [[Processor register|machine registers]]. Some programming languages (such as [[C (programming language)|C]]) allow the programmer to suggest that certain variables be kept in registers.\n\nData locality is a typical memory reference feature of regular programs (though many irregular memory access patterns exist). It makes the hierarchical memory layout profitable. In computers, memory is divided into a hierarchy in order to speed up data accesses.  The lower levels of the memory hierarchy tend to be slower, but larger.  Thus, a program will achieve greater performance if it uses memory while it is cached in the upper levels of the memory hierarchy and avoids bringing other data into the upper levels of the hierarchy that will displace data that will be used shortly in the future.  This is an ideal, and sometimes cannot be achieved.\n\nTypical memory hierarchy (access times and cache sizes are approximations of typical values used {{As of|2013|lc=on}} for the purpose of discussion; actual values and actual numbers of levels in the hierarchy vary):\n* [[CPU register]]s (8-256 registers) &ndash; immediate access, with the speed of the innermost core of the processor\n* L1 [[CPU cache]]s (32&nbsp;KiB to 512&nbsp;[[KiB]]) &ndash; fast access, with the speed of the innermost memory bus owned exclusively by each core\n* L2 CPU caches (128&nbsp;KiB to 24&nbsp;[[MiB]]) &ndash; slightly slower access, with the speed of the [[memory bus]] shared between twins of cores\n* L3 CPU caches (2&nbsp;MiB to 32&nbsp;[[MiB]]) &ndash; even slower access, with the speed of the memory bus shared between even more cores of the same processor\n* Main [[physical memory]] ([[RAM]]) (256&nbsp;MiB to 64&nbsp;[[GiB]]) &ndash; slow access, the speed of which is limited by the spatial distances and general hardware interfaces between the processor and the memory modules on the [[motherboard]]\n* Disk ([[virtual memory]], [[file system]]) (1&nbsp;GiB to 256&nbsp;[[TiB]]) &ndash; very slow, due to the narrower (in bit width), physically much longer data channel between the main board of the computer and the disk devices, and due to the extraneous software protocol needed on the top of the slow hardware interface\n* Remote memory (other computers or the cloud) (practically unlimited) &ndash; speed varies from very slow to extremely slow\n\nModern machines tend to read blocks of lower memory into the next level of the memory hierarchy.  If this displaces used memory, the [[operating system]] tries to predict which data will be accessed least (or latest) and move it down the memory hierarchy.  Prediction algorithms tend to be simple to reduce hardware complexity, though they are becoming somewhat more complicated.\n\n=== Matrix multiplication ===\nA common example is [[Matrix multiplication algorithm|matrix multiplication]]:\n\n<syntaxhighlight lang=\"pascal\" line=\"1\">\nfor i in 0..n\n  for j in 0..m\n    for k in 0..p\n      C[i][j] = C[i][j] + A[i][k] * B[k][j];\n</syntaxhighlight>\n\nBy switching the looping order for <code>j</code> and <code>k</code>, the speedup in large matrix multiplications becomes dramatic, at least for languages that put contiguous array elements in the last dimension.  This will not change the mathematical result, but it improves efficiency.  In this case, \"large\" means, approximately, more than 100,000 elements in each matrix, or enough addressable memory such that the matrices will not fit in L1 and L2 caches.\n\n<syntaxhighlight lang=\"pascal\" line=\"1\">\nfor i in 0..n\n  for k in 0..p\n    for j in 0..m\n      C[i][j] = C[i][j] + A[i][k] * B[k][j];\n</syntaxhighlight>\n\nThe reason for this speedup is that in the first case, the reads of <code>A[i][k]</code> are in cache (since the <code>k</code> index is the contiguous, last dimension), but <code>B[k][j]</code> is not, so there is a cache miss penalty on <code>B[k][j]</code>. <code>C[i][j]</code> is irrelevant, because it can be factored out of the inner loop{{why?|date=December 2019}}. In the second case, the reads and writes of <code>C[i][j]</code> are both in cache, the reads of <code>B[k][j]</code> are in cache, and the read of <code>A[i][k]</code> can be factored out of the inner loop{{Explain|date=December 2019}}. Thus, the second example has no cache miss penalty in the inner loop while the first example has a cache penalty.\n\nOn a year 2014 processor, the second case is approximately five times faster than the first case, when written in [[C (programming language)|C]] and compiled with <code>gcc -O3</code>.  (A careful examination of the disassembled code shows that in the first case, [[GNU Compiler Collection|GCC]] uses [[SIMD]] instructions and in the second case it does not, but the cache penalty is much worse than the SIMD gain.){{Citation needed|date=September 2014}}\n\nTemporal locality can also be improved in the above example by using a technique called [[Loop blocking|blocking]]. The larger matrix can be divided into evenly sized sub-matrices, so that the smaller blocks can be referenced (multiplied) several times while in memory.\n\n<syntaxhighlight lang=\"pascal\" line=\"1\">\nfor (ii = 0; ii < SIZE; ii += BLOCK_SIZE)\n  for (kk = 0; kk < SIZE; kk += BLOCK_SIZE)\n    for (jj = 0; jj < SIZE; jj += BLOCK_SIZE)\n      maxi = min(ii + BLOCK_SIZE, SIZE);\n      for (i = ii; i < maxi; i++)\n        maxk = min(kk + BLOCK_SIZE, SIZE);\n        for (k = kk; k < maxk; k++)\n          maxj = min(jj + BLOCK_SIZE, SIZE);\n          for (j = jj; j < maxj; j++)\n            C[i][j] = C[i][j] + A[i][k] * B[k][j];\n</syntaxhighlight>\n\nThe temporal locality of the above solution is provided because a block can be used several times before moving on, so that it is moved in and out of memory less often.  Spatial locality is improved because elements with consecutive memory addresses tend to be pulled up the memory hierarchy together.\n\n== See also ==\n{{Portal|Computer programming}}\n\n* [[Cache-oblivious algorithm]]\n* [[File system fragmentation]]\n* [[Partitioned global address space]]\n* [[Row- and column-major order]]\n* [[Scalable locality]]\n* [[Scratchpad memory]]\n* [[Working set]]\n\n== References ==\n{{Reflist}}\n\n== Bibliography ==\n* [[Peter J. Denning]], [http://denninginstitute.com/pjd/PUBS/CACMcols/cacmJul05.pdf \"The Locality Principle\"], ''Communications of the ACM'', Volume 48, Issue 7, (2005), Pages 19\u201324\n* Peter J. Denning, Stuart C. Schwartz, [http://denninginstitute.com/pjd/PUBS/WSProp_1972.pdf \"Properties of the Working-Set Model\"], ''Communications of the ACM'',  Volume 15,  Issue 3  (March 1972), Pages 191-198\n\n{{DEFAULTSORT:Locality Of Reference}}\n[[Category:Computer memory]]\n[[Category:Cache (computing)]]\n[[Category:Software optimization]]\n", "text_old": "{{more citations needed|date=July 2008}}\nIn [[computer science]], '''locality of reference''', also known as the '''principle of locality''',<ref>Not to be confused with the [[principle of locality]] in physics.</ref> is the tendency of a processor to access the same set of memory locations repetitively over a short period of time.<ref>{{Cite book|title=Computer organization and architecture : designing for performance|last=William.|first=Stallings|date=2010|publisher=Prentice Hall|isbn=9780136073734|edition= 8th|location=Upper Saddle River, NJ|oclc=268788976}}</ref> There are two basic types of reference locality{{snd}} temporal and spatial locality. Temporal locality refers to the reuse of specific data, and/or resources, within a relatively small time duration. Spatial locality (also termed ''data locality''<ref name=\"NistBig1\">\"NIST Big Data Interoperability Framework: Volume 1\", [https://doi.org/10.6028/NIST.SP.1500-1r2 urn:doi:10.6028/NIST.SP.1500-1r2</ref>) refers to the use of data elements within relatively close storage locations. Sequential locality, a special case of spatial locality, occurs when data elements are arranged and accessed linearly, such as, traversing the elements in a one-dimensional [[Array data structure|array]].\n\nLocality is a type of [[predictability|predictable]] behavior that occurs in computer systems. Systems that exhibit strong ''locality of reference'' are great candidates for performance optimization through the use of techniques such as the [[CPU cache|caching]], [[prefetch instruction|prefetching]] for memory and advanced [[branch predictor]]s at the [[Pipeline (computing)|pipelining]] stage of a processor core.\n\n== Types of locality ==\nThere are several different types of locality of reference:\n\n* '''Temporal locality''': If at one point a particular memory location is referenced, then it is likely that the same location will be referenced again in the near future. There is a temporal proximity between the adjacent references to the same memory location. In this case it is common to make efforts to store a copy of the referenced data in faster memory storage, to reduce the latency of subsequent references. Temporal locality is a special case of spatial locality (see below), namely when the prospective location is identical to the present location.\n* '''Spatial locality''': If a particular storage location is referenced at a particular time, then it is likely that nearby memory locations will be referenced in the near future. In this case it is common to attempt to guess the size and shape of the area around the current reference for which it is worthwhile to prepare faster access for subsequent reference.\n** '''Memory locality''' (or ''data locality''<ref name=\"NistBig1\"/>):  Spatial locality explicitly relating to [[computer memory|memory]].\n* '''[[Branch (computer science)|Branch]] locality''': If there are only a few possible alternatives for the prospective part of the path in the spatial-temporal coordinate space. This is the case when an instruction loop has a simple structure, or the possible outcome of a small system of conditional branching instructions is restricted to a small set of possibilities.  Branch locality is typically not a spatial locality since the few possibilities can be located far away from each other.\n* '''Equidistant locality''': It is halfway between the spatial locality and the branch locality.  Consider a loop accessing locations in an equidistant pattern, i.e., the path in the spatial-temporal coordinate space is a dotted line.  In this case, a simple linear function can predict which location will be accessed in the near future.\n\nIn order to benefit from the very frequently occurring temporal and spatial locality, most of the information storage systems are [[Computer data storage#Hierarchy of storage|hierarchical]]. The equidistant locality is usually supported by the diverse nontrivial increment instructions of the processors. For branch locality, the contemporary processors have sophisticated branch predictors, and on the basis of this prediction the memory manager of the processor tries to collect and preprocess the data of the plausible alternatives.\n\n==Relevance==\nThere are several reasons for locality.  These reasons are either goals to achieve or circumstances to accept, depending on the aspect.  The reasons below are not [[Disjoint sets|disjoint]]; in fact, the list below goes from the most general case to special cases:\n\n* '''Predictability''': Locality is merely one type of predictable behavior in computer systems.\n* '''Structure of the program''': Locality occurs often because of the way in which computer programs are created, for handling decidable problems.  Generally, related data is stored in nearby locations in storage.  One common pattern in computing involves the processing of several items, one at a time. This means that if a lot of processing is done, the single item will be accessed more than once, thus leading to temporal locality of reference.  Furthermore, moving to the next item implies that the next item will be read, hence spatial locality of reference, since memory locations are typically read in batches.\n* '''Linear data structures''': Locality often occurs because code contains loops that tend to reference arrays or other data structures by indices. Sequential locality, a special case of spatial locality, occurs when relevant data elements are arranged and accessed linearly. For example, the simple traversal of elements in a one-dimensional array, from the base address to the highest element would exploit the sequential locality of the array in memory.<ref>Aho, Lam, Sethi, and Ullman. \"Compilers: Principles, Techniques & Tools\" 2nd ed. Pearson Education, Inc. 2007</ref> Equidistant locality occurs when the linear traversal is over a longer area of adjacent [[data structure]]s with identical structure and size, accessing mutually corresponding elements of each structure rather than each entire structure.  This is the case when a [[Matrix (mathematics)|matrix]] is represented as a sequential matrix of rows and the requirement is to access a single column of the matrix.\n* '''Efficiency of memory hierarchy use''': Although [[random access memory]] presents the programmer with the ability to read or write anywhere at any time, in practice [[latency (engineering)|latency]] and throughput are affected by the efficiency of the [[Cache (computing)|cache]], which is improved by increasing the locality of reference. Poor locality of reference results in cache [[Thrashing (computer science)|thrashing]] and [[cache pollution]] and to avoid it, data elements with poor locality can be bypassed from cache.<ref>\"[https://www.academia.edu/24842555/A_Survey_of_Cache_Bypassing_Techniques A Survey Of Cache Bypassing Techniques]\", JLPEA, vol. 6, no. 2, 2016</ref>\n\n== General usage ==\nIf most of the time the substantial portion of the references aggregate into clusters, and if the shape of this system of clusters can be well predicted, then it can be used for performance optimization. There are several ways to benefit from locality using [[optimization (computer science)|optimization]] techniques. Common techniques are:\n\n* Increasing the locality of references (generally on the software side)\n* Exploiting the locality of references: Generally achieved on the hardware side, temporal and spatial locality can be capitalized by hierarchical storage hardware. The equidistant locality can be used by the appropriately specialized instructions of the processors, this possibility is not only the responsibility of hardware, but the software as well, whether its structure is suitable for compiling a binary program that calls the specialized instructions in question.  The branch locality is a more elaborate possibility, hence more developing effort is needed, but there is much larger reserve for future exploration in this kind of locality than in all the remaining ones.\n\n== Spatial and temporal locality usage ==\n\n=== Hierarchial memory ===\n{{main|Memory hierarchy}}\n\nHierarchical memory is a hardware optimization that takes the benefits of spatial and temporal locality and can be used on several levels of the memory hierarchy. [[Paging]] obviously benefits from temporal and spatial locality.  A cache is a simple example of exploiting temporal locality, because it is a specially designed, faster but smaller memory area, generally used to keep recently referenced data and data near recently referenced data, which can lead to potential performance increases.\n\nData elements in a cache do not necessarily correspond to data elements that are spatially close in the main memory; however, data elements are brought into cache one [[cache line]] at a time. This means that spatial locality is again important: if one element is referenced, a few neighboring elements will also be brought into cache. Finally, temporal locality plays a role on the lowest level, since results that are referenced very closely together can be kept in the [[Processor register|machine registers]]. Some programming languages (such as [[C (programming language)|C]]) allow the programmer to suggest that certain variables be kept in registers.\n\nData locality is a typical memory reference feature of regular programs (though many irregular memory access patterns exist). It makes the hierarchical memory layout profitable. In computers, memory is divided into a hierarchy in order to speed up data accesses.  The lower levels of the memory hierarchy tend to be slower, but larger.  Thus, a program will achieve greater performance if it uses memory while it is cached in the upper levels of the memory hierarchy and avoids bringing other data into the upper levels of the hierarchy that will displace data that will be used shortly in the future.  This is an ideal, and sometimes cannot be achieved.\n\nTypical memory hierarchy (access times and cache sizes are approximations of typical values used {{As of|2013|lc=on}} for the purpose of discussion; actual values and actual numbers of levels in the hierarchy vary):\n* [[CPU register]]s (8-256 registers) &ndash; immediate access, with the speed of the innermost core of the processor\n* L1 [[CPU cache]]s (32&nbsp;KiB to 512&nbsp;[[KiB]]) &ndash; fast access, with the speed of the innermost memory bus owned exclusively by each core\n* L2 CPU caches (128&nbsp;KiB to 24&nbsp;[[MiB]]) &ndash; slightly slower access, with the speed of the [[memory bus]] shared between twins of cores\n* L3 CPU caches (2&nbsp;MiB to 32&nbsp;[[MiB]]) &ndash; even slower access, with the speed of the memory bus shared between even more cores of the same processor\n* Main [[physical memory]] ([[RAM]]) (256&nbsp;MiB to 64&nbsp;[[GiB]]) &ndash; slow access, the speed of which is limited by the spatial distances and general hardware interfaces between the processor and the memory modules on the [[motherboard]]\n* Disk ([[virtual memory]], [[file system]]) (1&nbsp;GiB to 256&nbsp;[[TiB]]) &ndash; very slow, due to the narrower (in bit width), physically much longer data channel between the main board of the computer and the disk devices, and due to the extraneous software protocol needed on the top of the slow hardware interface\n* Remote memory (other computers or the cloud) (practically unlimited) &ndash; speed varies from very slow to extremely slow\n\nModern machines tend to read blocks of lower memory into the next level of the memory hierarchy.  If this displaces used memory, the [[operating system]] tries to predict which data will be accessed least (or latest) and move it down the memory hierarchy.  Prediction algorithms tend to be simple to reduce hardware complexity, though they are becoming somewhat more complicated.\n\n=== Matrix multiplication ===\nA common example is [[Matrix multiplication algorithm|matrix multiplication]]:\n\n<syntaxhighlight lang=\"pascal\" line=\"1\">\nfor i in 0..n\n  for j in 0..m\n    for k in 0..p\n      C[i][j] = C[i][j] + A[i][k] * B[k][j];\n</syntaxhighlight>\n\nBy switching the looping order for <code>j</code> and <code>k</code>, the speedup in large matrix multiplications becomes dramatic, at least for languages that put contiguous array elements in the last dimension.  This will not change the mathematical result, but it improves efficiency.  In this case, \"large\" means, approximately, more than 100,000 elements in each matrix, or enough addressable memory such that the matrices will not fit in L1 and L2 caches.\n\n<syntaxhighlight lang=\"pascal\" line=\"1\">\nfor i in 0..n\n  for k in 0..p\n    for j in 0..m\n      C[i][j] = C[i][j] + A[i][k] * B[k][j];\n</syntaxhighlight>\n\nThe reason for this speedup is that in the first case, the reads of <code>A[i][k]</code> are in cache (since the <code>k</code> index is the contiguous, last dimension), but <code>B[k][j]</code> is not, so there is a cache miss penalty on <code>B[k][j]</code>. <code>C[i][j]</code> is irrelevant, because it can be factored out of the inner loop{{why?|date=December 2019}}. In the second case, the reads and writes of <code>C[i][j]</code> are both in cache, the reads of <code>B[k][j]</code> are in cache, and the read of <code>A[i][k]</code> can be factored out of the inner loop{{Explain|date=December 2019}}. Thus, the second example has no cache miss penalty in the inner loop while the first example has a cache penalty.\n\nOn a year 2014 processor, the second case is approximately five times faster than the first case, when written in [[C (programming language)|C]] and compiled with <code>gcc -O3</code>.  (A careful examination of the disassembled code shows that in the first case, [[GNU Compiler Collection|GCC]] uses [[SIMD]] instructions and in the second case it does not, but the cache penalty is much worse than the SIMD gain.){{Citation needed|date=September 2014}}\n\nTemporal locality can also be improved in the above example by using a technique called [[Loop blocking|blocking]]. The larger matrix can be divided into evenly sized sub-matrices, so that the smaller blocks can be referenced (multiplied) several times while in memory.\n\n<syntaxhighlight lang=\"pascal\" line=\"1\">\nfor (ii = 0; ii < SIZE; ii += BLOCK_SIZE)\n  for (kk = 0; kk < SIZE; kk += BLOCK_SIZE)\n    for (jj = 0; jj < SIZE; jj += BLOCK_SIZE)\n      maxi = min(ii + BLOCK_SIZE, SIZE);\n      for (i = ii; i < maxi; i++)\n        maxk = min(kk + BLOCK_SIZE, SIZE);\n        for (k = kk; k < maxk; k++)\n          maxj = min(jj + BLOCK_SIZE, SIZE);\n          for (j = jj; j < maxj; j++)\n            C[i][j] = C[i][j] + A[i][k] * B[k][j];\n</syntaxhighlight>\n\nThe temporal locality of the above solution is provided because a block can be used several times before moving on, so that it is moved in and out of memory less often.  Spatial locality is improved because elements with consecutive memory addresses tend to be pulled up the memory hierarchy together.\n\n== See also ==\n{{Portal|Computer programming}}\n\n* [[Cache-oblivious algorithm]]\n* [[File system fragmentation]]\n* [[Partitioned global address space]]\n* [[Row- and column-major order]]\n* [[Scalable locality]]\n* [[Scratchpad memory]]\n* [[Working set]]\n\n== References ==\n{{Reflist}}\n\n== Bibliography ==\n* [[Peter J. Denning]], [http://denninginstitute.com/pjd/PUBS/CACMcols/cacmJul05.pdf \"The Locality Principle\"], ''Communications of the ACM'', Volume 48, Issue 7, (2005), Pages 19\u201324\n* Peter J. Denning, Stuart C. Schwartz, [http://denninginstitute.com/pjd/PUBS/WSProp_1972.pdf \"Properties of the Working-Set Model\"], ''Communications of the ACM'',  Volume 15,  Issue 3  (March 1972), Pages 191-198\n\n{{DEFAULTSORT:Locality Of Reference}}\n[[Category:Computer memory]]\n[[Category:Cache (computing)]]\n[[Category:Software optimization]]\n", "name_user": "Shwe pan kone", "label": "unsafe", "comment": "", "url_page": "//en.wikipedia.org/wiki/Locality_of_reference"}
