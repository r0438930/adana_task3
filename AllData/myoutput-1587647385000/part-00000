{"title_page": "Survey data collection", "text_new": "With the application of probability [[Sampling (statistics)|sampling]] in the 1930s, surveys became a standard tool for [[empirical research]] in [[social sciences]], [[marketing]], and official statistics.<ref name=\"Vehovar 2008\">\n{{cite book\n  |last1=Vehovar|first1=V.\n  |last2=Lozar Manfreda|first2=K.\n  |year=2008\n  |chapter=Overview: Online Surveys\n  |title=The SAGE Handbook of Online Research Methods\n  |editor1-first=N.|editor1-last=Fielding\n  |editor2-first=R. M.|editor2-last=Lee\n  |editor3-first=G.|editor3-last=Blank\n  |pages=177\u2013194\n  |location=London|publisher=SAGE\n  |isbn=978-1-4129-2293-7\n}}</ref> The methods involved in '''survey data collection''' are any of a number of ways in which data can be [[data collection|collected]] for a [[Survey methodology|statistical survey]]. These are methods that are used to collect information from a sample of individuals in a systematic way. First there was the change from traditional paper-and-pencil interviewing (PAPI) to computer-assisted interviewing (CAI). Now, face-to-face surveys (CAPI), telephone surveys ([[CATI]]), and mail surveys (CASI, CSAQ) are increasingly replaced by web surveys.<ref name=\"Bethlehem 2012\">\n{{cite book\n  |last1=Bethlehem|first1=J.\n  |last2=Biffignandi|first2=S.\n  |year=2012\n  |title=Handbook of Web Surveys\n  |series=Wiley Handbooks in Survey Methodology\n  |volume=567\n  |location=New Jersey|publisher=John Wiley & Sons\n  |isbn=978-1-118-12172-6\n}}</ref>\n\n=='''Modes of data collection'''==\nThere are several ways of administering a survey.<ref>{{cite web |url = http://www.surveysoftware.info/en/ |title = The essential guide to CATI CAWI CAPI methodologies |publisher = }}</ref> Within a survey, different methods can be used for different parts. For example, interviewer administration can be used for general topics but self-administration for sensitive topics. The choice between administration modes is influenced by several factors, including 1) costs, 2) coverage of the target population (including group-specific preferences for certain modes<ref>{{Cite journal|last=Agley|first=Jon|last2=Meyerson|first2=Beth|last3=Eldridge|first3=Lori|last4=Smith|first4=Carriann|last5=Arora|first5=Prachi|last6=Richardson|first6=Chanel|last7=Miller|first7=Tara|date=February 2019|title=Just the fax, please: Updating electronic/hybrid methods for surveying pharmacists|journal=Research in Social and Administrative Pharmacy|language=en|volume=15|issue=2|pages=226\u2013227|doi=10.1016/j.sapharm.2018.10.028|pmid=30416040}}</ref>), 3) flexibility of asking questions, 4) respondents\u2019 willingness to participate and 5) response accuracy. Different methods create [[Lance Price|mode effects]] that change how respondents answer. The most common modes of administration are listed under the following headings.<ref>\n{{cite book\n  |last=[[Gideon J. Mellenbergh|Mellenbergh, G.J.]]|year=2008\n  |chapter=Surveys\n  |editor1-first=H.J.|editor1-last=Ad\u00e8r |editor-link1 = Herman J. Ad\u00e8r\n  |editor2-first=G.J.|editor2-last=Mellenbergh \n  |title=Advising on Research Methods: A consultant's companion\n  |pages=183\u2013209\n  |location=Huizen, The Netherlands|publisher=Johannes van Kessel Publishing\n  |isbn=978-90-79418-01-5\n}}</ref>\n\n==='''''Mobile surveys'''''===\nMobile data collection or mobile surveys is an increasingly popular method of data collection. Over 50% of surveys today are opened on mobile devices.<ref>{{cite web |url = http://www.questback.com/online-customer-surveys |title = Mobile-ready. Event driven. Feature rich. Online customer surveys |publisher = [[QuestBack]] |archiveurl = https://www.webcitation.org/6cTYw0aQt?url=http://www.questback.com/online-customer-surveys |archivedate = 22 October 2015 |url-status = live }}</ref> The survey, form, app or collection tool is on a mobile device such as a smart phone or a tablet. These devices offer innovative ways to gather data, and eliminate the laborious \"data entry\" (of paper form data into a computer), which delays data analysis and understanding. By eliminating paper, mobile data collection can also dramatically reduce costs: one World Bank study in Guatemala found a 71% decrease in cost while using mobile data collection, compared to the previous paper-based approach.<ref>{{cite web|last1=Schuster|first1=Christian|last2=Perez Brito|first2=Carlos|url=http://home.magpi.com/case-study/evaluating-cash-transfers-in-guatemala/| title = Evaluating Cash Transfers in Guatemala |website=Magpi|accessdate=27 November 2016}}</ref>\n\nSMS surveys can reach any handset, in any language and in any country. As they are not dependent on internet access and the answers can be sent when its convenient, they are a suitable mobile survey data collection channel for many situations that require fast, high volume responses. As a result, SMS surveys can deliver 80% of responses in less than 2 hours <ref>{{cite web|last1=Global|first1=OnePoint|url=http://www.onepointglobal.com/MobileSurveys/SMSsurveys| title = SMS surveys |website=OnePoint Global|accessdate=27 June 2016}}</ref> and often at much lower cost compared to face-to-face surveys, due to the elimination of travel/personnel costs.<ref>{{cite web|last1=Selanikio|first1=Joel|url=http://home.magpi.com/more-data-for-less-money/| title = Getting More Data for Less Money |website=Magpi|accessdate=9 November 2016}}</ref>\n\nApart from the high mobile phone penetration,<ref>Revilla, M., Toninelli, D., Ochoa, C., and G. Loewe (2015). \u201cWho has access to mobile devices in an online opt-in panel? An analysis of potential respondents for mobile surveys\u201d. In D. Toninelli, R. Pinter, and P. de Pedraza (eds), Mobile Research Methods: Opportunities and challenges of mobile research methodologies, pp. 119-139 (Chapter 8). London: Ubiquity Press. {{ISBN|978-1-909188-53-2}}. DOI: https://dx.doi.org/10.5334/bar.h. License: CC-BY 4.0.</ref><ref>{{cite journal|url=http://www.surveypractice.org/index.php/SurveyPractice/article/view/250|title=Do You Know Which Device Your Respondent Has Used to Take Your Online Survey?|first=Mario|last=Callegaro|date=3 October 2013|journal=Survey Practice|volume=3|issue=6|via=www.surveypractice.org}}</ref> further advantages are quicker response times and the possibility to reach previously hard-to-reach target groups. In this way, mobile technology allows marketers, researchers and employers to create real and meaningful mobile engagement in environments different from the traditional one in front of a desktop computer.<ref>{{cite web |url = http://surveyanyplace.com/why-mobile-surveys/ |title = Mobile engagement becomes standard operating procedure |publisher = Survey Anyplace |url-status = dead |archiveurl = https://web.archive.org/web/20140208043428/http://surveyanyplace.com/why-mobile-surveys/ |archivedate = 2014-02-08 }}</ref><ref>{{cite journal|url = http://homepage.univie.ac.at/andreas.hergovich/php/reaching_the_mobile_respondent_soc.sci.comp.rev.pdf |title = Reaching the Mobile Respondent: Determinants of High-Level Mobile Phone Use Among a High-Coverage Group  |journal = Social Science Computer Review  |volume = 28  |issue = 3  |pages = 336\u2013349  |doi = 10.1177/0894439309353099|year = 2010  |last1 = Burger  |first1 = Christoph  |last2 = Riemer  |first2 = Valentin  |last3 = Grafeneder  |first3 = J\u00fcrgen  |last4 = Woisetschl\u00e4ger  |first4 = Bianca  |last5 = Vidovic  |first5 = Dragana  |last6 = Hergovich  |first6 = Andreas  }}</ref> However, even when using mobile devices to answer the web surveys, most respondents still answer from home.<ref>{{cite journal|title=Sensitive Topics in PC Web and Mobile Web Surveys: Is There a Difference?|first1=Aigul|last1=Mavletova|first2=Mick P.|last2=Couper|date=22 November 2013|journal=Survey Research Methods|volume=7|issue=3|pages=191\u2013205|doi=10.18148/srm/2013.v7i3.5458}}</ref><ref>{{cite journal | last1 = Toninelli | first1 = D. | last2 = Revilla | first2 = M. | year = 2016 | title = Smartphones vs PCs: Does the Device Affect the Web Survey Experience and the Measurement Error for Sensitive Topics? A Replication of the Mavletova & Couper's 2013 Experiment | url = | journal = Survey Research Methods | volume = 10 | issue = 2| pages = 153\u2013169 | doi = 10.18148/srm/2016.v10i2.6274 }}</ref>\n\n===Online surveys===\nOnline ([[Internet]]) surveys are becoming an essential research tool for a variety of research fields, including marketing, social and official statistics research. According to [[ESOMAR]] online survey research accounted for 20% of global data-collection expenditure in 2006.<ref name=\"Vehovar 2008\" /> They offer capabilities beyond those available for any other type of self-administered questionnaire.<ref name=\"Dillman 2006\" /> [[Custom online panel|Online consumer panels]] are also used extensively for carrying out surveys but the quality is considered inferior because the panelists are regular contributors and tend to be fatigued. However, when estimating the measurement quality (defined as product of reliability and validity) using a [[Multitrait-multimethod matrix|multitrait-mutlimethod approach (MTMM)]],  some studies found a quite reasonable quality<ref>{{cite journal|url=https://journals.sub.uni-hamburg.de/giga/jpla/article/viewArticle/903|title=Quality of Different Scales in an Online Survey in Mexico and Colombia|first1=Melanie|last1=Revilla|first2=Carlos|last2=Ochoa|date=14 December 2015|journal=Journal of Politics in Latin America|volume=7|issue=3|pages=157\u2013177|via=journals.sub.uni-hamburg.de|doi=10.1177/1866802X1500700305}}</ref><ref>{{cite web|url=http://www.press.uchicago.edu/ucp/books/book/distributed/S/bo22196267.html|title=Revilla, M., and W.E. Saris (2015). \"Estimating and comparing the quality of different scales of an online survey using an MTMM approach\". In Engel, U. (Ed), Survey Measurements: Techniques, Data Quality and sources of Error. Chapter 5, pp. 53-74. Campus. Frankfurt. New York. ISBN 9783593502809. Available at press.uchicago.edu|publisher=}}</ref> and even that the quality of a series of questions in an online opt-in panel (Netquest) was very similar to the measurement quality for the same questions asked in the [[European Social Survey|European Social Survey (ESS)]], which is a face-to-face survey.<ref>{{cite journal|url=https://www.mrs.org.uk/ijmr_article/article/104501|title=Can a non-probabilistic online panel achieve question quality similar to that of the European Social Survey?|first1=Melanie|last1=Revilla|first2=Willem|last2=Saris|first3=Germ\u00e1n|last3=Loewe|first4=Carlos|last4=Ochoa|date=26 May 2015|journal=International Journal of Market Research|volume=57|issue=3|pages=395\u2013412|doi=10.2501/IJMR-2015-034}}</ref>\n\n[[File:US Navy 030618-N-2893B-001 Information Technician 1st Class Annette Leasure takes a few minutes to fill out the BUPERS Online Uniform Survey Questionnaire.jpg|thumb|US Navy 030618-N-2893B-001 Information Technician 1st Class Annette Leasure takes a few minutes to fill out the BUPERS Online Uniform Survey Questionnaire]]\nSome studies have compared the quality of face-to-face surveys and/or telephone surveys with that of online surveys, for single questions, but also for more complex concepts measured with more than one question (also called Composite Scores or Index).<ref>Revilla, M. (2015). \u201cComparison of the quality estimates in a mixed-mode and a unimode design: an experiment from the European Social Survey\u201d, Quality and Quantity. 2015, 49(3): 1219-1238. Published online first 13 of June 2014. DOI: 10.1007/s11135-014-0044-5</ref><ref>{{cite journal|title=Measurement invariance and quality of composite scores in a face-to-face and a web survey|first=Melanie A.|last=Revilla|date=30 December 2012|journal=Survey Research Methods|volume=7|issue=1|pages=17\u201328|doi=10.18148/srm/2013.v7i1.5098}}</ref><ref>{{cite journal|title=Quality in Unimode and Mixed-Mode designs: A Multitrait-Multimethod approach|first=Melanie|last=Revilla|date=31 December 2010|journal=Survey Research Methods|volume=4|issue=3|pages=151\u2013164|doi=10.18148/srm/2010.v4i3.4278}}</ref> Focusing only on probability-based surveys (also for the online ones), they found overall that the face-to-face (using show-cards) and web surveys have quite similar levels of measurement quality, whereas the telephone surveys were performing worse.\nOther studies comparing paper-and-pencil questionnaires with web-based questionnaires showed that employees preferred online survey approaches to the paper-and-pencil format.\nThere are also concerns about what has been called \"ballot stuffing\" in which employees make repeated responses to the same survey. Some employees are also concerned about privacy. Even if they do not provide their names when responding to a company survey, can they be certain that their anonymity is protected? Such fears prevent some employees from expressing an opinion.<ref>{{cite book|last=Schultz & Schultz|first=Duane|title=Psychology and work today|year=2010|publisher=Prentice Hall|location=New York|isbn=978-0-205-68358-1|pages=40}}</ref>\n\n====Advantages of online surveys====\n* Web surveys are faster, simpler, and cheaper.<ref name=\"Bethlehem 2012\" /> However, lower costs are not so straightforward in practice, as they are strongly interconnected to errors. Because response rate comparisons to other survey modes are usually not favourable for online surveys, efforts to achieve a higher response rate (e.g., with traditional solicitation methods) may substantially increase costs.<ref name=\"Vehovar 2008\" />\n* The entire data collection period is significantly shortened, as all data can be collected and processed in little more than a month.<ref name=\"Bethlehem 2012\" />\n* Interaction between the respondent and the questionnaire is more dynamic compared to e-mail or paper surveys.<ref name=\"Dillman 2006\">\n{{cite book\n  |last=Dillman|first=D.A.\n  |year=2006\n  |title=Mail and Internet Surveys: The Tailored Design Method\n  |edition=2nd\n  |location=New Jersey|publisher=John Wiley & Sons\n  |isbn=978-0-470-03856-7\n}}</ref> Online surveys are also less intrusive, and they suffer less from social desirability effects.<ref name=\"Bethlehem 2012\" />\n* Complex skip patterns can be implemented in ways that are mostly invisible to the respondent.<ref name=\"Dillman 2006\" />\n* Pop-up instructions can be provided for individual questions to provide help with questions exactly where assistance is required.<ref name=\"Dillman 2006\" />\n* Questions with long lists of answer choices can be used to provide immediate coding of answers to certain questions that are usually asked in an open-ended fashion in paper questionnaires.<ref name=\"Dillman 2006\" />\n* Online surveys can be tailored to the situation (e.g., respondents may be allowed save a partially completed form, the questionnaire may be preloaded with already available information, etc.).<ref name=\"Bethlehem 2012\" />\n* Online questionnaires may be improved by applying usability testing, where usability is measured with reference to the speed with which a task can be performed, the frequency of errors and user satisfaction with the interface.<ref name=\"Bethlehem 2012\" />\n\n====Key methodological issues of online surveys====\n* '''[[Sampling (statistics)|Sampling]]'''. The difference between probability samples (where the inclusion probabilities for all units of the target population is known in advance) and non-probability samples (which often require less time and effort but generally do not support statistical inference) is crucial. Probability samples are highly affected by problems of non-coverage (not all members of the general population have Internet access) and frame problems (online survey invitations are most conveniently distributed using e-mail, but there are no e-mail directories of the general population that might be used as a sampling frame). Because coverage and frame problems can significantly impact data quality, they should be adequately reported when disseminating the research results.<ref name=\"Vehovar 2008\" /><ref name=\"Wright\">{{cite journal|last1=Wright|first1=Kevin|title=Researching Internet-Based Populations: Advantages and Disadvantages of Online Survey Research, Online Questionnaire Authoring Software Packages, and Web Survey Services|journal=Journal of Computer-Mediated Communication|date=1 April 2005|volume=10|issue=3|pages=1034|url=https://academic.oup.com/jcmc/article/10/3/JCMC1034/4614509|accessdate=6 March 2018}}</ref>\n* '''Invitations''' to online surveys. Due to the lack of sampling frames many online survey invitations are published in the form of an URL link on web sites or in other media, which leads to sample selection bias that is out of research control and to non-probability samples. Traditional solicitation modes, such as telephone or mail invitations to web surveys, can help overcoming probability sampling issues in online surveys. However, such approaches are faced with problems of dramatically higher costs and questionable effectiveness.<ref name=\"Vehovar 2008\" />\n* '''[[Response rate (survey)|Non-response]]'''. Online survey response rates are generally low and also vary extremely \u2013 from less than 1% in enterprise surveys with e-mail invitations to almost 100% in specific membership surveys. In addition to refusing participation, terminating surveying during the process or not answering certain questions, several other non-response patterns can be observed in online surveys, such as lurking respondents and a combination of partial and item non-response.  Response rates can be increased by offering monetary or some other type of incentive to the respondents, by contacting respondents several times (follow-up), and by keeping the questionnaire difficulty as low as possible.<ref name=\"Vehovar 2008\" />  There are draw-backs to using an incentive to garner a response.  Non-bias responses could be questioned in this type of situation.  The most concrete way to gain feedback is to publicize what is done with the results.  To take concrete actions based on feedback and to show that to the customer base is extremely motivating to customers to continue to let their voice be heard.   \n* '''Acquiescence bias'''. Due to a phenomenon inherently present in human nature, many people have acquiescent personalities and are more likely to agree with statements than disagree - regardless of the content. Often, those people see the question-asker as an expert in their field which causes them to be more likely to react positively to the question asked. That being said, acquiescence bias (also known as the friendliness bias or \u201cyea-saying\u201d) manifests itself when a respondent shows a tendency to agree with whatever it is that you are asking or stating, even though they might not actually agree.<ref>Jovancic, Nemanja. [https://www.leadquizzes.com/blog/types-of-bias-in-research/ \"4 Types of Bias in Research and How to Make Your Surveys Bias-Free\"]. ''LeadQuizzes''. Retrieved March 16, 2020.</ref>\n* '''Platform Issues'''. Lack of familiarity with the platform used can cause participants and clients confusion.{{citation needed|date=April 2017}}\n* '''Questionnaire design'''. While modern web questionnaires offer a range of design features (different question types, images, multimedia), the use of such elements should be limited to the extent necessary for respondents to understand questions or to stimulate the response. It should not affect their responses, because that would mean lower validity and reliability of data. Appropriate questionnaire design can help lowering the measurement error that can arise also due to the respondents or the survey mode itself (respondent\u2019s motivation, computer literacy, abilities, privacy concerns, etc.).<ref name=\"Vehovar 2008\" />\n* '''Post-survey adjustments'''. Various robust procedures have been developed for situations where sampling deviate from probability selection, or, when we face non-coverage and non-response problems. The standard statistical inference procedures (e.g. confidence interval calculations and hypothesis testing) still require a probability sample. The actual survey practice, particularly in marketing research and in public opinion polling, which massively neglects the principles of probability samples, increasingly requires from the statistical profession to specify the conditions where non-probability samples may work.<ref name=\"Vehovar 2008\" />\n\nThese issues, and potential remedies, are discussed in a number of sources.<ref>Salant, Priscilla, and Don A. Dillman. \"How to Conduct your own Survey: Leading professional give you proven techniques for getting reliable results.\" (1995).</ref><ref>Kalton, Graham. Introduction to survey sampling. Vol. 35. Sage, 1983.</ref>\n\n===Telephone===\n{{Prose|section|date=January 2012}}\n\n* Use of interviewers encourages sample persons to respond, leading to higher response rates.<ref>\n{{cite book\n  |last=Groves|first=R.M.\n  |year=1989\n  |title=Survey Costs and Survey Errors\n  |location=New York|publisher=Wiley\n  |isbn=978-0-471-67851-9\n}}</ref>\n* Interviewers can increase comprehension of questions by answering respondents' questions.\n* Fairly cost efficient, depending on local call charge structure\n* Good for large national (or international) [[sampling frame]]s\n* Some potential for interviewer bias (e.g., some people may be more willing to discuss a sensitive issue with a female interviewer than with a male one)\n* Cannot be used for non-audio information (graphics, demonstrations, taste/smell samples)\n* Three types:\n** Traditional telephone interviews\n** Computer assisted telephone dialing\n** [[Computer assisted telephone interviewing]] ([[CATI]])\n\n===Mail===\n* The questionnaire may be handed to the respondents or mailed to them, but in all cases they are returned to the researcher via mail.\n*An advantage is, is that cost is very low, since bulk postage is cheap in most countries\n* Long delays, often several months, before the surveys are returned and statistical analysis can begin\n* Not suitable for issues that may require clarification\n* Respondents can answer at their own convenience (allowing them to break up long surveys; also useful if they need to check records to answer a question)\n* No interviewer bias\n* Non-response bias can be corrected by extrapolation across waves<ref>{{cite journal | url = http://marketing.wharton.upenn.edu/ideas/pdf/Armstrong/EstimatingNonresponseBias.pdf | title = Estimating Nonresponse Bias in Mail Surveys | author = J. Scott Armstrong and Terry S. Overton | journal = Journal of Marketing Research | volume = 14 | issue = 3 | pages = 396\u2013402 | year = 1977 | doi = 10.2307/3150783 | url-status = dead | archiveurl = https://web.archive.org/web/20100620200022/http://marketing.wharton.upenn.edu/ideas/pdf/Armstrong/EstimatingNonresponseBias.pdf | archivedate = 2010-06-20 | jstor = 3150783 | citeseerx = 10.1.1.36.7783 }}</ref>\n* Large amount of information can be obtained: some mail surveys are as long as 50 pages\n* Response rates can be improved by using mail panels\n* Response rates can be improved by using prepaid monetary incentives<ref>{{cite journal|url=http://www.forecastingprinciples.com/paperpdf/Monetary%20Incentives.pdf | title = Monetary Incentives in Mail Surveys | author = J. Scott Armstrong| journal = Public Opinion Quarterly | volume = 39 | pages = 111\u2013116 | year = 1975 | doi=10.1086/268203}}</ref>\n* Response rates are affected by the class of mail through which the survey was sent<ref>{{cite journal|url=http://www.forecastingprinciples.com/paperpdf/Class%20of%20Mail.pdf | title = Class of Mail Does Affect Response Rates to Mailed Questionnaires: Evidence from Meta-Analysis (with a Reply by Lee Harvey)| author = J. Scott Armstrong| journal = Journal of the Market Research Society | volume = 32 | pages = 469\u2013472 | year = 1990}}</ref>\n** Members of the panel have agreed to participate\n** Panels can be used in longitudinal designs where the same respondents are surveyed several times\n\n===Face-to-face===\n* Suitable for locations where telephone or mail are not developed\n* Potential for interviewer bias\n* Easy to manipulate by completing multiple times to skew results\n\n===Mixed-mode surveys===\nResearchers can combine several above methods for the data collection. For example, researchers can invite shoppers at malls, and send willing participants questionnaires by emails. With the introduction of computers to the survey process, survey mode now includes combinations of different approaches or mixed-mode designs. Some of the most common methods are:<ref name=\"Groves 2009\">\n{{cite book\n  |last1=Groves|first1=R.M.\n  |last2=Fowler|first2=F. J.\n  |last3=Couper|first3=M.P.\n  |last4=Lepkowski|first4=J.M.\n  |last5=Singer|first5=E.\n  |last6=Tourangeau|first6=R.\n  |year=2009\n  |title=Survey Methodology\n  |location=New Jersey|publisher=John Wiley & Sons\n  |isbn=978-1-118-21134-2\n}}</ref>\n*Computer-assisted personal interviewing (CAPI): The computer displays the questions on screen, the interviewer reads them to the respondent, and then enters the respondent's answers.\n*Audio computer-assisted self-interviewing (audio CASI): The respondent operates the computer, the computer displays the question on the screen and plays recordings of the questions to the respondents, who then enters his/her answers.\n*Computer-assisted telephone interviewing (CATI)\n*Interactive voice response (IVR): The computer plays recordings of the questions to respondents over the telephone, who then respond by using the keypad of the telephone or speaking their answers aloud.\n*Web surveys: The computer administers the questions online.\n\n==See also==\n{{wikiversity|Survey research and design in psychology/Assessment/Data collection and entry/Survey administration}}\n* [[Assessment (disambiguation)|Assessment]]\n* [[Comparison of survey software]]\n* [[Data collection system]]\n{{clear}}\n\n==References==\n{{reflist}}\n\n[[Category:Survey methodology|*]]\n", "text_old": "With the application of probability [[Sampling (statistics)|sampling]] in the 1930s, surveys became a standard tool for [[empirical research]] in [[social sciences]], [[marketing]], and official statistics.<ref name=\"Vehovar 2008\">\n{{cite book\n  |last1=Vehovar|first1=V.\n  |last2=Lozar Manfreda|first2=K.\n  |year=2008\n  |chapter=Overview: Online Surveys\n  |title=The SAGE Handbook of Online Research Methods\n  |editor1-first=N.|editor1-last=Fielding\n  |editor2-first=R. M.|editor2-last=Lee\n  |editor3-first=G.|editor3-last=Blank\n  |pages=177\u2013194\n  |location=London|publisher=SAGE\n  |isbn=978-1-4129-2293-7\n}}</ref> The methods involved in '''survey data collection''' are any of a number of ways in which data can be [[data collection|collected]] for a [[Survey methodology|statistical survey]]. These are methods that are used to collect information from a sample of individuals in a systematic way. First there was the change from traditional paper-and-pencil interviewing (PAPI) to computer-assisted interviewing (CAI). Now, face-to-face surveys (CAPI), telephone surveys ([[CATI]]), and mail surveys (CASI, CSAQ) are increasingly replaced by web surveys.<ref name=\"Bethlehem 2012\">\n{{cite book\n  |last1=Bethlehem|first1=J.\n  |last2=Biffignandi|first2=S.\n  |year=2012\n  |title=Handbook of Web Surveys\n  |series=Wiley Handbooks in Survey Methodology\n  |volume=567\n  |location=New Jersey|publisher=John Wiley & Sons\n  |isbn=978-1-118-12172-6\n}}</ref>\n\n==Modes of data collection==\nThere are several ways of administering a survey.<ref>{{cite web |url = http://www.surveysoftware.info/en/ |title = The essential guide to CATI CAWI CAPI methodologies |publisher = }}</ref> Within a survey, different methods can be used for different parts. For example, interviewer administration can be used for general topics but self-administration for sensitive topics. The choice between administration modes is influenced by several factors, including 1) costs, 2) coverage of the target population (including group-specific preferences for certain modes<ref>{{Cite journal|last=Agley|first=Jon|last2=Meyerson|first2=Beth|last3=Eldridge|first3=Lori|last4=Smith|first4=Carriann|last5=Arora|first5=Prachi|last6=Richardson|first6=Chanel|last7=Miller|first7=Tara|date=February 2019|title=Just the fax, please: Updating electronic/hybrid methods for surveying pharmacists|journal=Research in Social and Administrative Pharmacy|language=en|volume=15|issue=2|pages=226\u2013227|doi=10.1016/j.sapharm.2018.10.028|pmid=30416040}}</ref>), 3) flexibility of asking questions, 4) respondents\u2019 willingness to participate and 5) response accuracy. Different methods create [[mode effect]]s that change how respondents answer. The most common modes of administration are listed under the following headings.<ref>\n{{cite book\n  |last=[[Gideon J. Mellenbergh|Mellenbergh, G.J.]]|year=2008\n  |chapter=Surveys\n  |editor1-first=H.J.|editor1-last=Ad\u00e8r |editor-link1 = Herman J. Ad\u00e8r\n  |editor2-first=G.J.|editor2-last=Mellenbergh \n  |title=Advising on Research Methods: A consultant's companion\n  |pages=183\u2013209\n  |location=Huizen, The Netherlands|publisher=Johannes van Kessel Publishing\n  |isbn=978-90-79418-01-5\n}}</ref>\n\n===Mobile surveys===\nMobile data collection or mobile surveys is an increasingly popular method of data collection. Over 50% of surveys today are opened on mobile devices.<ref>{{cite web |url = http://www.questback.com/online-customer-surveys |title = Mobile-ready. Event driven. Feature rich. Online customer surveys |publisher = [[QuestBack]] |archiveurl = https://www.webcitation.org/6cTYw0aQt?url=http://www.questback.com/online-customer-surveys |archivedate = 22 October 2015 |url-status = live }}</ref> The survey, form, app or collection tool is on a mobile device such as a smart phone or a tablet. These devices offer innovative ways to gather data, and eliminate the laborious \"data entry\" (of paper form data into a computer), which delays data analysis and understanding. By eliminating paper, mobile data collection can also dramatically reduce costs: one World Bank study in Guatemala found a 71% decrease in cost while using mobile data collection, compared to the previous paper-based approach.<ref>{{cite web|last1=Schuster|first1=Christian|last2=Perez Brito|first2=Carlos|url=http://home.magpi.com/case-study/evaluating-cash-transfers-in-guatemala/| title = Evaluating Cash Transfers in Guatemala |website=Magpi|accessdate=27 November 2016}}</ref>\n\nSMS surveys can reach any handset, in any language and in any country. As they are not dependent on internet access and the answers can be sent when its convenient, they are a suitable mobile survey data collection channel for many situations that require fast, high volume responses. As a result, SMS surveys can deliver 80% of responses in less than 2 hours <ref>{{cite web|last1=Global|first1=OnePoint|url=http://www.onepointglobal.com/MobileSurveys/SMSsurveys| title = SMS surveys |website=OnePoint Global|accessdate=27 June 2016}}</ref> and often at much lower cost compared to face-to-face surveys, due to the elimination of travel/personnel costs.<ref>{{cite web|last1=Selanikio|first1=Joel|url=http://home.magpi.com/more-data-for-less-money/| title = Getting More Data for Less Money |website=Magpi|accessdate=9 November 2016}}</ref>\n\nApart from the high mobile phone penetration,<ref>Revilla, M., Toninelli, D., Ochoa, C., and G. Loewe (2015). \u201cWho has access to mobile devices in an online opt-in panel? An analysis of potential respondents for mobile surveys\u201d. In D. Toninelli, R. Pinter, and P. de Pedraza (eds), Mobile Research Methods: Opportunities and challenges of mobile research methodologies, pp. 119-139 (Chapter 8). London: Ubiquity Press. {{ISBN|978-1-909188-53-2}}. DOI: https://dx.doi.org/10.5334/bar.h. License: CC-BY 4.0.</ref><ref>{{cite journal|url=http://www.surveypractice.org/index.php/SurveyPractice/article/view/250|title=Do You Know Which Device Your Respondent Has Used to Take Your Online Survey?|first=Mario|last=Callegaro|date=3 October 2013|journal=Survey Practice|volume=3|issue=6|via=www.surveypractice.org}}</ref> further advantages are quicker response times and the possibility to reach previously hard-to-reach target groups. In this way, mobile technology allows marketers, researchers and employers to create real and meaningful mobile engagement in environments different from the traditional one in front of a desktop computer.<ref>{{cite web |url = http://surveyanyplace.com/why-mobile-surveys/ |title = Mobile engagement becomes standard operating procedure |publisher = Survey Anyplace |url-status = dead |archiveurl = https://web.archive.org/web/20140208043428/http://surveyanyplace.com/why-mobile-surveys/ |archivedate = 2014-02-08 }}</ref><ref>{{cite journal|url = http://homepage.univie.ac.at/andreas.hergovich/php/reaching_the_mobile_respondent_soc.sci.comp.rev.pdf |title = Reaching the Mobile Respondent: Determinants of High-Level Mobile Phone Use Among a High-Coverage Group  |journal = Social Science Computer Review  |volume = 28  |issue = 3  |pages = 336\u2013349  |doi = 10.1177/0894439309353099|year = 2010  |last1 = Burger  |first1 = Christoph  |last2 = Riemer  |first2 = Valentin  |last3 = Grafeneder  |first3 = J\u00fcrgen  |last4 = Woisetschl\u00e4ger  |first4 = Bianca  |last5 = Vidovic  |first5 = Dragana  |last6 = Hergovich  |first6 = Andreas  }}</ref> However, even when using mobile devices to answer the web surveys, most respondents still answer from home.<ref>{{cite journal|title=Sensitive Topics in PC Web and Mobile Web Surveys: Is There a Difference?|first1=Aigul|last1=Mavletova|first2=Mick P.|last2=Couper|date=22 November 2013|journal=Survey Research Methods|volume=7|issue=3|pages=191\u2013205|doi=10.18148/srm/2013.v7i3.5458}}</ref><ref>{{cite journal | last1 = Toninelli | first1 = D. | last2 = Revilla | first2 = M. | year = 2016 | title = Smartphones vs PCs: Does the Device Affect the Web Survey Experience and the Measurement Error for Sensitive Topics? A Replication of the Mavletova & Couper's 2013 Experiment | url = | journal = Survey Research Methods | volume = 10 | issue = 2| pages = 153\u2013169 | doi = 10.18148/srm/2016.v10i2.6274 }}</ref>\n\n===Online surveys===\nOnline ([[Internet]]) surveys are becoming an essential research tool for a variety of research fields, including marketing, social and official statistics research. According to [[ESOMAR]] online survey research accounted for 20% of global data-collection expenditure in 2006.<ref name=\"Vehovar 2008\" /> They offer capabilities beyond those available for any other type of self-administered questionnaire.<ref name=\"Dillman 2006\" /> [[Custom online panel|Online consumer panels]] are also used extensively for carrying out surveys but the quality is considered inferior because the panelists are regular contributors and tend to be fatigued. However, when estimating the measurement quality (defined as product of reliability and validity) using a [[Multitrait-multimethod matrix|multitrait-mutlimethod approach (MTMM)]],  some studies found a quite reasonable quality<ref>{{cite journal|url=https://journals.sub.uni-hamburg.de/giga/jpla/article/viewArticle/903|title=Quality of Different Scales in an Online Survey in Mexico and Colombia|first1=Melanie|last1=Revilla|first2=Carlos|last2=Ochoa|date=14 December 2015|journal=Journal of Politics in Latin America|volume=7|issue=3|pages=157\u2013177|via=journals.sub.uni-hamburg.de|doi=10.1177/1866802X1500700305}}</ref><ref>{{cite web|url=http://www.press.uchicago.edu/ucp/books/book/distributed/S/bo22196267.html|title=Revilla, M., and W.E. Saris (2015). \"Estimating and comparing the quality of different scales of an online survey using an MTMM approach\". In Engel, U. (Ed), Survey Measurements: Techniques, Data Quality and sources of Error. Chapter 5, pp. 53-74. Campus. Frankfurt. New York. ISBN 9783593502809. Available at press.uchicago.edu|publisher=}}</ref> and even that the quality of a series of questions in an online opt-in panel (Netquest) was very similar to the measurement quality for the same questions asked in the [[European Social Survey|European Social Survey (ESS)]], which is a face-to-face survey.<ref>{{cite journal|url=https://www.mrs.org.uk/ijmr_article/article/104501|title=Can a non-probabilistic online panel achieve question quality similar to that of the European Social Survey?|first1=Melanie|last1=Revilla|first2=Willem|last2=Saris|first3=Germ\u00e1n|last3=Loewe|first4=Carlos|last4=Ochoa|date=26 May 2015|journal=International Journal of Market Research|volume=57|issue=3|pages=395\u2013412|doi=10.2501/IJMR-2015-034}}</ref>\n\n[[File:US Navy 030618-N-2893B-001 Information Technician 1st Class Annette Leasure takes a few minutes to fill out the BUPERS Online Uniform Survey Questionnaire.jpg|thumb|US Navy 030618-N-2893B-001 Information Technician 1st Class Annette Leasure takes a few minutes to fill out the BUPERS Online Uniform Survey Questionnaire]]\nSome studies have compared the quality of face-to-face surveys and/or telephone surveys with that of online surveys, for single questions, but also for more complex concepts measured with more than one question (also called Composite Scores or Index).<ref>Revilla, M. (2015). \u201cComparison of the quality estimates in a mixed-mode and a unimode design: an experiment from the European Social Survey\u201d, Quality and Quantity. 2015, 49(3): 1219-1238. Published online first 13 of June 2014. DOI: 10.1007/s11135-014-0044-5</ref><ref>{{cite journal|title=Measurement invariance and quality of composite scores in a face-to-face and a web survey|first=Melanie A.|last=Revilla|date=30 December 2012|journal=Survey Research Methods|volume=7|issue=1|pages=17\u201328|doi=10.18148/srm/2013.v7i1.5098}}</ref><ref>{{cite journal|title=Quality in Unimode and Mixed-Mode designs: A Multitrait-Multimethod approach|first=Melanie|last=Revilla|date=31 December 2010|journal=Survey Research Methods|volume=4|issue=3|pages=151\u2013164|doi=10.18148/srm/2010.v4i3.4278}}</ref> Focusing only on probability-based surveys (also for the online ones), they found overall that the face-to-face (using show-cards) and web surveys have quite similar levels of measurement quality, whereas the telephone surveys were performing worse.\nOther studies comparing paper-and-pencil questionnaires with web-based questionnaires showed that employees preferred online survey approaches to the paper-and-pencil format.\nThere are also concerns about what has been called \"ballot stuffing\" in which employees make repeated responses to the same survey. Some employees are also concerned about privacy. Even if they do not provide their names when responding to a company survey, can they be certain that their anonymity is protected? Such fears prevent some employees from expressing an opinion.<ref>{{cite book|last=Schultz & Schultz|first=Duane|title=Psychology and work today|year=2010|publisher=Prentice Hall|location=New York|isbn=978-0-205-68358-1|pages=40}}</ref>\n\n====Advantages of online surveys====\n* Web surveys are faster, simpler, and cheaper.<ref name=\"Bethlehem 2012\" /> However, lower costs are not so straightforward in practice, as they are strongly interconnected to errors. Because response rate comparisons to other survey modes are usually not favourable for online surveys, efforts to achieve a higher response rate (e.g., with traditional solicitation methods) may substantially increase costs.<ref name=\"Vehovar 2008\" />\n* The entire data collection period is significantly shortened, as all data can be collected and processed in little more than a month.<ref name=\"Bethlehem 2012\" />\n* Interaction between the respondent and the questionnaire is more dynamic compared to e-mail or paper surveys.<ref name=\"Dillman 2006\">\n{{cite book\n  |last=Dillman|first=D.A.\n  |year=2006\n  |title=Mail and Internet Surveys: The Tailored Design Method\n  |edition=2nd\n  |location=New Jersey|publisher=John Wiley & Sons\n  |isbn=978-0-470-03856-7\n}}</ref> Online surveys are also less intrusive, and they suffer less from social desirability effects.<ref name=\"Bethlehem 2012\" />\n* Complex skip patterns can be implemented in ways that are mostly invisible to the respondent.<ref name=\"Dillman 2006\" />\n* Pop-up instructions can be provided for individual questions to provide help with questions exactly where assistance is required.<ref name=\"Dillman 2006\" />\n* Questions with long lists of answer choices can be used to provide immediate coding of answers to certain questions that are usually asked in an open-ended fashion in paper questionnaires.<ref name=\"Dillman 2006\" />\n* Online surveys can be tailored to the situation (e.g., respondents may be allowed save a partially completed form, the questionnaire may be preloaded with already available information, etc.).<ref name=\"Bethlehem 2012\" />\n* Online questionnaires may be improved by applying usability testing, where usability is measured with reference to the speed with which a task can be performed, the frequency of errors and user satisfaction with the interface.<ref name=\"Bethlehem 2012\" />\n\n====Key methodological issues of online surveys====\n* '''[[Sampling (statistics)|Sampling]]'''. The difference between probability samples (where the inclusion probabilities for all units of the target population is known in advance) and non-probability samples (which often require less time and effort but generally do not support statistical inference) is crucial. Probability samples are highly affected by problems of non-coverage (not all members of the general population have Internet access) and frame problems (online survey invitations are most conveniently distributed using e-mail, but there are no e-mail directories of the general population that might be used as a sampling frame). Because coverage and frame problems can significantly impact data quality, they should be adequately reported when disseminating the research results.<ref name=\"Vehovar 2008\" /><ref name=\"Wright\">{{cite journal|last1=Wright|first1=Kevin|title=Researching Internet-Based Populations: Advantages and Disadvantages of Online Survey Research, Online Questionnaire Authoring Software Packages, and Web Survey Services|journal=Journal of Computer-Mediated Communication|date=1 April 2005|volume=10|issue=3|pages=1034|url=https://academic.oup.com/jcmc/article/10/3/JCMC1034/4614509|accessdate=6 March 2018}}</ref>\n* '''Invitations''' to online surveys. Due to the lack of sampling frames many online survey invitations are published in the form of an URL link on web sites or in other media, which leads to sample selection bias that is out of research control and to non-probability samples. Traditional solicitation modes, such as telephone or mail invitations to web surveys, can help overcoming probability sampling issues in online surveys. However, such approaches are faced with problems of dramatically higher costs and questionable effectiveness.<ref name=\"Vehovar 2008\" />\n* '''[[Response rate (survey)|Non-response]]'''. Online survey response rates are generally low and also vary extremely \u2013 from less than 1% in enterprise surveys with e-mail invitations to almost 100% in specific membership surveys. In addition to refusing participation, terminating surveying during the process or not answering certain questions, several other non-response patterns can be observed in online surveys, such as lurking respondents and a combination of partial and item non-response.  Response rates can be increased by offering monetary or some other type of incentive to the respondents, by contacting respondents several times (follow-up), and by keeping the questionnaire difficulty as low as possible.<ref name=\"Vehovar 2008\" />  There are draw-backs to using an incentive to garner a response.  Non-bias responses could be questioned in this type of situation.  The most concrete way to gain feedback is to publicize what is done with the results.  To take concrete actions based on feedback and to show that to the customer base is extremely motivating to customers to continue to let their voice be heard.   \n* '''Acquiescence bias'''. Due to a phenomenon inherently present in human nature, many people have acquiescent personalities and are more likely to agree with statements than disagree - regardless of the content. Often, those people see the question-asker as an expert in their field which causes them to be more likely to react positively to the question asked. That being said, acquiescence bias (also known as the friendliness bias or \u201cyea-saying\u201d) manifests itself when a respondent shows a tendency to agree with whatever it is that you are asking or stating, even though they might not actually agree.<ref>Jovancic, Nemanja. [https://www.leadquizzes.com/blog/types-of-bias-in-research/ \"4 Types of Bias in Research and How to Make Your Surveys Bias-Free\"]. ''LeadQuizzes''. Retrieved March 16, 2020.</ref>\n* '''Platform Issues'''. Lack of familiarity with the platform used can cause participants and clients confusion.{{citation needed|date=April 2017}}\n* '''Questionnaire design'''. While modern web questionnaires offer a range of design features (different question types, images, multimedia), the use of such elements should be limited to the extent necessary for respondents to understand questions or to stimulate the response. It should not affect their responses, because that would mean lower validity and reliability of data. Appropriate questionnaire design can help lowering the measurement error that can arise also due to the respondents or the survey mode itself (respondent\u2019s motivation, computer literacy, abilities, privacy concerns, etc.).<ref name=\"Vehovar 2008\" />\n* '''Post-survey adjustments'''. Various robust procedures have been developed for situations where sampling deviate from probability selection, or, when we face non-coverage and non-response problems. The standard statistical inference procedures (e.g. confidence interval calculations and hypothesis testing) still require a probability sample. The actual survey practice, particularly in marketing research and in public opinion polling, which massively neglects the principles of probability samples, increasingly requires from the statistical profession to specify the conditions where non-probability samples may work.<ref name=\"Vehovar 2008\" />\n\nThese issues, and potential remedies, are discussed in a number of sources.<ref>Salant, Priscilla, and Don A. Dillman. \"How to Conduct your own Survey: Leading professional give you proven techniques for getting reliable results.\" (1995).</ref><ref>Kalton, Graham. Introduction to survey sampling. Vol. 35. Sage, 1983.</ref>\n\n===Telephone===\n{{Prose|section|date=January 2012}}\n\n* Use of interviewers encourages sample persons to respond, leading to higher response rates.<ref>\n{{cite book\n  |last=Groves|first=R.M.\n  |year=1989\n  |title=Survey Costs and Survey Errors\n  |location=New York|publisher=Wiley\n  |isbn=978-0-471-67851-9\n}}</ref>\n* Interviewers can increase comprehension of questions by answering respondents' questions.\n* Fairly cost efficient, depending on local call charge structure\n* Good for large national (or international) [[sampling frame]]s\n* Some potential for interviewer bias (e.g., some people may be more willing to discuss a sensitive issue with a female interviewer than with a male one)\n* Cannot be used for non-audio information (graphics, demonstrations, taste/smell samples)\n* Three types:\n** Traditional telephone interviews\n** Computer assisted telephone dialing\n** [[Computer assisted telephone interviewing]] ([[CATI]])\n\n===Mail===\n* The questionnaire may be handed to the respondents or mailed to them, but in all cases they are returned to the researcher via mail.\n*An advantage is, is that cost is very low, since bulk postage is cheap in most countries\n* Long delays, often several months, before the surveys are returned and statistical analysis can begin\n* Not suitable for issues that may require clarification\n* Respondents can answer at their own convenience (allowing them to break up long surveys; also useful if they need to check records to answer a question)\n* No interviewer bias\n* Non-response bias can be corrected by extrapolation across waves<ref>{{cite journal | url = http://marketing.wharton.upenn.edu/ideas/pdf/Armstrong/EstimatingNonresponseBias.pdf | title = Estimating Nonresponse Bias in Mail Surveys | author = J. Scott Armstrong and Terry S. Overton | journal = Journal of Marketing Research | volume = 14 | issue = 3 | pages = 396\u2013402 | year = 1977 | doi = 10.2307/3150783 | url-status = dead | archiveurl = https://web.archive.org/web/20100620200022/http://marketing.wharton.upenn.edu/ideas/pdf/Armstrong/EstimatingNonresponseBias.pdf | archivedate = 2010-06-20 | jstor = 3150783 | citeseerx = 10.1.1.36.7783 }}</ref>\n* Large amount of information can be obtained: some mail surveys are as long as 50 pages\n* Response rates can be improved by using mail panels\n* Response rates can be improved by using prepaid monetary incentives<ref>{{cite journal|url=http://www.forecastingprinciples.com/paperpdf/Monetary%20Incentives.pdf | title = Monetary Incentives in Mail Surveys | author = J. Scott Armstrong| journal = Public Opinion Quarterly | volume = 39 | pages = 111\u2013116 | year = 1975 | doi=10.1086/268203}}</ref>\n* Response rates are affected by the class of mail through which the survey was sent<ref>{{cite journal|url=http://www.forecastingprinciples.com/paperpdf/Class%20of%20Mail.pdf | title = Class of Mail Does Affect Response Rates to Mailed Questionnaires: Evidence from Meta-Analysis (with a Reply by Lee Harvey)| author = J. Scott Armstrong| journal = Journal of the Market Research Society | volume = 32 | pages = 469\u2013472 | year = 1990}}</ref>\n** Members of the panel have agreed to participate\n** Panels can be used in longitudinal designs where the same respondents are surveyed several times\n\n===Face-to-face===\n* Suitable for locations where telephone or mail are not developed\n* Potential for interviewer bias\n* Easy to manipulate by completing multiple times to skew results\n\n===Mixed-mode surveys===\nResearchers can combine several above methods for the data collection. For example, researchers can invite shoppers at malls, and send willing participants questionnaires by emails. With the introduction of computers to the survey process, survey mode now includes combinations of different approaches or mixed-mode designs. Some of the most common methods are:<ref name=\"Groves 2009\">\n{{cite book\n  |last1=Groves|first1=R.M.\n  |last2=Fowler|first2=F. J.\n  |last3=Couper|first3=M.P.\n  |last4=Lepkowski|first4=J.M.\n  |last5=Singer|first5=E.\n  |last6=Tourangeau|first6=R.\n  |year=2009\n  |title=Survey Methodology\n  |location=New Jersey|publisher=John Wiley & Sons\n  |isbn=978-1-118-21134-2\n}}</ref>\n*Computer-assisted personal interviewing (CAPI): The computer displays the questions on screen, the interviewer reads them to the respondent, and then enters the respondent's answers.\n*Audio computer-assisted self-interviewing (audio CASI): The respondent operates the computer, the computer displays the question on the screen and plays recordings of the questions to the respondents, who then enters his/her answers.\n*Computer-assisted telephone interviewing (CATI)\n*Interactive voice response (IVR): The computer plays recordings of the questions to respondents over the telephone, who then respond by using the keypad of the telephone or speaking their answers aloud.\n*Web surveys: The computer administers the questions online.\n\n==See also==\n{{wikiversity|Survey research and design in psychology/Assessment/Data collection and entry/Survey administration}}\n* [[Assessment (disambiguation)|Assessment]]\n* [[Comparison of survey software]]\n* [[Data collection system]]\n{{clear}}\n\n==References==\n{{reflist}}\n\n[[Category:Survey methodology|*]]\n", "name_user": "Nadeem lala", "label": "unsafe", "comment": "(\u2192\u200eMobile surveys)", "url_page": "//en.wikipedia.org/wiki/Survey_data_collection"}
