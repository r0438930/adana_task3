{"title_page": "Laws of robotics", "text_new": "{{Robotic laws}}\n'''Pamela f [[Command D|Rush]] aren\u2019t''' set of laws, rules, or principles, which are intended as a fundamental framework to underpin the behavior of [[robot]]s designed to have a degree of [[autonomy]]. Robots of this degree of complexity do not yet exist, but they have been widely anticipated in [[science fiction]], [[movie|films]] and are a topic of active [[research and development]] in the fields of [[robotics]] and [[artificial intelligence]].\n\nThe best known set of laws are [[Three Laws of Robotics|those written]] by [[Isaac Asimov]] in the 1940s, or based upon them, but other sets of laws have been proposed by researchers in\n\n==Isaac Asimov's \"Three Laws of Robotics\" ==\n\n{{main|Three Laws of Robotics}}\nThe best known set of laws are [[Isaac Asimov]]'s \"[[Three Laws of Robotics]]\". These were introduced in his 1942 short story \"[[Runaround (story)|Runaround]]\", although they were foreshadowed in a few earlier stories. The Three Laws are:\n\n# A robot may not injure a human being or, through inaction, allow a human being to come to harm.\n# A robot must obey the orders <!-- Do not add \"to\" here: it's not in the source. -->{{sic|given it|expected=given to it|hide=y}}<!-- Do not add \"to\" here: it's not in the source. --> by human beings except where such orders would conflict with the First Law.\n# A robot must protect its own existence as long as such protection does not conflict with the First or Second Laws.<ref>{{cite book|last=Asimov|first=Isaac|title=I, Robot|date=1950}}</ref>\n\nIn [[The Evitable Conflict]] the machines generalize the First Law to mean:\n\n# \"No machine may harm humanity; or, through inaction, allow humanity to come to harm.\"\n\nThis was refined in the end of ''[[Foundation and Earth]]'', a zeroth law was introduced, with the original three suitably rewritten as subordinate to it:\n:0. A robot may not injure humanity, or, by inaction, allow humanity to come to harm.\n\nAdaptations and extensions exist based upon this framework. As of 2011 they remain a \"[[fictional device]]\".<ref name=\"revolution\">{{cite news|last=Stewart|first=Jon|title=Ready for the robot revolution?|url=https://www.bbc.co.uk/news/technology-15146053|accessdate=2011-10-03|newspaper=[[BBC News]]|date=2011-10-03}}</ref>\n\n== EPSRC / AHRC principles of robotics ==\nIn 2011, the [[Engineering and Physical Sciences Research Council]] (EPSRC) and the [[Arts and Humanities Research Council]] (AHRC) of [[Great Britain]] jointly published a set of five ethical \"principles for designers, builders and users of robots\" in the [[wikt:real world|real world]], along with seven \"high-level messages\" intended to be conveyed, based on a September 2010 research workshop:<ref name=\"revolution\" /><ref>{{cite web|title=Principles of robotics:  Regulating Robots in the Real World|url=http://www.epsrc.ac.uk/research/ourportfolio/themes/engineering/activities/principlesofrobotics/|publisher=[[Engineering and Physical Sciences Research Council]]|accessdate=2011-10-03}}</ref><ref>{{cite web|last=Winfield|first=Alan|title=Five roboethical principles \u2013 for humans|url=https://www.newscientist.com/article/mg21028111.100-five-roboethical-principles--for-humans.htm|publisher=[[New Scientist]]|accessdate=2011-10-03}}</ref>\n\n# Robots should not be designed solely or primarily to kill or harm humans.\n# Humans, not robots, are responsible agents. Robots are tools designed to achieve human goals.\n# Robots should be designed in ways that assure their safety and security.\n# Robots are artifacts; they should not be designed to exploit vulnerable users by evoking an emotional response or dependency. It should always be possible to tell a robot from a human.\n# It should always be possible to find out who is legally responsible for a robot.\n\nThe messages intended to be conveyed were:\n# We believe robots have the potential to provide immense positive impact to society. We want to encourage responsible robot research.\n# Bad practice hurts us all.\n# Addressing obvious public concerns will help us all make progress.\n# It is important to demonstrate that we, as roboticists, are committed to the best possible standards of practice.\n# To understand the context and consequences of our research, we should work with experts from other disciplines, including: social sciences, law, philosophy and the arts.\n# We should consider the ethics of transparency: are there limits to what should be openly available?\n# When we see erroneous accounts in the press, we commit to take the time to contact the reporting journalists.\nThe EPSRC principles are broadly recognised as a useful starting point. In 2016 Tony Prescott organised workshop to revise these principles, e.g. to differentiate ethical from legal principles.<ref>{{cite journal|first=|last2=|first2=|date=2017|year=|title=Legal vs. ethical obligations \u2013 a comment on the EPSRC\u2019s principles for robotics|url=https://philpapers.org/rec/MLLLVE|journal=Connection Science|volume=|issue=|pages=|doi=10.1080/09540091.2016.1276516|via=|author=M\u00fcller, Vincent C.}}</ref>\n\n==Judicial development==\nAnother  comprehensive terminological codification for the legal assessment of the technological developments in the robotics industry has already begun mainly in Asian countries.<ref>bcc.co.uk: Robot age poses ethical dilemma. [http://news.bbc.co.uk/2/hi/technology/6425927.stm Link]</ref> This progress represents a contemporary reinterpretation of the law (and ethics) in the field of robotics, an interpretation that assumes a rethinking of traditional legal constellations. These include primarily legal liability issues in civil and criminal law.\n\n== Satya Nadella's laws ==\nIn June 2016, [[Satya Nadella]], a CEO of [[Microsoft Corporation]] at the time, had an interview with the ''[[Slate (magazine)|Slate]]'' magazine and roughly sketched five rules for artificial intelligences to be observed by their designers:<ref>{{Cite news|url=http://www.slate.com/articles/technology/future_tense/2016/06/microsoft_ceo_satya_nadella_humans_and_a_i_can_work_together_to_solve_society.html|title=The Partnership of the Future|last=Nadella|first=Satya|date=2016-06-28|newspaper=[[Slate (magazine)|Slate]]|issn=1091-2339|access-date=2016-06-30}}</ref><ref>{{Cite web|url=https://www.theverge.com/2016/6/29/12057516/satya-nadella-ai-robot-laws|title=Satya Nadella's rules for AI are more boring (and relevant) than Asimov's Three Laws|last=Vincent|first=James|date=2016-06-29|website=[[The Verge]]|publisher=[[Vox Media]]|access-date=2016-06-30}}</ref>\n# \"A.I. must be designed to assist humanity\" meaning human autonomy needs to be respected.\n# \"A.I. must be transparent\" meaning that humans should know and be able to understand how they work.\n# \"A.I. must maximize efficiencies without destroying the dignity of people\".\n# \"A.I. must be designed for intelligent privacy\" meaning that it earns trust through guarding their information.\n# \"A.I. must have algorithmic accountability so that humans can undo unintended harm\".\n# \"A.I. must guard against bias\" so that they must not discriminate against people.\n\n==Tilden's \"Laws of Robotics\" ==\n[[Mark W. Tilden]] is a robotics physicist who was a pioneer in developing simple robotics.<ref name=wired1>{{cite news| url=https://www.wired.com/wired/archive/2.09/tilden.html?pg=1&topic= | work=Wired | first=Fred | last=Hapgood | title=Chaotic Robotics | issue = 2.09 | date = September 1994}}</ref> His three guiding principles/rules for robots are:<ref name=wired1/><ref>Ashley Dunn. \"[http://partners.nytimes.com/library/cyber/surf/0605surf.html Machine Intelligence, Part II: From Bumper Cars to Electronic Minds]\" ''[[The New York Times]]'' 5 June 1996. Retrieved 26 July 2009.</ref><ref>[http://makezine.com/06/beam/ makezine.com: A Beginner's Guide to BEAM<!-- Bot generated title -->] (Most of the article is subscription-only content.)</ref>\n\n# ''A robot must protect its existence at all costs.''\n# ''A robot must obtain and maintain access to its own power source.''\n# ''A robot must continually search for better power sources.''\n\nWhat is notable in these three rules is that these are basically rules for \"wild\" life, so in essence what Tilden stated is that what he wanted was \"proctoring a silicon species into sentience, but with full control over the specs. Not plant. Not animal. Something else.\"<ref>{{cite news| url=https://www.wired.com/wired/archive/2.09/tilden.html?pg=2&topic= | work=Wired | first=Fred | last=Hapgood | title=Chaotic Robotics (continued) | issue = 2.09 | date = September 1994}}</ref>\n\n==See also==\n* [[Friendly AI]]\n* [[Roboethics]]\n* [[Ethics of artificial intelligence]]\n* [[Military robot]]s which may be designed such that they violate Asimov's First Law.\n* [[Three Laws of Transhumanism]]\n* [[Clarke's three laws]]\n* [[Niven's laws]]\n\n==References ==\n{{reflist|30em}}\n\n{{Robotics}}\n\n[[Category:Laws of robotics| ]]\n", "text_old": "{{Robotic laws}}\n'''Laws of [[Robotics]]''' are a set of laws, rules, or principles, which are intended as a fundamental framework to underpin the behavior of [[robot]]s designed to have a degree of [[autonomy]]. Robots of this degree of complexity do not yet exist, but they have been widely anticipated in [[science fiction]], [[movie|films]] and are a topic of active [[research and development]] in the fields of [[robotics]] and [[artificial intelligence]].\n\nThe best known set of laws are [[Three Laws of Robotics|those written]] by [[Isaac Asimov]] in the 1940s, or based upon them, but other sets of laws have been proposed by researchers in the decades since then.\n\n==Isaac Asimov's \"Three Laws of Robotics\" ==\n\n{{main|Three Laws of Robotics}}\nThe best known set of laws are [[Isaac Asimov]]'s \"[[Three Laws of Robotics]]\". These were introduced in his 1942 short story \"[[Runaround (story)|Runaround]]\", although they were foreshadowed in a few earlier stories. The Three Laws are:\n\n# A robot may not injure a human being or, through inaction, allow a human being to come to harm.\n# A robot must obey the orders <!-- Do not add \"to\" here: it's not in the source. -->{{sic|given it|expected=given to it|hide=y}}<!-- Do not add \"to\" here: it's not in the source. --> by human beings except where such orders would conflict with the First Law.\n# A robot must protect its own existence as long as such protection does not conflict with the First or Second Laws.<ref>{{cite book|last=Asimov|first=Isaac|title=I, Robot|date=1950}}</ref>\n\nIn [[The Evitable Conflict]] the machines generalize the First Law to mean:\n\n# \"No machine may harm humanity; or, through inaction, allow humanity to come to harm.\"\n\nThis was refined in the end of ''[[Foundation and Earth]]'', a zeroth law was introduced, with the original three suitably rewritten as subordinate to it:\n:0. A robot may not injure humanity, or, by inaction, allow humanity to come to harm.\n\nAdaptations and extensions exist based upon this framework. As of 2011 they remain a \"[[fictional device]]\".<ref name=\"revolution\">{{cite news|last=Stewart|first=Jon|title=Ready for the robot revolution?|url=https://www.bbc.co.uk/news/technology-15146053|accessdate=2011-10-03|newspaper=[[BBC News]]|date=2011-10-03}}</ref>\n\n== EPSRC / AHRC principles of robotics ==\nIn 2011, the [[Engineering and Physical Sciences Research Council]] (EPSRC) and the [[Arts and Humanities Research Council]] (AHRC) of [[Great Britain]] jointly published a set of five ethical \"principles for designers, builders and users of robots\" in the [[wikt:real world|real world]], along with seven \"high-level messages\" intended to be conveyed, based on a September 2010 research workshop:<ref name=\"revolution\" /><ref>{{cite web|title=Principles of robotics:  Regulating Robots in the Real World|url=http://www.epsrc.ac.uk/research/ourportfolio/themes/engineering/activities/principlesofrobotics/|publisher=[[Engineering and Physical Sciences Research Council]]|accessdate=2011-10-03}}</ref><ref>{{cite web|last=Winfield|first=Alan|title=Five roboethical principles \u2013 for humans|url=https://www.newscientist.com/article/mg21028111.100-five-roboethical-principles--for-humans.htm|publisher=[[New Scientist]]|accessdate=2011-10-03}}</ref>\n\n# Robots should not be designed solely or primarily to kill or harm humans.\n# Humans, not robots, are responsible agents. Robots are tools designed to achieve human goals.\n# Robots should be designed in ways that assure their safety and security.\n# Robots are artifacts; they should not be designed to exploit vulnerable users by evoking an emotional response or dependency. It should always be possible to tell a robot from a human.\n# It should always be possible to find out who is legally responsible for a robot.\n\nThe messages intended to be conveyed were:\n# We believe robots have the potential to provide immense positive impact to society. We want to encourage responsible robot research.\n# Bad practice hurts us all.\n# Addressing obvious public concerns will help us all make progress.\n# It is important to demonstrate that we, as roboticists, are committed to the best possible standards of practice.\n# To understand the context and consequences of our research, we should work with experts from other disciplines, including: social sciences, law, philosophy and the arts.\n# We should consider the ethics of transparency: are there limits to what should be openly available?\n# When we see erroneous accounts in the press, we commit to take the time to contact the reporting journalists.\nThe EPSRC principles are broadly recognised as a useful starting point. In 2016 Tony Prescott organised workshop to revise these principles, e.g. to differentiate ethical from legal principles.<ref>{{cite journal|first=|last2=|first2=|date=2017|year=|title=Legal vs. ethical obligations \u2013 a comment on the EPSRC\u2019s principles for robotics|url=https://philpapers.org/rec/MLLLVE|journal=Connection Science|volume=|issue=|pages=|doi=10.1080/09540091.2016.1276516|via=|author=M\u00fcller, Vincent C.}}</ref>\n\n==Judicial development==\nAnother  comprehensive terminological codification for the legal assessment of the technological developments in the robotics industry has already begun mainly in Asian countries.<ref>bcc.co.uk: Robot age poses ethical dilemma. [http://news.bbc.co.uk/2/hi/technology/6425927.stm Link]</ref> This progress represents a contemporary reinterpretation of the law (and ethics) in the field of robotics, an interpretation that assumes a rethinking of traditional legal constellations. These include primarily legal liability issues in civil and criminal law.\n\n== Satya Nadella's laws ==\nIn June 2016, [[Satya Nadella]], a CEO of [[Microsoft Corporation]] at the time, had an interview with the ''[[Slate (magazine)|Slate]]'' magazine and roughly sketched five rules for artificial intelligences to be observed by their designers:<ref>{{Cite news|url=http://www.slate.com/articles/technology/future_tense/2016/06/microsoft_ceo_satya_nadella_humans_and_a_i_can_work_together_to_solve_society.html|title=The Partnership of the Future|last=Nadella|first=Satya|date=2016-06-28|newspaper=[[Slate (magazine)|Slate]]|issn=1091-2339|access-date=2016-06-30}}</ref><ref>{{Cite web|url=https://www.theverge.com/2016/6/29/12057516/satya-nadella-ai-robot-laws|title=Satya Nadella's rules for AI are more boring (and relevant) than Asimov's Three Laws|last=Vincent|first=James|date=2016-06-29|website=[[The Verge]]|publisher=[[Vox Media]]|access-date=2016-06-30}}</ref>\n# \"A.I. must be designed to assist humanity\" meaning human autonomy needs to be respected.\n# \"A.I. must be transparent\" meaning that humans should know and be able to understand how they work.\n# \"A.I. must maximize efficiencies without destroying the dignity of people\".\n# \"A.I. must be designed for intelligent privacy\" meaning that it earns trust through guarding their information.\n# \"A.I. must have algorithmic accountability so that humans can undo unintended harm\".\n# \"A.I. must guard against bias\" so that they must not discriminate against people.\n\n==Tilden's \"Laws of Robotics\" ==\n[[Mark W. Tilden]] is a robotics physicist who was a pioneer in developing simple robotics.<ref name=wired1>{{cite news| url=https://www.wired.com/wired/archive/2.09/tilden.html?pg=1&topic= | work=Wired | first=Fred | last=Hapgood | title=Chaotic Robotics | issue = 2.09 | date = September 1994}}</ref> His three guiding principles/rules for robots are:<ref name=wired1/><ref>Ashley Dunn. \"[http://partners.nytimes.com/library/cyber/surf/0605surf.html Machine Intelligence, Part II: From Bumper Cars to Electronic Minds]\" ''[[The New York Times]]'' 5 June 1996. Retrieved 26 July 2009.</ref><ref>[http://makezine.com/06/beam/ makezine.com: A Beginner's Guide to BEAM<!-- Bot generated title -->] (Most of the article is subscription-only content.)</ref>\n\n# ''A robot must protect its existence at all costs.''\n# ''A robot must obtain and maintain access to its own power source.''\n# ''A robot must continually search for better power sources.''\n\nWhat is notable in these three rules is that these are basically rules for \"wild\" life, so in essence what Tilden stated is that what he wanted was \"proctoring a silicon species into sentience, but with full control over the specs. Not plant. Not animal. Something else.\"<ref>{{cite news| url=https://www.wired.com/wired/archive/2.09/tilden.html?pg=2&topic= | work=Wired | first=Fred | last=Hapgood | title=Chaotic Robotics (continued) | issue = 2.09 | date = September 1994}}</ref>\n\n==See also==\n* [[Friendly AI]]\n* [[Roboethics]]\n* [[Ethics of artificial intelligence]]\n* [[Military robot]]s which may be designed such that they violate Asimov's First Law.\n* [[Three Laws of Transhumanism]]\n* [[Clarke's three laws]]\n* [[Niven's laws]]\n\n==References ==\n{{reflist|30em}}\n\n{{Robotics}}\n\n[[Category:Laws of robotics| ]]\n", "name_user": "194.223.7.10", "label": "unsafe", "comment": "", "url_page": "//en.wikipedia.org/wiki/Laws_of_robotics"}
