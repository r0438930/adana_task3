{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TO DO:\n",
    "\n",
    "- recall and precision metrics\n",
    "- making function to be able to predict incoming stream!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part I: Preprocessing the Document Edits\n",
    "\n",
    "## Step 0: Import, Initialization and Loading\n",
    "\n",
    "IDEA: load all the part files into a single dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container { width:95% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import difflib\n",
    "import shutil\n",
    "import sys\n",
    "import os\n",
    "\n",
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:95% !important; }</style>\"))\n",
    "\n",
    "from threading import Thread\n",
    "\n",
    "class StreamingThread(Thread):\n",
    "    def __init__(self, ssc):\n",
    "        Thread.__init__(self)\n",
    "        self.ssc = ssc\n",
    "    def run(self):\n",
    "        ssc.start()\n",
    "        ssc.awaitTermination()\n",
    "    def stop(self):\n",
    "        print('----- Stopping... this may take a few seconds -----')\n",
    "        self.ssc.stop(stopSparkContext=False, stopGraceFully=True)\n",
    "\n",
    "from pyspark.ml import Pipeline\n",
    "        \n",
    "from pyspark.ml.feature import Tokenizer, RegexTokenizer, StopWordsRemover, HashingTF, IDF, OneHotEncoder, StringIndexer, VectorAssembler\n",
    "\n",
    "from pyspark.sql.types import IntegerType, StringType, ArrayType\n",
    "from pyspark.streaming import StreamingContext\n",
    "from pyspark.sql import Row\n",
    "from pyspark.sql.functions import udf, struct, array, col, lit, monotonically_increasing_id\n",
    "\n",
    "from pyspark.sql.types import StructType\n",
    "from pyspark.sql.types import StructField\n",
    "\n",
    "from pyspark.ml.classification import NaiveBayes, NaiveBayesModel\n",
    "from pyspark.ml.linalg import Vectors\n",
    "from pyspark.mllib.regression import LabeledPoint\n",
    "from pyspark.mllib.evaluation import MultilabelMetrics\n",
    "from pyspark.mllib.util import MLUtils\n",
    "\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "\n",
    "from nltk.stem.snowball import *\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://192.168.0.222:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v2.4.5</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>PySparkShell</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        "
      ],
      "text/plain": [
       "<SparkContext master=local[*] appName=PySparkShell>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - hive</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://192.168.0.222:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v2.4.5</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>PySparkShell</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x112ee7898>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading the data into a single dataframe\n",
    "\n",
    "The idea is to load all the saved partfiles into a single dataframe. Next this dataframe can be used to train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+------+--------------------+--------------------+--------------------+--------------------+--------------------+\n",
      "|             comment| label|           name_user|            text_new|            text_old|          title_page|            url_page|\n",
      "+--------------------+------+--------------------+--------------------+--------------------+--------------------+--------------------+\n",
      "|                    |  safe|            SnapSnap|{{Use dmy dates|d...|{{Use dmy dates|d...| Love Wedding Repeat|//en.wikipedia.or...|\n",
      "|                    |  safe|           Altin0000|{{short descripti...|{{short descripti...|2020 coronavirus ...|//en.wikipedia.or...|\n",
      "|                    |  safe|  AttackHelicopter51|{{Proposed deleti...|{{Proposed deleti...|Jawan Canteen Ind...|//en.wikipedia.or...|\n",
      "|small spacing adj...|  safe|           Brykeller|{{Infobox militar...|{{Infobox militar...|               VF-10|//en.wikipedia.or...|\n",
      "|                    |  safe|              Mika1h|{{EngvarB|date=Se...|{{EngvarB|date=Se...|      Mike Singleton|//en.wikipedia.or...|\n",
      "|→‎In popular cult...|  safe|            Cuprum17|{{short descripti...|{{short descripti...|United States Coa...|//en.wikipedia.or...|\n",
      "|        (→‎Episodes)|unsafe|2001:48f8:3028:d7...|{{Use American En...|{{Use American En...|SpongeBob SquareP...|//en.wikipedia.or...|\n",
      "|                    |  safe|         Maister1921|{{Infobox volleyb...|{{Infobox volleyb...|VERVA Warszawa OR...|//en.wikipedia.or...|\n",
      "|          (A9sjwisn)|unsafe|      114.124.241.64|{{Peacock|date=Fe...|{{Peacock|date=Fe...|        Erik Qualman|//en.wikipedia.or...|\n",
      "|       →‎April 12–13|  safe|              Alehol|{{short descripti...|{{short descripti...|   Tornadoes of 2020|//en.wikipedia.or...|\n",
      "|                 All|  safe|              Exarcu|{{short descripti...|{{short descripti...|Nick Baines (bishop)|//en.wikipedia.or...|\n",
      "|     →‎Early history|  safe|            Bneu2013|{{Other uses|Copp...|{{Other uses|Copp...|Copper Basin (Ten...|//en.wikipedia.or...|\n",
      "|    →‎Actor:caps fix|  safe|             Carrite|{{construction}}\n",
      "...|{{construction}}\n",
      "...|     Olan Montgomery|//en.wikipedia.or...|\n",
      "|Attribution: text...|  safe|             Diannaa|[[File:Cd-map.png...|[[File:Cd-map.png...|     Geology of Chad|//en.wikipedia.or...|\n",
      "|          →‎Episodes|  safe|            Deltasim|{{Infobox animang...|{{Infobox animang...|Wan Wan Celeb Sor...|//en.wikipedia.or...|\n",
      "|edited §3, refere...|  safe|               Hiabc|{{Refimprove|date...|{{Refimprove|date...|2nd Portuguese In...|//en.wikipedia.or...|\n",
      "|→‎Efforts to repe...|  safe|      Marylandeditor|{{Infobox anthem\n",
      "...|{{Infobox anthem\n",
      "...|Maryland, My Mary...|//en.wikipedia.or...|\n",
      "|→‎History:adding ...|  safe|           Radiohawk|{{short descripti...|{{short descripti...|                WYNT|//en.wikipedia.or...|\n",
      "|                    |unsafe|       87.113.41.178|{{Short descripti...|{{Short descripti...|      Rochelle Humes|//en.wikipedia.or...|\n",
      "|                    |  safe|          Benniboi01|{{Italic title}}\n",
      "...|{{Italic title}}\n",
      "...|              Atrypa|//en.wikipedia.or...|\n",
      "+--------------------+------+--------------------+--------------------+--------------------+--------------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def load_rdd(base_directory):\n",
    "    # Get all the directory names of the saved myoutput folders\n",
    "    foldernames = os.listdir(base_directory)\n",
    "    \n",
    "    # Create a dataframe from the rdds in the first folder of the list\n",
    "    first_directory = base_directory + \"/\" + foldernames[0]\n",
    "    rdd = sc.textFile(first_directory)\n",
    "    df = spark.read.json(rdd)\n",
    "    \n",
    "    # Remove this folder from the list to prevent it from being added twice\n",
    "    foldernames.remove(foldernames[0])\n",
    "    \n",
    "    for i in range(len(foldernames)):\n",
    "        \n",
    "        if foldernames[i] == '.DS_Store':\n",
    "            continue\n",
    "        \n",
    "        directory = base_directory + \"/\" + foldernames[i]\n",
    "        rdd = sc.textFile(directory)\n",
    "        df_temp = spark.read.json(rdd)\n",
    "        df = df.union(df_temp)\n",
    "\n",
    "    return df\n",
    "\n",
    "# base_directory = r'/Users/Simon/Documents/GitHub/adana_task3/Spark_Cleaned/myoutput-1586797640000/part-00003'\n",
    "# base_directory = r'/Users/Simon/Documents/GitHub/adana_task3/Spark_Cleaned/myoutput-1586797640000'\n",
    "base_directory = r'/Users/Simon/Documents/GitHub/adana_task3/Spark_Data_Limited'\n",
    "# base_directory = r'/Users/Simon/Documents/GitHub/adana_task3/Spark_Cleaned'\n",
    "\n",
    "df = load_rdd(base_directory)    \n",
    "df.show()\n",
    "\n",
    "# df.select('text_new').collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Checking the frequency of each label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df.groupBy(\"label\") \\\n",
    "#     .count() \\\n",
    "#     .orderBy(col(\"count\").desc()) \\\n",
    "#     .show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1a: Tokenization & Normalization\n",
    "\n",
    "The regexTokenizer is used because of its extra functionality compared to the standard Tokenizer built into spark. Also useful is that the tokens are normalized (decapitalized). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(dataframe):\n",
    "    df = dataframe\n",
    "    \n",
    "    rt_old = RegexTokenizer(inputCol=\"text_old\", outputCol=\"words_old\", toLowercase=True, pattern=(\"\\\\W\"))\n",
    "    countTokens = udf(lambda words: len(words), IntegerType())\n",
    "\n",
    "    # regexTokenized_old = rt_old.transform(df)\n",
    "    # df_step1a = regexTokenized_old.withColumn(\"tokens_old\", countTokens(col(\"words_old\")))\n",
    "\n",
    "    regexTokenized_old = rt_old.transform(df)\n",
    "    df = regexTokenized_old.withColumn(\"tokens_old\", countTokens(col(\"words_old\")))\n",
    "\n",
    "    #########################################################################################\n",
    "\n",
    "    rt_new = RegexTokenizer(inputCol=\"text_new\", outputCol=\"words_new\", toLowercase=True, pattern=(\"\\\\W\"))\n",
    "    # regexTokenized_new = rt_new.transform(df_step1a)\n",
    "    # df_step1b = regexTokenized_new.withColumn(\"tokens_new\", countTokens(col(\"words_new\")))\n",
    "\n",
    "    regexTokenized_new = rt_new.transform(df)\n",
    "    df = regexTokenized_new.withColumn(\"tokens_new\", countTokens(col(\"words_new\")))\n",
    "\n",
    "    # df_step1b.show(truncate=False)\n",
    "    \n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_step1b.show()\n",
    "# df.show"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1b: Delta Generator\n",
    "\n",
    "In this crucial step the difference between input and output text is determined. The difference is found using the unified_diff function accesible in through the difflib python library. The function takes two lists of strings as inputs and computes the deleted and inserted (replaced) words. This difference is used to later classify the text edit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_difference(text_old,text_new):\n",
    "#     text_old = df_step1b.select(\"words_old\").collect()[0][0]\n",
    "#     text_new = df_step1b.select(\"words_new\").collect()[0][0]\n",
    "\n",
    "    # print(text_old)\n",
    "\n",
    "    new_words = []\n",
    "    deleted_words = []\n",
    "\n",
    "    for line in difflib.unified_diff(text_old, text_new, fromfile='before.txt', tofile='after.txt'):\n",
    "    #     sys.stdout.write(line)\n",
    "\n",
    "        if \"-\" in line and \" \" not in line:\n",
    "            new_line = line.replace(\"-\", \"\")\n",
    "            deleted_words.append(new_line)\n",
    "        elif \"+\" in line and \" \" not in line:\n",
    "            new_line = line.replace(\"+\", \"\")\n",
    "            new_words.append(new_line)\n",
    "\n",
    "    #     print(line)\n",
    "\n",
    "\n",
    "    # print(\"Deleted words: \", deleted_words)\n",
    "    # print(\"Inserted words: \", new_words)\n",
    "\n",
    "    edited_words = deleted_words + new_words\n",
    "    print(edited_words)\n",
    "    \n",
    "    return edited_words\n",
    "\n",
    "# text_old = df_step1b.select(\"words_old\").collect()[0][0]\n",
    "# text_new = df_step1b.select(\"words_new\").collect()[0][0]\n",
    "# edited_words = text_difference(text_old,text_new)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### User Defined Function\n",
    "\n",
    "This code calculated the difference between the input and output text. This is accomplished by defining a UDF and a seperate function arrayUdf(). The udf is called on two columns *'words_old'* and *'words_new'*. Next a lambda function is defined to iterate over each row of the two input columns. Within the udf is refered to another function arrayUdf() which requires two inputs: the two tokenized lists of words which will be used to compute the difference. The arrayUdf() function acts as an itermediary to call on a different function: text_difference(). The text_difference() function uses the unified_diff generator from the difflib package to return the deltas between two lists of strings.\n",
    "\n",
    "Through experimentation with the unified_diff generator, we found that it was much easier to first tokenize the input and output text and then compute the difference between the two tokenized lists of words. This in contrast to passing the two texts (*'text_old'* and *'text_new'*) of the rdd's as input directly and then tokenizing this *'difference_text'*. Although the latter method might create less computational overhead due to less tokenization, the former method proves to be much more reliable to determine which words have been deleted and which words are new."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def diff_text(dataframe):\n",
    "    df = dataframe\n",
    "    \n",
    "    def arrayUdf(text_old,text_new):\n",
    "        edited_words = text_difference(text_old,text_new)\n",
    "        return edited_words\n",
    "\n",
    "    # countTokens = udf(lambda words: len(words), IntegerType())\n",
    "    callArrayUdf = udf(lambda row: arrayUdf(row[0],row[1]), ArrayType(StringType()))\n",
    "\n",
    "    spark.udf.register(\"callArrayUdf\",callArrayUdf)\n",
    "    #calling udf function\n",
    "\n",
    "\n",
    "    # df_step1c = df_step1b.withColumn(\"diff_text\", callArrayUdf(struct('words_old','words_new')))\n",
    "\n",
    "    df = df.withColumn(\"diff_text\", callArrayUdf(struct('words_old','words_new')))\n",
    "\n",
    "    # df_step1c.select(\"diff_text\").show(truncate=False)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cross validate the change by checking the difference in number of tokens created\n",
    "\n",
    "# tokens_old = df_step1b.select(\"tokens_old\").collect()[0][0]\n",
    "# tokens_new = df_step1b.select(\"tokens_new\").collect()[0][0]\n",
    "\n",
    "# diff_tokens = tokens_new - tokens_old\n",
    "# print(\"The difference in number of tokens for input and output text = \", diff_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Stop Word Removal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def stop_words_removal(dataframe):\n",
    "\n",
    "    df = dataframe\n",
    "    \n",
    "    locale = sc._jvm.java.util.Locale\n",
    "    locale.setDefault(locale.forLanguageTag(\"en-US\"))\n",
    "\n",
    "    stopwords = StopWordsRemover.loadDefaultStopWords(\"english\")\n",
    "    extra_stopwords = [\"http\",\"https\",\"ref\",\"www\",\"com\",\"org\",\"url\",\"web\"]\n",
    "    stopwords = stopwords + extra_stopwords\n",
    "    # print(stopwords)\n",
    "\n",
    "    remover = StopWordsRemover(inputCol=\"diff_text\", outputCol=\"words_clean\",stopWords=stopwords)\n",
    "    stopwords = remover.getStopWords()\n",
    "\n",
    "\n",
    "    # df_step2 = remover.transform(df_step1c)\n",
    "\n",
    "    # df_step2.select(\"words_clean\").show(truncate=False)\n",
    "\n",
    "    df = remover.transform(df)\n",
    "\n",
    "    # (inputCol=\"words\", outputCol=\"filtered\",stopWords=StopWordsRemover.loadDefaultStopWords(\"english\"))\n",
    "    # remover.transform(df_tokenized).show(truncate=False)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Stemming\n",
    "\n",
    "The chosen algorithm for stemming is the snowball stemming algorithm (a variant of the Porter algorithm). The snowball stemmer was chosen because it is slightly more aggresive at stemming the tokenized words than the standard Porter algorithm while still being less aggresive than the Lancaster algorithm. It is a nice 'middle ground' between the two stemming variants."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stemming(dataframe):\n",
    "    \n",
    "    df = dataframe\n",
    "    \n",
    "    # stemmer = SnowballStemmer('english')\n",
    "    stemmer = PorterStemmer()\n",
    "\n",
    "    stemmer_udf = udf(lambda tokens: [stemmer.stem(token) for token in tokens], ArrayType(StringType()))\n",
    "\n",
    "    # df_step3 = df_step2.withColumn(\"words_stemmed\", stemmer_udf(\"words_clean\"))\n",
    "\n",
    "    df = df.withColumn(\"words_stemmed\", stemmer_udf(\"words_clean\"))\n",
    "\n",
    "    # df_step3.select(\"words_stemmed\").show(truncate=False)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Feature Vectorization (TF-IDF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vectorization(dataframe):\n",
    "    \n",
    "    df = dataframe\n",
    "    \n",
    "    tf = HashingTF(inputCol=\"words_stemmed\", outputCol=\"tf\")#, numFeatures=20)\n",
    "\n",
    "    # df_step4a = tf.transform(df_step3)\n",
    "    df = tf.transform(df)\n",
    "\n",
    "\n",
    "    idf = IDF(inputCol=\"tf\", outputCol=\"tf_idf\")\n",
    "    # idfModel = idf.fit(df_step4a)\n",
    "    # df_step4b = idfModel.transform(df_step4a)\n",
    "\n",
    "    idfModel = idf.fit(df)\n",
    "    df = idfModel.transform(df)\n",
    "\n",
    "    # df_step4a.show(truncate=False)\n",
    "    # df_step4b.select(\"words_stemmed\",\"tf_idf\").show(truncate=False)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: String Indexer\n",
    "\n",
    "In this final step the labels (*Safe, Unsafe and Vandal*) are encoded to label indices. The most frequent label gets index 0 while the least frequent label gets the last index depending on the number of indices. In this case the least frequent label gets index 2.\n",
    "\n",
    "In our data 0 corresponds to *Safe*, 1 corresponds to *Unsafe*, and 2 corresponds to *Vandal*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def string_indexer(dataframe):\n",
    "    \n",
    "    df = dataframe\n",
    "    \n",
    "    label_indexer = StringIndexer(inputCol = \"label\", outputCol = \"label_index\")\n",
    "\n",
    "    # df_step5a = label_indexer.fit(df_step4b).transform(df_step4b)\n",
    "    # df_step5b = df_step5a.select(\"tf_idf\",\"label_index\")\n",
    "\n",
    "    #  # Renaming the columns\n",
    "    # df_final = df_step5b.withColumnRenamed(\"tf_idf\",\"features\")\n",
    "    # df_final = df_final.withColumnRenamed(\"label_index\",\"label\")\n",
    "\n",
    "    df = label_indexer.fit(df).transform(df)\n",
    "    \n",
    "\n",
    "    # # Renaming the columns\n",
    "    df_final = df.select(\"tf_idf\",\"label_index\")\n",
    "    df_final = df_final.withColumnRenamed(\"label_index\",\"actual_label\")\n",
    "\n",
    "    # df_final.show()\n",
    "    \n",
    "    return df , df_final"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: Assembly of the preprocessing steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessing(dataframe):\n",
    "    \n",
    "    df = dataframe\n",
    "    \n",
    "    df = tokenize(df)\n",
    "    df = diff_text(df)\n",
    "    df = stop_words_removal(df)\n",
    "    df = stemming(df)\n",
    "    df = vectorization(df)\n",
    "    df , df_label = string_indexer(df)\n",
    "    \n",
    "    return df,df_label\n",
    "\n",
    "df_final , df_label = preprocessing(df)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part II: Multinomial Naive Bayes Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rdd_final = df_final.rdd\n",
    "(training_data, test_data) = df_label.randomSplit([0.7, 0.3], seed = 42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show the frequency of each label in the test set\n",
    "# test_data.groupBy(\"actual_label\") \\\n",
    "#     .count() \\\n",
    "#     .orderBy(col(\"count\").desc()) \\\n",
    "#     .show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from pyspark.ml.classification import LogisticRegression\n",
    "\n",
    "# lr = LogisticRegression(maxIter=20, regParam=0.3, elasticNetParam=0)\n",
    "# lrModel = lr.fit(training_data)\n",
    "# predictions = lrModel.transform(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "nb = NaiveBayes(smoothing=1.0, modelType=\"multinomial\",featuresCol='tf_idf', labelCol='actual_label')\n",
    "model_nb = nb.fit(training_data)\n",
    "predictions = model_nb.transform(test_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predictions.filter(predictions['prediction'] == 1).select('label','probability','prediction').show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predictions.filter(predictions['prediction'] == 2).select('label','probability','prediction').show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predictions.filter(predictions['label'] == 1).select('label','probability','prediction').show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predictions.filter(predictions['label'] == 2).select('label','probability','prediction').show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Saving the Model\n",
    "Use pickle package to save a model to a file and load a model from a file.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# output_dir = r'/Users/Simon/Documents/GitHub/adana_task3'\n",
    "# shutil.rmtree(output_dir, ignore_errors=True)\n",
    "# model_nb.save(sc, output_dir)\n",
    "# sameModel = nb.load(sc, output_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part III: Evaluation of the Model\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7812234644979984"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluator = MulticlassClassificationEvaluator(predictionCol=\"prediction\", labelCol='actual_label')\n",
    "evaluator.evaluate(predictions)\n",
    "\n",
    "# Result = 0.7410821256831497\n",
    "# Result = 0.8053634745632435"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluator.setMetricName(\"recall\")\n",
    "evaluator.evaluate(predictions)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# labelsAndPredictions = predictions.select('label','prediction').rdd\n",
    "\n",
    "# metrics = MultilabelMetrics(labelsAndPredictions)\n",
    "\n",
    "# # # Summary stats\n",
    "# # print(\"Recall = %s\" % all_metrics.recall())\n",
    "# # print(\"Precision = %s\" % all_metrics.precision())\n",
    "# # print(\"F1 measure = %s\" % all_metrics.f1Measure())\n",
    "# # print(\"Accuracy = %s\" % all_metrics.accuracy)\n",
    "\n",
    "# # Individual label stats\n",
    "# labels = labelsAndPredictions.flatMap(lambda x: x[1]).distinct().collect()\n",
    "# for label in labels:\n",
    "#     print(\"Class %s precision = %s\" % (label, metrics.precision(label)))\n",
    "#     print(\"Class %s recall = %s\" % (label, metrics.recall(label)))\n",
    "#     print(\"Class %s F1 Measure = %s\" % (label, metrics.f1Measure(label)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# globals()['models_loaded'] = False\n",
    "\n",
    "# # the predict function will be registered as a udf!\n",
    "# # we use a df with a diff column\n",
    "# def predict(df):\n",
    "#     if any([x in df.diff.lower() for x in ['bad', 'lol', 'joke']]):\n",
    "#         return 'vandal'\n",
    "#     else:\n",
    "#         return 'safe'\n",
    "\n",
    "# predict_udf = udf(predict, StringType()) # user-defined-function (pyspark)\n",
    "\n",
    "# def process(time, rdd):\n",
    "#     if rdd.isEmpty():\n",
    "#         return\n",
    "    \n",
    "#     print(\"========= %s =========\" % str(time))\n",
    "    \n",
    "#     # Convert to data frame\n",
    "#     print(\"Show rdd\")\n",
    "#     rdd.show()\n",
    "#     print()\n",
    "#     df = spark.read.json(rdd)\n",
    "#     print(\"Show df\")\n",
    "#     df.show()\n",
    "    \n",
    "#     # Tip: making a diff will probably help a lot as a feature in any model:\n",
    "#     diff = make_diff(df.first().text_old, df.first().text_new)\n",
    "#     df_withdiff = df.withColumn(\"diff\", lit(diff))\n",
    "#     print(\"Show df_withdiff\")\n",
    "#     print(lit(diff))\n",
    "#     df_withdiff.select('diff').show()\n",
    "\n",
    "    \n",
    "#     # Utilize our predict function. Implementation of the udf!!!\n",
    "#     df_withpreds = df_withdiff.withColumn(\"pred\", predict_udf(\n",
    "#         struct([df_withdiff[x] for x in df_withdiff.columns])\n",
    "#     ))\n",
    "#     print(\"Show df_withpreds\")\n",
    "#     df_withpreds.show()\n",
    "    \n",
    "#     # Normally, you wouldn't use a UDF (User Defined Function) Python function to predict (you can)\n",
    "#     # But an MLlib model you've built and saved with Spark\n",
    "#     # In this case, you need to prevent loading your model in every call to \"process\" as follows:\n",
    "    \n",
    "#     # Load in the model if not yet loaded:\n",
    "#     if not globals()['models_loaded']:\n",
    "#         # load in your models here\n",
    "#         globals()['my_model'] = NaiveBayes # Replace '***' with:    [...].load('my_logistic_regression')\n",
    "#         globals()['models_loaded'] = True\n",
    "        \n",
    "#     # And then predict using the loaded model: \n",
    "#     # df_label = preprocessing(df)[1]\n",
    "#     # df_result = globals()['my_model'].transform(df_label)\n",
    "#     # df_result.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
