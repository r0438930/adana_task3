{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deleting the empty folders\n",
    "\n",
    "Before preprocessing the data and training a model, there are many directories or folder which do not contain 'part files'. It is easier to delete these folders since they are not significant."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "import numpy\n",
    "\n",
    "from pyspark.ml import Pipeline\n",
    "        \n",
    "from pyspark.ml.feature import Tokenizer, RegexTokenizer, StopWordsRemover, HashingTF, IDF, OneHotEncoder, StringIndexer, VectorAssembler\n",
    "\n",
    "from pyspark.sql.types import IntegerType, StringType, ArrayType\n",
    "from pyspark.streaming import StreamingContext\n",
    "from pyspark.sql import Row\n",
    "from pyspark.sql.functions import udf, struct, array, col, lit, monotonically_increasing_id\n",
    "\n",
    "from pyspark.sql.types import StructType\n",
    "from pyspark.sql.types import StructField\n",
    "\n",
    "from pyspark.ml.classification import NaiveBayes, NaiveBayesModel\n",
    "from pyspark.ml.linalg import Vectors\n",
    "from pyspark.mllib.regression import LabeledPoint\n",
    "from pyspark.mllib.evaluation import MultilabelMetrics\n",
    "from pyspark.mllib.util import MLUtils\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Users/Simon/Documents/GitHub/adana_task3/AllData2'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Determine directories of the data and the location for the cleaned data\n",
    "# directory = r'/Users/Simon/Documents/GitHub/adana_task3/Spark_Temp'\n",
    "# new_directory = r'/Users/Simon/Documents/GitHub/adana_task3/Spark_Cleaned'\n",
    "\n",
    "directory = r'/Users/Simon/Documents/GitHub/adana_task3/Spark_Data'\n",
    "new_directory = r'/Users/Simon/Documents/GitHub/adana_task3/AllData2'\n",
    "\n",
    "# directory = r'/Users/Simon/Desktop/DataStream2Cedric'\n",
    "# new_directory = r'/Users/Simon/Desktop/AllData1'\n",
    "\n",
    "filenames = os.listdir(directory) #a list of all the folder names\n",
    "\n",
    "shutil.rmtree(new_directory, ignore_errors=False, onerror=None) #Reset (delete) the new directory\n",
    "shutil.copytree(directory, new_directory) #Copy the data to a new directory to built in some redundancy \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Total Number of Original Folders is:  4296\n",
      "\n",
      "The Total Number of Removed Folders is:  4274\n",
      "\n",
      "Remaining folders:  22\n"
     ]
    }
   ],
   "source": [
    "## Removing the folders which do not contain 'part files'. These folders do not contain any useful data.\n",
    "counter = 0\n",
    "removed_folders = []\n",
    "for i in range(len(filenames)):\n",
    "    remove = False\n",
    "    folder = new_directory + \"/\" + filenames[i]\n",
    "    \n",
    "    if filenames[i] == '.DS_Store' or filenames[i] == 'Icon\\r':\n",
    "        continue\n",
    "        \n",
    "    if len(os.listdir(folder)) <= 1:\n",
    "        remove = True\n",
    "        removed_folders.append(filenames[i])\n",
    "        counter += 1\n",
    "        shutil.rmtree(folder, ignore_errors=False, onerror=None)\n",
    "        \n",
    "print(\"The Total Number of Original Folders is: \", len(filenames))\n",
    "print()\n",
    "\n",
    "print(\"The Total Number of Removed Folders is: \", counter)\n",
    "print()\n",
    "\n",
    "print(\"Remaining folders: \", (len(filenames)) - counter )\n",
    "# print(removed_folders)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_rdd(base_directory):\n",
    "    # Get all the directory names of the saved myoutput folders\n",
    "    foldernames = os.listdir(base_directory)\n",
    "    \n",
    "    # Create list of full directories\n",
    "    full_directories = []\n",
    "    \n",
    "    for i in range(len(foldernames)):\n",
    "        \n",
    "        if foldernames[i] == '.DS_Store':\n",
    "            continue\n",
    "        \n",
    "        directory_temp = base_directory + \"/\" + foldernames[i]\n",
    "        full_directories.append(directory_temp)\n",
    "    \n",
    "    print(\"Number of directories included: \", len(full_directories))\n",
    "    \n",
    "    df = spark.read.format('json').load(path=full_directories)\n",
    "\n",
    "    return df\n",
    "\n",
    "# base_directory = r'/Users/Simon/Documents/GitHub/adana_task3/Spark_Cleaned/myoutput-1586797640000/part-00003'\n",
    "# base_directory = r'/Users/Simon/Documents/GitHub/adana_task3/Spark_Cleaned/myoutput-1586797640000'\n",
    "# base_directory = r'/Users/Simon/Documents/GitHub/adana_task3/Spark_Data_Limited'\n",
    "# base_directory = r'/Users/Simon/Documents/GitHub/adana_task3/Spark_Cleaned'\n",
    "\n",
    "# df = load_rdd(base_directory) \n",
    "# print(\"Number of part files included: \", len(df.toPandas()))\n",
    "\n",
    "# df.write.csv(r'/Users/Simon/Documents/GitHub/adana_task3/All_Data.csv')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The idea is to filter our data so that approximatily the same number of instances of each class are present in the data!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
