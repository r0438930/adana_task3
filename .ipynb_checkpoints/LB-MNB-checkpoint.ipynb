{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TO DO:\n",
    "\n",
    "- error with ancillary features ---> NO CLUE!\n",
    "    - de error die ik krijg komt niet altijd voor. Enkel bij bepaalde dataframes\n",
    "    - Onderzoek de dataframe door te vergroten tot error op duikt. Dat weet je welke rij de error veroorzaakt en kan je mss oplossen\n",
    "    \n",
    "    \n",
    "- filter inputdata so that at least 100 vandals -> make dataset more balanced!\n",
    "\n",
    "- new feature: Identify large edit -> to protect vectorizer -> assume vandal!\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "- change threshold for predicting a certain class!\n",
    "  \n",
    "- make an ensemble\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "- EXTRA: nog een aantal extra ancillary features toevoegen!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part I: Preprocessing the Document Edits\n",
    "\n",
    "## Step 0: Import, Initialization and Loading\n",
    "\n",
    "IDEA: load all the part files into a single dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "//anaconda3/lib/python3.7/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.\n",
      "  warnings.warn(msg, category=FutureWarning)\n",
      "//anaconda3/lib/python3.7/site-packages/sklearn/base.py:318: UserWarning: Trying to unpickle estimator CountVectorizer from version 0.20.2 when using version 0.22.2.post1. This might lead to breaking code or invalid results. Use at your own risk.\n",
      "  UserWarning)\n",
      "//anaconda3/lib/python3.7/site-packages/sklearn/utils/deprecation.py:144: FutureWarning: The sklearn.svm.classes module is  deprecated in version 0.22 and will be removed in version 0.24. The corresponding classes / functions should instead be imported from sklearn.svm. Anything that cannot be imported from sklearn.svm is now part of the private API.\n",
      "  warnings.warn(message, FutureWarning)\n",
      "//anaconda3/lib/python3.7/site-packages/sklearn/base.py:318: UserWarning: Trying to unpickle estimator LinearSVC from version 0.20.2 when using version 0.22.2.post1. This might lead to breaking code or invalid results. Use at your own risk.\n",
      "  UserWarning)\n",
      "//anaconda3/lib/python3.7/site-packages/sklearn/utils/deprecation.py:144: FutureWarning: The sklearn.preprocessing.label module is  deprecated in version 0.22 and will be removed in version 0.24. The corresponding classes / functions should instead be imported from sklearn.preprocessing. Anything that cannot be imported from sklearn.preprocessing is now part of the private API.\n",
      "  warnings.warn(message, FutureWarning)\n",
      "//anaconda3/lib/python3.7/site-packages/sklearn/base.py:318: UserWarning: Trying to unpickle estimator LabelEncoder from version 0.20.2 when using version 0.22.2.post1. This might lead to breaking code or invalid results. Use at your own risk.\n",
      "  UserWarning)\n",
      "//anaconda3/lib/python3.7/site-packages/sklearn/base.py:318: UserWarning: Trying to unpickle estimator _SigmoidCalibration from version 0.20.2 when using version 0.22.2.post1. This might lead to breaking code or invalid results. Use at your own risk.\n",
      "  UserWarning)\n",
      "//anaconda3/lib/python3.7/site-packages/sklearn/base.py:318: UserWarning: Trying to unpickle estimator CalibratedClassifierCV from version 0.20.2 when using version 0.22.2.post1. This might lead to breaking code or invalid results. Use at your own risk.\n",
      "  UserWarning)\n",
      "//anaconda3/lib/python3.7/site-packages/fuzzywuzzy/fuzz.py:11: UserWarning: Using slow pure-python SequenceMatcher. Install python-Levenshtein to remove this warning\n",
      "  warnings.warn('Using slow pure-python SequenceMatcher. Install python-Levenshtein to remove this warning')\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>.container { width:95% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import emoji\n",
    "import re as re\n",
    "import difflib\n",
    "import shutil\n",
    "import sys\n",
    "import os\n",
    "\n",
    "import profanity_check as pc\n",
    "from fuzzywuzzy import fuzz\n",
    "\n",
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:95% !important; }</style>\"))\n",
    "\n",
    "from threading import Thread\n",
    "\n",
    "class StreamingThread(Thread):\n",
    "    def __init__(self, ssc):\n",
    "        Thread.__init__(self)\n",
    "        self.ssc = ssc\n",
    "    def run(self):\n",
    "        ssc.start()\n",
    "        ssc.awaitTermination()\n",
    "    def stop(self):\n",
    "        print('----- Stopping... this may take a few seconds -----')\n",
    "        self.ssc.stop(stopSparkContext=False, stopGraceFully=True)\n",
    "        \n",
    "# from pyspark import SparkContext\n",
    "# SparkContext.setSystemProperty('spark.executor.memory', '2g')\n",
    "\n",
    "from pyspark.ml import Pipeline\n",
    "        \n",
    "from pyspark.ml.feature import Tokenizer, RegexTokenizer, StopWordsRemover, HashingTF, IDF\n",
    "from pyspark.ml.feature import OneHotEncoder, StringIndexer, VectorAssembler,IndexToString\n",
    "\n",
    "from pyspark.sql.types import IntegerType, StringType, ArrayType, FloatType\n",
    "from pyspark.streaming import StreamingContext\n",
    "from pyspark.sql import Row\n",
    "from pyspark.sql.functions import udf, struct, array, col, lit, monotonically_increasing_id\n",
    "\n",
    "from pyspark.sql.types import StructType\n",
    "from pyspark.sql.types import StructField\n",
    "\n",
    "from pyspark.ml.classification import NaiveBayes, NaiveBayesModel, RandomForestClassifier\n",
    "from pyspark.ml.linalg import Vectors\n",
    "from pyspark.mllib.regression import LabeledPoint\n",
    "from pyspark.mllib.evaluation import MultilabelMetrics\n",
    "from pyspark.mllib.util import MLUtils\n",
    "\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "\n",
    "from nltk.stem.snowball import *\n",
    "\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "from sklearn.metrics import multilabel_confusion_matrix\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://192.168.0.222:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v2.4.5</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>PySparkShell</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        "
      ],
      "text/plain": [
       "<SparkContext master=local[*] appName=PySparkShell>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - hive</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://192.168.0.222:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v2.4.5</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>PySparkShell</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x1108df908>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading the data into a single dataframe\n",
    "\n",
    "The idea is to load all the saved partfiles into a single dataframe. Next this dataframe can be used to train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of directories included:  1120\n",
      "2703\n",
      "+--------------------+------+--------------------+--------------------+--------------------+--------------------+--------------------+\n",
      "|             comment| label|           name_user|            text_new|            text_old|          title_page|            url_page|\n",
      "+--------------------+------+--------------------+--------------------+--------------------+--------------------+--------------------+\n",
      "|       (→‎Etymology)|vandal|2601:442:4580:559...|{{redirect2|Masje...|{{redirect2|Masje...|              Mosque|//en.wikipedia.or...|\n",
      "|→‎Comedy:#blackAF...|  safe|          Thornstrom|{{about||Netflix ...|{{about||Netflix ...|List of original ...|//en.wikipedia.or...|\n",
      "|→‎2010s:Minor spe...|  safe|       BorderRegions|{{short descripti...|{{short descripti...|List of attacks r...|//en.wikipedia.or...|\n",
      "|Minor edit in 'Re...|  safe|         Legofan2006|{{distinguish|Ron...|{{distinguish|Ron...|   Cristiano Ronaldo|//en.wikipedia.or...|\n",
      "|                    |  safe|               Tj902|{{short descripti...|{{short descripti...|List of Criterion...|//en.wikipedia.or...|\n",
      "|(→‎France:per con...|unsafe|            Benica11|{{pp-protected|sm...|{{pp-protected|sm...|2019–20 coronavir...|//en.wikipedia.or...|\n",
      "|   →‎Data and graphs|  safe|              Wikmoz|{{pp-protected|sm...|{{pp-protected|sm...|2019–20 coronavir...|//en.wikipedia.or...|\n",
      "|removed three of ...|  safe|          123William|{{pp-protected|sm...|{{pp-protected|sm...|2019–20 coronavir...|//en.wikipedia.or...|\n",
      "|→‎Face masks and ...|  safe|      TheAwesomeHwyh|{{pp-protected|sm...|{{pp-protected|sm...|2019–20 coronavir...|//en.wikipedia.or...|\n",
      "|shouldn't it be \"...|  safe|      TheAwesomeHwyh|{{pp-protected|sm...|{{pp-protected|sm...|2019–20 coronavir...|//en.wikipedia.or...|\n",
      "|   Fixed cite errors|  safe|           John B123|{{short descripti...|{{short descripti...|2020 coronavirus ...|//en.wikipedia.or...|\n",
      "|→‎Containment and...|  safe|           John B123|{{short descripti...|{{short descripti...|2020 coronavirus ...|//en.wikipedia.or...|\n",
      "|                    |  safe|        Billybob2002|{{short descripti...|{{short descripti...|List of United St...|//en.wikipedia.or...|\n",
      "|→‎Current members...|  safe|        Billybob2002|{{short descripti...|{{short descripti...|List of United St...|//en.wikipedia.or...|\n",
      "|→‎April 2020:Clea...|  safe|          Iridescent|{{short descripti...|{{short descripti...|2020 coronavirus ...|//en.wikipedia.or...|\n",
      "|           →‎Finland|  safe|       Arturolorioli|{{short descripti...|{{short descripti...|List of paratroop...|//en.wikipedia.or...|\n",
      "|                    |unsafe|      193.104.197.48|{{Redirect|AI|oth...|{{Redirect|AI|oth...|Artificial intell...|//en.wikipedia.or...|\n",
      "|clean up,typo(s) ...|  safe|            JHunterJ|{{infobox civilia...|{{infobox civilia...|Springfield race ...|//en.wikipedia.or...|\n",
      "|                    |  safe|        ComeradeAlex|{{pp-vandalism|sm...|{{pp-vandalism|sm...|2020 Democratic P...|//en.wikipedia.or...|\n",
      "|        →‎April 2020|  safe|          EditorRock|{{pp-vandalism|sm...|{{pp-vandalism|sm...|2020 Democratic P...|//en.wikipedia.or...|\n",
      "+--------------------+------+--------------------+--------------------+--------------------+--------------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def load_rdd(base_directory):\n",
    "    # Get all the directory names of the saved myoutput folders\n",
    "    foldernames = os.listdir(base_directory)\n",
    "    \n",
    "    # Create list of full directorie names\n",
    "    full_directories = []\n",
    "    \n",
    "    for i in range(len(foldernames)):\n",
    "        \n",
    "        if foldernames[i] == '.DS_Store':\n",
    "            continue\n",
    "        \n",
    "        directory_temp = base_directory + \"/\" + foldernames[i]\n",
    "        full_directories.append(directory_temp)\n",
    "    \n",
    "    print(\"Number of directories included: \", len(full_directories))\n",
    "    \n",
    "    df = spark.read.format('json').load(path=full_directories)\n",
    "    return df\n",
    "\n",
    "# base_directory = r'/Users/Simon/Documents/GitHub/adana_task3/Spark_Cleaned/myoutput-1586797640000/part-00003'\n",
    "# base_directory = r'/Users/Simon/Documents/GitHub/adana_task3/Spark_Cleaned/myoutput-1586797640000'\n",
    "# base_directory = r'/Users/Simon/Documents/GitHub/adana_task3/Spark_Data_Limited'\n",
    "base_directory = r'/Users/Simon/Documents/GitHub/adana_task3/Spark_Cleaned'\n",
    "\n",
    "df = load_rdd(base_directory)\n",
    "print(len(df.toPandas()))\n",
    "df.show()\n",
    "    \n",
    "df_selection = df.toPandas().head(10)\n",
    "\n",
    "## To run faster we only take a selection of the data:\n",
    "df_selection = sqlContext.createDataFrame(df_selection)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Checking the frequency of each label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-----+\n",
      "| label|count|\n",
      "+------+-----+\n",
      "|  safe|    8|\n",
      "|unsafe|    1|\n",
      "|vandal|    1|\n",
      "+------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_selection.groupBy(\"label\") \\\n",
    "    .count() \\\n",
    "    .orderBy(col(\"count\").desc()) \\\n",
    "    .show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1a: Tokenization & Normalization\n",
    "\n",
    "The regexTokenizer is used because of its extra functionality compared to the standard Tokenizer built into spark. Also useful is that the tokens are normalized (decapitalized). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(dataframe):\n",
    "    df = dataframe\n",
    "    \n",
    "    rt_old = RegexTokenizer(inputCol=\"text_old\", outputCol=\"words_old\", toLowercase=True, pattern=(\"\\\\W\"))\n",
    "    countTokens = udf(lambda words: len(words), IntegerType())\n",
    "\n",
    "    # regexTokenized_old = rt_old.transform(df)\n",
    "    # df_step1a = regexTokenized_old.withColumn(\"tokens_old\", countTokens(col(\"words_old\")))\n",
    "\n",
    "    regexTokenized_old = rt_old.transform(df)\n",
    "    df = regexTokenized_old.withColumn(\"tokens_old\", countTokens(col(\"words_old\")))\n",
    "\n",
    "    #########################################################################################\n",
    "\n",
    "    rt_new = RegexTokenizer(inputCol=\"text_new\", outputCol=\"words_new\", toLowercase=True, pattern=(\"\\\\W\"))\n",
    "    # regexTokenized_new = rt_new.transform(df_step1a)\n",
    "    # df_step1b = regexTokenized_new.withColumn(\"tokens_new\", countTokens(col(\"words_new\")))\n",
    "\n",
    "    regexTokenized_new = rt_new.transform(df)\n",
    "    df = regexTokenized_new.withColumn(\"tokens_new\", countTokens(col(\"words_new\")))\n",
    "\n",
    "    # df_step1b.show(truncate=False)\n",
    "    \n",
    "    print(\"Step 1a Done\")\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1b: Delta Generator\n",
    "\n",
    "In this crucial step the difference between input and output text is determined. The difference is found using the unified_diff function accesible in through the difflib python library. The function takes two lists of strings as inputs and computes the deleted and inserted (replaced) words. This difference is used to later classify the text edit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_difference(text_old,text_new):\n",
    "#     text_old = df_step1b.select(\"words_old\").collect()[0][0]\n",
    "#     text_new = df_step1b.select(\"words_new\").collect()[0][0]\n",
    "\n",
    "    # print(text_old)\n",
    "\n",
    "    new_words = []\n",
    "    deleted_words = []\n",
    "\n",
    "    for line in difflib.unified_diff(text_old, text_new, fromfile='before.txt', tofile='after.txt'):\n",
    "    #     sys.stdout.write(line)\n",
    "\n",
    "        if \"-\" in line and \" \" not in line:\n",
    "            new_line = line.replace(\"-\", \"\")\n",
    "            deleted_words.append(new_line)\n",
    "        elif \"+\" in line and \" \" not in line:\n",
    "            new_line = line.replace(\"+\", \"\")\n",
    "            new_words.append(new_line)\n",
    "\n",
    "    #     print(line)\n",
    "\n",
    "\n",
    "    # print(\"Deleted words: \", deleted_words)\n",
    "    # print(\"Inserted words: \", new_words)\n",
    "\n",
    "    edited_words = deleted_words + new_words\n",
    "    \n",
    "    if len(edited_words) == 0:\n",
    "        edited_words = [\"empty\"]\n",
    "    \n",
    "    print(edited_words)\n",
    "    \n",
    "    return edited_words\n",
    "\n",
    "# text_old = df_step1b.select(\"words_old\").collect()[0][0]\n",
    "# text_new = df_step1b.select(\"words_new\").collect()[0][0]\n",
    "# edited_words = text_difference(text_old,text_new)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### User Defined Function\n",
    "\n",
    "This code calculated the difference between the input and output text. This is accomplished by defining a UDF and a seperate function arrayUdf(). The udf is called on two columns *'words_old'* and *'words_new'*. Next a lambda function is defined to iterate over each row of the two input columns. Within the udf is refered to another function arrayUdf() which requires two inputs: the two tokenized lists of words which will be used to compute the difference. The arrayUdf() function acts as an itermediary to call on a different function: text_difference(). The text_difference() function uses the unified_diff generator from the difflib package to return the deltas between two lists of strings.\n",
    "\n",
    "Through experimentation with the unified_diff generator, we found that it was much easier to first tokenize the input and output text and then compute the difference between the two tokenized lists of words. This in contrast to passing the two texts (*'text_old'* and *'text_new'*) of the rdd's as input directly and then tokenizing this *'difference_text'*. Although the latter method might create less computational overhead due to less tokenization, the former method proves to be much more reliable to determine which words have been deleted and which words are new."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def diff_text(dataframe):\n",
    "    df = dataframe\n",
    "    \n",
    "    def arrayUdf(text_old,text_new):\n",
    "        edited_words = text_difference(text_old,text_new)\n",
    "        return edited_words\n",
    "\n",
    "    # countTokens = udf(lambda words: len(words), IntegerType())\n",
    "    callArrayUdf = udf(lambda row: arrayUdf(row[0],row[1]), ArrayType(StringType()))\n",
    "\n",
    "    spark.udf.register(\"callArrayUdf\",callArrayUdf)\n",
    "    #calling udf function\n",
    "\n",
    "\n",
    "    # df_step1c = df_step1b.withColumn(\"diff_text\", callArrayUdf(struct('words_old','words_new')))\n",
    "\n",
    "    df = df.withColumn(\"diff_text\", callArrayUdf(struct('words_old','words_new')))\n",
    "\n",
    "    # df_step1c.select(\"diff_text\").show(truncate=False)\n",
    "    \n",
    "    print(\"Step 1b Done\")\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cross validate the change by checking the difference in number of tokens created\n",
    "\n",
    "# tokens_old = df_step1b.select(\"tokens_old\").collect()[0][0]\n",
    "# tokens_new = df_step1b.select(\"tokens_new\").collect()[0][0]\n",
    "\n",
    "# diff_tokens = tokens_new - tokens_old\n",
    "# print(\"The difference in number of tokens for input and output text = \", diff_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Stop Word Removal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def stop_words_removal(dataframe):\n",
    "\n",
    "    df = dataframe\n",
    "    \n",
    "    locale = sc._jvm.java.util.Locale\n",
    "    locale.setDefault(locale.forLanguageTag(\"en-US\"))\n",
    "\n",
    "    stopwords = StopWordsRemover.loadDefaultStopWords(\"english\")\n",
    "    extra_stopwords = [\"http\",\"https\",\"ref\",\"www\",\"com\",\"org\",\"url\",\"web\"]\n",
    "    stopwords = stopwords + extra_stopwords\n",
    "    # print(stopwords)\n",
    "\n",
    "    remover = StopWordsRemover(inputCol=\"diff_text\", outputCol=\"words_clean\",stopWords=stopwords)\n",
    "    stopwords = remover.getStopWords()\n",
    "\n",
    "\n",
    "    # df_step2 = remover.transform(df_step1c)\n",
    "\n",
    "    # df_step2.select(\"words_clean\").show(truncate=False)\n",
    "\n",
    "    df = remover.transform(df)\n",
    "\n",
    "    # (inputCol=\"words\", outputCol=\"filtered\",stopWords=StopWordsRemover.loadDefaultStopWords(\"english\"))\n",
    "    # remover.transform(df_tokenized).show(truncate=False)\n",
    "    \n",
    "    print(\"Step 2 Done\")\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Stemming\n",
    "\n",
    "The chosen algorithm for stemming is the snowball stemming algorithm (a variant of the Porter algorithm). The snowball stemmer was chosen because it is slightly more aggresive at stemming the tokenized words than the standard Porter algorithm while still being less aggresive than the Lancaster algorithm. It is a nice 'middle ground' between the two stemming variants."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stemming(dataframe):\n",
    "    \n",
    "    df = dataframe\n",
    "    \n",
    "    # stemmer = SnowballStemmer('english')\n",
    "    stemmer = PorterStemmer()\n",
    "\n",
    "    stemmer_udf = udf(lambda tokens: [stemmer.stem(token) for token in tokens], ArrayType(StringType()))\n",
    "\n",
    "    # df_step3 = df_step2.withColumn(\"words_stemmed\", stemmer_udf(\"words_clean\"))\n",
    "\n",
    "    df = df.withColumn(\"words_stemmed\", stemmer_udf(\"words_clean\"))\n",
    "\n",
    "    # df_step3.select(\"words_stemmed\").show(truncate=False)\n",
    "    \n",
    "    print(\"Step 3 Done\")\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Feature Vectorization (TF-IDF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vectorization(dataframe):\n",
    "    \n",
    "    df = dataframe\n",
    "    \n",
    "    tf = HashingTF(inputCol=\"words_stemmed\", outputCol=\"tf\")#, numFeatures=20)\n",
    "\n",
    "    # df_step4a = tf.transform(df_step3)\n",
    "    df = tf.transform(df)\n",
    "\n",
    "\n",
    "    idf = IDF(inputCol=\"tf\", outputCol=\"tf_idf\")\n",
    "    # idfModel = idf.fit(df_step4a)\n",
    "    # df_step4b = idfModel.transform(df_step4a)\n",
    "\n",
    "    idfModel = idf.fit(df)\n",
    "    df = idfModel.transform(df)\n",
    "\n",
    "    # df_step4a.show(truncate=False)\n",
    "    # df_step4b.select(\"words_stemmed\",\"tf_idf\").show(truncate=False)\n",
    "    \n",
    "    print(\"Step 4 Done\")\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: String Indexer\n",
    "\n",
    "In this final step the labels (*Safe, Unsafe and Vandal*) are encoded to label indices. The most frequent label gets index 0 while the least frequent label gets the last index depending on the number of indices. In this case the least frequent label gets index 2.\n",
    "\n",
    "In our data 0 corresponds to *Safe*, 1 corresponds to *Unsafe*, and 2 corresponds to *Vandal*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def string_indexer(dataframe):\n",
    "    \n",
    "    df = dataframe\n",
    "    \n",
    "    label_indexer = StringIndexer(inputCol = \"label\", outputCol = \"label_index\")\n",
    "    \n",
    "#     indexToLabel = label_indexer.labels\n",
    "    \n",
    "    # df_step5a = label_indexer.fit(df_step4b).transform(df_step4b)\n",
    "    # df_step5b = df_step5a.select(\"tf_idf\",\"label_index\")\n",
    "\n",
    "    #  # Renaming the columns\n",
    "    # df_final = df_step5b.withColumnRenamed(\"tf_idf\",\"features\")\n",
    "    # df_final = df_final.withColumnRenamed(\"label_index\",\"label\")\n",
    "\n",
    "    df = label_indexer.fit(df).transform(df)\n",
    "    \n",
    "\n",
    "    # # Renaming the columns\n",
    "    df_label = df.select(\"tf_idf\",\"label_index\")\n",
    "    df_label = df_label.withColumnRenamed(\"label_index\",\"actual_label\")\n",
    "\n",
    "    # df_final.show()\n",
    "    \n",
    "    print(\"Step 5 Done\")\n",
    "    return df , df_label #, indexToLabel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: Assembly of the preprocessing steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 1a Done\n",
      "Step 1b Done\n",
      "Step 2 Done\n",
      "Step 3 Done\n",
      "Step 4 Done\n",
      "Step 5 Done\n"
     ]
    }
   ],
   "source": [
    "def preprocessing(dataframe):\n",
    "    \n",
    "    df = dataframe\n",
    "    \n",
    "    df = tokenize(df)\n",
    "    df = diff_text(df)\n",
    "    df = stop_words_removal(df)\n",
    "    df = stemming(df)\n",
    "    df = vectorization(df)\n",
    "    df , df_label = string_indexer(df)\n",
    "    \n",
    "    return df,df_label\n",
    "\n",
    "\n",
    "df_final , df_label = preprocessing(df_selection)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part II: Ancillary Features\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# old_text = [\"car\", \"train\", \"boat\"]\n",
    "# new_text = [\"car\", \"train\", \"boat\", \"biiike\", \"pl4n3\"]\n",
    "# diff_text = [\"biiike\", \"pl4n3\", \"pl4n3\"]\n",
    "\n",
    "# word = 'biiike'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def maxRepeat(diff_text):\n",
    "    \n",
    "    h = len(diff_text)\n",
    "    count = 0\n",
    "    \n",
    "    for i in range(0, h):\n",
    "        l = len(diff_text[i])\n",
    "        \n",
    "        #Find the maximum repeating character\n",
    "                   \n",
    "        for j in range(0, l):\n",
    "            cur_count = 1\n",
    "            for k in range(j + 1, l):\n",
    "                if(diff_text[i][j] != diff_text[i][k]):\n",
    "                    break\n",
    "                cur_count += 1\n",
    "                                \n",
    "                #update result if required\n",
    "                if cur_count > count:\n",
    "                    count = cur_count\n",
    "                            \n",
    "    return count\n",
    "\n",
    "# print(maxRepeat(diff_text))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#empty revision\n",
    "##checks if edit is is empty or non empty\n",
    "\n",
    "def empty(text_list):\n",
    "    if len(text_list) == 0 or text_list[0] == \"empty\":\n",
    "        empty = 1\n",
    "    else:\n",
    "        empty = 0\n",
    "\n",
    "    return empty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ratio between old text and new text; if  > 1 new text is longer than old text\n",
    "#i would consider it vandal if there is a significant deviation from 1\n",
    "\n",
    "def size_ratio(old_text_list, new_text_list):\n",
    "    len_old_text = len(old_text_list)\n",
    "    len_new_text = len(new_text_list)\n",
    "    \n",
    "    if len_old_text == 0:\n",
    "        ratio = 0.0\n",
    "    else:\n",
    "        ratio = round(len_new_text / len_old_text,3)\n",
    "    \n",
    "    return ratio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#counts the alphanumberic strings in the diffrence list eg (dkfdj125kd,...) the strings with numbers and letters\n",
    "## Since these strings are likely to be vandal\n",
    "## Absolute count or ratio better?\n",
    "\n",
    "def alphanumeric_count(difference_list):\n",
    "    alpha_num = 0\n",
    "    for element in difference_list:\n",
    "        if element.isdigit():\n",
    "            continue\n",
    "        elif element.isalpha():\n",
    "            continue\n",
    "        else:\n",
    "            alpha_num += 1\n",
    "    \n",
    "    return alpha_num"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ratio of vulgar words in the edit\n",
    "\n",
    "def vulgar(old_text_list,difference_list):\n",
    "    \n",
    "    vulgar_list_edit = pc.predict(difference_list)\n",
    "    vulgar_list_old = pc.predict(old_text_list)\n",
    "    count1 = 0\n",
    "    count2 = 0\n",
    "    \n",
    "    for i in vulgar_list_edit:\n",
    "        count1 += i\n",
    "        \n",
    "    for k in vulgar_list_old:\n",
    "        count2 += k\n",
    "        \n",
    "    ratio1 = count1 #/ len(difference_list)\n",
    "    \n",
    "    if count2 == 0:\n",
    "        ratio2 = 0.0\n",
    "    \n",
    "    else:\n",
    "        ratio2 = round(count1 / count2,3)\n",
    "        \n",
    "    return ratio2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Gives a similarity metric between original and new text\n",
    "#How less similar the more suspicous\n",
    "\n",
    "def similarity(old_text_list, new_text_list):\n",
    "    old = ''.join(old_text_list)\n",
    "    new = ''.join(new_text_list)\n",
    "    ratio = round(fuzz.token_set_ratio(old, new)/100,3)\n",
    "    \n",
    "    return ratio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_ancillary_features(dataframe):\n",
    "    df = dataframe\n",
    "    \n",
    "    ## UDF for computing longest repeated character\n",
    "    cafUdf1 = udf(lambda row: maxRepeat(row[2]), IntegerType())\n",
    "    spark.udf.register(\"cafUdf1\", cafUdf1)\n",
    "    df = df.withColumn(\"longest_repeated_char\", cafUdf1(struct('words_old','words_new','diff_text')))\n",
    "    \n",
    "    ## UDF for checking if edit is empty or not\n",
    "    cafUdf2 = udf(lambda row: empty(row[2]), IntegerType())\n",
    "    spark.udf.register(\"cafUdf2\", cafUdf2)\n",
    "    df = df.withColumn(\"empty_edit\", cafUdf2(struct('words_old','words_new','diff_text')))\n",
    "    \n",
    "    ## UDF to determine size ratio between input and output text\n",
    "    cafUdf3 = udf(lambda row: size_ratio(row[0],row[1]), FloatType())\n",
    "    spark.udf.register(\"cafUdf3\", cafUdf3)\n",
    "    df = df.withColumn(\"size_ratio\", cafUdf3(struct('words_old','words_new','diff_text')))\n",
    "    \n",
    "    ## UDF to determine number of alphanumeric words in an edit\n",
    "    cafUdf4 = udf(lambda row: alphanumeric_count(row[2]), IntegerType())\n",
    "    spark.udf.register(\"cafUdf4\", cafUdf4)\n",
    "    df = df.withColumn(\"alpha_count\", cafUdf4(struct('words_old','words_new','diff_text')))\n",
    "    \n",
    "    ## UDF to determine the ratio of vulgar words in the text\n",
    "    cafUdf5 = udf(lambda row: vulgar(row[0],row[2]), FloatType())\n",
    "    spark.udf.register(\"cafUdf5\", cafUdf5)\n",
    "    df = df.withColumn(\"vulgar_ratio\", cafUdf5(struct('words_old','words_new','diff_text')))\n",
    "    \n",
    "    ## UDF to determine the ratio of vulgar words in the text\n",
    "    cafUdf6 = udf(lambda row: similarity(row[0],row[1]), FloatType())\n",
    "    spark.udf.register(\"cafUdf6\", cafUdf6)\n",
    "    df = df.withColumn(\"similarity\", cafUdf6(struct('words_old','words_new','diff_text')))\n",
    "    \n",
    "    print(\"Ancillary Features Done\")\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ancillary Features Done\n"
     ]
    },
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o506.showString.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 3 in stage 13.0 failed 1 times, most recent failure: Lost task 3.0 in stage 13.0 (TID 4317, localhost, executor driver): net.razorvine.pickle.PickleException: expected zero arguments for construction of ClassDict (for numpy.dtype)\n\tat net.razorvine.pickle.objects.ClassDictConstructor.construct(ClassDictConstructor.java:23)\n\tat net.razorvine.pickle.Unpickler.load_reduce(Unpickler.java:707)\n\tat net.razorvine.pickle.Unpickler.dispatch(Unpickler.java:175)\n\tat net.razorvine.pickle.Unpickler.load(Unpickler.java:99)\n\tat net.razorvine.pickle.Unpickler.loads(Unpickler.java:112)\n\tat org.apache.spark.sql.execution.python.BatchEvalPythonExec$$anonfun$evaluate$1.apply(BatchEvalPythonExec.scala:90)\n\tat org.apache.spark.sql.execution.python.BatchEvalPythonExec$$anonfun$evaluate$1.apply(BatchEvalPythonExec.scala:89)\n\tat scala.collection.Iterator$$anon$12.nextCur(Iterator.scala:435)\n\tat scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:441)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage4.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$13$$anon$1.hasNext(WholeStageCodegenExec.scala:636)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$2.apply(SparkPlan.scala:255)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$2.apply(SparkPlan.scala:247)\n\tat org.apache.spark.rdd.RDD$$anonfun$mapPartitionsInternal$1$$anonfun$apply$24.apply(RDD.scala:858)\n\tat org.apache.spark.rdd.RDD$$anonfun$mapPartitionsInternal$1$$anonfun$apply$24.apply(RDD.scala:858)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:346)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:310)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:346)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:310)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:123)\n\tat org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:748)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1891)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1879)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1878)\n\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1878)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:927)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:927)\n\tat scala.Option.foreach(Option.scala:257)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:927)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2112)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2061)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2050)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:738)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2061)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2082)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2101)\n\tat org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:365)\n\tat org.apache.spark.sql.execution.CollectLimitExec.executeCollect(limit.scala:38)\n\tat org.apache.spark.sql.Dataset.org$apache$spark$sql$Dataset$$collectFromPlan(Dataset.scala:3389)\n\tat org.apache.spark.sql.Dataset$$anonfun$head$1.apply(Dataset.scala:2550)\n\tat org.apache.spark.sql.Dataset$$anonfun$head$1.apply(Dataset.scala:2550)\n\tat org.apache.spark.sql.Dataset$$anonfun$52.apply(Dataset.scala:3370)\n\tat org.apache.spark.sql.execution.SQLExecution$$anonfun$withNewExecutionId$1.apply(SQLExecution.scala:80)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:127)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:75)\n\tat org.apache.spark.sql.Dataset.withAction(Dataset.scala:3369)\n\tat org.apache.spark.sql.Dataset.head(Dataset.scala:2550)\n\tat org.apache.spark.sql.Dataset.take(Dataset.scala:2764)\n\tat org.apache.spark.sql.Dataset.getRows(Dataset.scala:254)\n\tat org.apache.spark.sql.Dataset.showString(Dataset.scala:291)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:748)\nCaused by: net.razorvine.pickle.PickleException: expected zero arguments for construction of ClassDict (for numpy.dtype)\n\tat net.razorvine.pickle.objects.ClassDictConstructor.construct(ClassDictConstructor.java:23)\n\tat net.razorvine.pickle.Unpickler.load_reduce(Unpickler.java:707)\n\tat net.razorvine.pickle.Unpickler.dispatch(Unpickler.java:175)\n\tat net.razorvine.pickle.Unpickler.load(Unpickler.java:99)\n\tat net.razorvine.pickle.Unpickler.loads(Unpickler.java:112)\n\tat org.apache.spark.sql.execution.python.BatchEvalPythonExec$$anonfun$evaluate$1.apply(BatchEvalPythonExec.scala:90)\n\tat org.apache.spark.sql.execution.python.BatchEvalPythonExec$$anonfun$evaluate$1.apply(BatchEvalPythonExec.scala:89)\n\tat scala.collection.Iterator$$anon$12.nextCur(Iterator.scala:435)\n\tat scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:441)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage4.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$13$$anon$1.hasNext(WholeStageCodegenExec.scala:636)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$2.apply(SparkPlan.scala:255)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$2.apply(SparkPlan.scala:247)\n\tat org.apache.spark.rdd.RDD$$anonfun$mapPartitionsInternal$1$$anonfun$apply$24.apply(RDD.scala:858)\n\tat org.apache.spark.rdd.RDD$$anonfun$mapPartitionsInternal$1$$anonfun$apply$24.apply(RDD.scala:858)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:346)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:310)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:346)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:310)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:123)\n\tat org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\t... 1 more\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-23-8ab8b4929a86>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      7\u001b[0m df_ancillary = df_ancillary.select('diff_text','tf_idf','longest_repeated_char','empty_edit','size_ratio','alpha_count',\\\n\u001b[1;32m      8\u001b[0m                                    'vulgar_ratio','similarity','label_index')\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0mdf_ancillary\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/Downloads/spark/spark-2.4.5-bin-hadoop2.7/python/pyspark/sql/dataframe.py\u001b[0m in \u001b[0;36mshow\u001b[0;34m(self, n, truncate, vertical)\u001b[0m\n\u001b[1;32m    378\u001b[0m         \"\"\"\n\u001b[1;32m    379\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtruncate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbool\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mtruncate\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 380\u001b[0;31m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshowString\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m20\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvertical\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    381\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    382\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshowString\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtruncate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvertical\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Downloads/spark/spark-2.4.5-bin-hadoop2.7/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1255\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1256\u001b[0m         return_value = get_return_value(\n\u001b[0;32m-> 1257\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m   1258\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1259\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Downloads/spark/spark-2.4.5-bin-hadoop2.7/python/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m     61\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdeco\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m             \u001b[0ms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjava_exception\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoString\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Downloads/spark/spark-2.4.5-bin-hadoop2.7/python/lib/py4j-0.10.7-src.zip/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    326\u001b[0m                 raise Py4JJavaError(\n\u001b[1;32m    327\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 328\u001b[0;31m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[1;32m    329\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    330\u001b[0m                 raise Py4JError(\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o506.showString.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 3 in stage 13.0 failed 1 times, most recent failure: Lost task 3.0 in stage 13.0 (TID 4317, localhost, executor driver): net.razorvine.pickle.PickleException: expected zero arguments for construction of ClassDict (for numpy.dtype)\n\tat net.razorvine.pickle.objects.ClassDictConstructor.construct(ClassDictConstructor.java:23)\n\tat net.razorvine.pickle.Unpickler.load_reduce(Unpickler.java:707)\n\tat net.razorvine.pickle.Unpickler.dispatch(Unpickler.java:175)\n\tat net.razorvine.pickle.Unpickler.load(Unpickler.java:99)\n\tat net.razorvine.pickle.Unpickler.loads(Unpickler.java:112)\n\tat org.apache.spark.sql.execution.python.BatchEvalPythonExec$$anonfun$evaluate$1.apply(BatchEvalPythonExec.scala:90)\n\tat org.apache.spark.sql.execution.python.BatchEvalPythonExec$$anonfun$evaluate$1.apply(BatchEvalPythonExec.scala:89)\n\tat scala.collection.Iterator$$anon$12.nextCur(Iterator.scala:435)\n\tat scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:441)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage4.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$13$$anon$1.hasNext(WholeStageCodegenExec.scala:636)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$2.apply(SparkPlan.scala:255)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$2.apply(SparkPlan.scala:247)\n\tat org.apache.spark.rdd.RDD$$anonfun$mapPartitionsInternal$1$$anonfun$apply$24.apply(RDD.scala:858)\n\tat org.apache.spark.rdd.RDD$$anonfun$mapPartitionsInternal$1$$anonfun$apply$24.apply(RDD.scala:858)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:346)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:310)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:346)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:310)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:123)\n\tat org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:748)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1891)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1879)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1878)\n\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1878)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:927)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:927)\n\tat scala.Option.foreach(Option.scala:257)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:927)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2112)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2061)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2050)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:738)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2061)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2082)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2101)\n\tat org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:365)\n\tat org.apache.spark.sql.execution.CollectLimitExec.executeCollect(limit.scala:38)\n\tat org.apache.spark.sql.Dataset.org$apache$spark$sql$Dataset$$collectFromPlan(Dataset.scala:3389)\n\tat org.apache.spark.sql.Dataset$$anonfun$head$1.apply(Dataset.scala:2550)\n\tat org.apache.spark.sql.Dataset$$anonfun$head$1.apply(Dataset.scala:2550)\n\tat org.apache.spark.sql.Dataset$$anonfun$52.apply(Dataset.scala:3370)\n\tat org.apache.spark.sql.execution.SQLExecution$$anonfun$withNewExecutionId$1.apply(SQLExecution.scala:80)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:127)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:75)\n\tat org.apache.spark.sql.Dataset.withAction(Dataset.scala:3369)\n\tat org.apache.spark.sql.Dataset.head(Dataset.scala:2550)\n\tat org.apache.spark.sql.Dataset.take(Dataset.scala:2764)\n\tat org.apache.spark.sql.Dataset.getRows(Dataset.scala:254)\n\tat org.apache.spark.sql.Dataset.showString(Dataset.scala:291)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:748)\nCaused by: net.razorvine.pickle.PickleException: expected zero arguments for construction of ClassDict (for numpy.dtype)\n\tat net.razorvine.pickle.objects.ClassDictConstructor.construct(ClassDictConstructor.java:23)\n\tat net.razorvine.pickle.Unpickler.load_reduce(Unpickler.java:707)\n\tat net.razorvine.pickle.Unpickler.dispatch(Unpickler.java:175)\n\tat net.razorvine.pickle.Unpickler.load(Unpickler.java:99)\n\tat net.razorvine.pickle.Unpickler.loads(Unpickler.java:112)\n\tat org.apache.spark.sql.execution.python.BatchEvalPythonExec$$anonfun$evaluate$1.apply(BatchEvalPythonExec.scala:90)\n\tat org.apache.spark.sql.execution.python.BatchEvalPythonExec$$anonfun$evaluate$1.apply(BatchEvalPythonExec.scala:89)\n\tat scala.collection.Iterator$$anon$12.nextCur(Iterator.scala:435)\n\tat scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:441)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage4.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$13$$anon$1.hasNext(WholeStageCodegenExec.scala:636)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$2.apply(SparkPlan.scala:255)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$2.apply(SparkPlan.scala:247)\n\tat org.apache.spark.rdd.RDD$$anonfun$mapPartitionsInternal$1$$anonfun$apply$24.apply(RDD.scala:858)\n\tat org.apache.spark.rdd.RDD$$anonfun$mapPartitionsInternal$1$$anonfun$apply$24.apply(RDD.scala:858)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:346)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:310)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:346)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:310)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:123)\n\tat org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\t... 1 more\n"
     ]
    }
   ],
   "source": [
    "df_ancillary = compute_ancillary_features(df_final)\n",
    "\n",
    "# pd_df_ancillary = df_ancillary.toPandas()\n",
    "# pd_df_ancillary = pd_df_ancillary[pd_df_ancillary['empty_edit'] == 0]\n",
    "# df_ancillary = sqlContext.createDataFrame(pd_df_ancillary)\n",
    "\n",
    "df_ancillary = df_ancillary.select('diff_text','tf_idf','longest_repeated_char','empty_edit','size_ratio','alpha_count',\\\n",
    "                                   'vulgar_ratio','similarity','label_index')\n",
    "df_ancillary.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part III: Building Models\n",
    "\n",
    "## Multinomial Naive Bayes Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rdd_final = df_final.rdd\n",
    "(training_data_nb, test_data_nb) = df_label.randomSplit([0.7, 0.3], seed = 42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show the frequency of each label in the test set\n",
    "# test_data.groupBy(\"actual_label\") \\\n",
    "#     .count() \\\n",
    "#     .orderBy(col(\"count\").desc()) \\\n",
    "#     .show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from pyspark.ml.classification import LogisticRegression\n",
    "\n",
    "# lr = LogisticRegression(maxIter=20, regParam=0.3, elasticNetParam=0)\n",
    "# lrModel = lr.fit(training_data)\n",
    "# predictions = lrModel.transform(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nb = NaiveBayes(smoothing=1.0, modelType=\"multinomial\",featuresCol='tf_idf', labelCol='actual_label')\n",
    "model_nb = nb.fit(training_data_nb)\n",
    "predictions_nb = model_nb.transform(test_data_nb)\n",
    "\n",
    "# # Convert indexed labels back to original labels.\n",
    "# label_converter = IndexToString(inputCol=\"prediction\", outputCol=\"predictedLabel\",labels=indexToLabel)\n",
    "# predictions_nb = label_converter.fit(predictions_nb).transform(predictions_nb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predictions.filter(predictions['prediction'] == 1).select('label','probability','prediction').show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predictions.filter(predictions['prediction'] == 2).select('label','probability','prediction').show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predictions.filter(predictions['label'] == 1).select('label','probability','prediction').show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predictions.filter(predictions['label'] == 2).select('label','probability','prediction').show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Saving the Model\n",
    "Use pickle package to save a model to a file and load a model from a file.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# output_dir = r'/Users/Simon/Documents/GitHub/adana_task3'\n",
    "# shutil.rmtree(output_dir, ignore_errors=True)\n",
    "# model_nb.save(sc, output_dir)\n",
    "# sameModel = nb.load(sc, output_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forest Classisfier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assembler = VectorAssembler(inputCols=['longest_repeated_char','empty_edit','size_ratio','alpha_count','vulgar_ratio','similarity'],\\\n",
    "                            outputCol='features')\n",
    "\n",
    "df_ancillary = assembler.transform(df_ancillary)\n",
    "df_ancillary_vector = df_ancillary.select('features','label_index')\n",
    "df_ancillary_vector.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into training and test sets (30% held out for testing)\n",
    "(trainingData, testData) = df_ancillary_vector.randomSplit([0.7, 0.3])\n",
    "\n",
    "# Train a RandomForest model.\n",
    "rf = RandomForestClassifier(labelCol=\"label_index\", featuresCol=\"features\", numTrees=10)\n",
    "\n",
    "# Convert indexed labels back to original labels.\n",
    "# labelConverter = IndexToString(inputCol=\"prediction\", outputCol=\"predictedLabel\",\n",
    "#                                labels=labelIndexer.labels)\n",
    "\n",
    "# Chain indexers and forest in a Pipeline\n",
    "# pipeline = Pipeline(stages=[labelIndexer, featureIndexer, rf, labelConverter])\n",
    "\n",
    "# Train model.  This also runs the indexers.\n",
    "model_rf = rf.fit(trainingData)\n",
    "\n",
    "# Make predictions.\n",
    "predictions_rf = model_rf.transform(testData)\n",
    "predictions_rf.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part IV - A: Evaluation of the Naive Bayes Model\n",
    "\n",
    "## Accuracy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluator = MulticlassClassificationEvaluator(predictionCol=\"prediction\", labelCol='actual_label')\n",
    "accuracy_nb = evaluator.evaluate(predictions_nb)\n",
    "print(accuracy_nb)\n",
    "# Result = 0.7410821256831497\n",
    "# Result = 0.8053634745632435\n",
    "# Result = 0.8838408650684599\n",
    "# Result = 0.911560434636496"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Precision and Recall\n",
    "\n",
    "For the precision and recall measures we fall back on Sklearn packages because their implementation is much more straight forward as compared to the mllib packages. We are mostly interested in the unsafe and vandal isntances!\n",
    "\n",
    "**Binary Classification:**\n",
    "\n",
    "- Precision = TruePositives / (TruePositives + FalsePositives)\n",
    "- Recall = TruePositives / (TruePositives + FalseNegatives)\n",
    "\n",
    "**Multilabel Classification:**\n",
    "\n",
    "In an imbalanced classification problem with more than two classes, precision is calculated as the sum of true positives across all classes divided by the sum of true positives and false positives across all classes.\n",
    "\n",
    "- Precision = Sum c in C TruePositives_c / Sum c in C (TruePositives_c + FalsePositives_c)\n",
    "\n",
    "\n",
    "In an imbalanced classification problem with more than two classes, recall is calculated as the sum of true positives across all classes divided by the sum of true positives and false negatives across all classes.\n",
    "\n",
    "- Recall = Sum c in C TruePositives_c / Sum c in C (TruePositives_c + FalseNegatives_c)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd_predictions_nb = predictions_nb.toPandas()\n",
    "# print(pd_predictions_nb[pd_predictions_nb.prediction == 2.0].count())\n",
    "\n",
    "\n",
    "y_true = pd_predictions_nb['actual_label'].to_list()\n",
    "y_pred = pd_predictions_nb['prediction'].to_list()\n",
    "\n",
    "# precision_recall_fscore_support(y_true, y_pred, average='micro')\n",
    "# precision_recall_fscore_support(y_true, y_pred, average='macro')\n",
    "precision_recall_fscore_support(y_true, y_pred, average='weighted')\n",
    "# precision_recall_fscore_support(y_true, y_pred, average=None, labels=[0.0, 1.0, 2.0])\n",
    "# pd_predictions_nb.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Confusion Matrices and Manual Calculations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cm_arr = multilabel_confusion_matrix(y_true, y_pred,labels=[0.0, 1.0, 2.0])\n",
    "cm_arr_safe = cm_arr[0,:,:]\n",
    "cm_arr_unsafe = cm_arr[1,:,:]\n",
    "cm_arr_vandal = cm_arr[2,:,:]\n",
    "\n",
    "cm_safe = pd.DataFrame({'Pred_Other': cm_arr_safe[:,0], 'Pred_0': cm_arr_safe[:,1]})\n",
    "cm_safe = cm_safe.rename(index={0: 'Act_Other', 1: 'Act_0'})\n",
    "\n",
    "cm_unsafe = pd.DataFrame({'Pred_Other': cm_arr_unsafe[:,0], 'Pred_1': cm_arr_unsafe[:,1]})\n",
    "cm_unsafe = cm_unsafe.rename(index={0: 'Act_Other', 1: 'Act_1'})\n",
    "\n",
    "cm_vandal = pd.DataFrame({'Pred_Other': cm_arr_vandal[:,0], 'Pred_2': cm_arr_vandal[:,1]})\n",
    "cm_vandal = cm_vandal.rename(index={0: 'Act_Other', 1: 'Act_2'})\n",
    "\n",
    "cm_vandal.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "precision_safe = cm_arr_safe[1,1]/(cm_arr_safe[1,1] + cm_arr_safe[0,1])\n",
    "precision_unsafe = cm_arr_unsafe[1,1]/(cm_arr_unsafe[1,1] + cm_arr_unsafe[0,1])\n",
    "precision_vandal = cm_arr_vandal[1,1]/(cm_arr_vandal[1,1] + cm_arr_vandal[0,1])\n",
    "\n",
    "recall_safe = cm_arr_safe[1,1]/(cm_arr_safe[1,1] + cm_arr_safe[1,0])\n",
    "recall_unsafe = cm_arr_unsafe[1,1]/(cm_arr_unsafe[1,1] + cm_arr_unsafe[1,0])\n",
    "recall_vandal = cm_arr_vandal[1,1]/(cm_arr_vandal[1,1] + cm_arr_vandal[1,0])\n",
    "\n",
    "print(\"Precision for Label 0: \", precision_safe)\n",
    "print(\"Precision for Label 1: \", precision_unsafe)\n",
    "print(\"Precision for Label 2: \", precision_vandal)\n",
    "\n",
    "print(\"Recall for Label 0: \", recall_safe)\n",
    "print(\"Recall for Label 1: \", recall_unsafe)\n",
    "print(\"Recall for Label 2: \", recall_vandal)\n",
    "\n",
    "tp_tot = cm_arr_safe[1,1]+cm_arr_unsafe[1,1]+cm_arr_vandal[1,1]\n",
    "fp_tot = cm_arr_safe[0,1]+cm_arr_unsafe[0,1]+cm_arr_vandal[0,1]\n",
    "fn_tot = cm_arr_safe[1,0]+cm_arr_unsafe[1,0]+cm_arr_vandal[1,0]\n",
    "\n",
    "precision_tot = (tp_tot / (tp_tot + fp_tot))\n",
    "recall_tot = ( tp_tot / (tp_tot + fn_tot ))\n",
    "\n",
    "print(\"Total Precision: \", precision_tot)\n",
    "print(\"Total Recall: \", recall_tot)\n",
    "              "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part IV - B: Evaluation of the Random Forest Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluator2 = MulticlassClassificationEvaluator(predictionCol=\"prediction\", labelCol='label_index')\n",
    "accuracy_rf = evaluator2.evaluate(predictions_rf)\n",
    "print(accuracy_rf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd_predictions_rf = predictions_rf.toPandas()\n",
    "# print(pd_predictions_nb[pd_predictions_nb.prediction == 2.0].count())\n",
    "\n",
    "\n",
    "y_true = pd_predictions_rf['label_index'].to_list()\n",
    "y_pred = pd_predictions_rf['prediction'].to_list()\n",
    "\n",
    "# precision_recall_fscore_support(y_true, y_pred, average='micro')\n",
    "# precision_recall_fscore_support(y_true, y_pred, average='macro')\n",
    "precision_recall_fscore_support(y_true, y_pred, average='weighted')\n",
    "# precision_recall_fscore_support(y_true, y_pred, average=None, labels=[0.0, 1.0, 2.0])\n",
    "# pd_predictions_nb.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part V: Running the Models in a Streaming Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "globals()['models_loaded'] = False\n",
    "\n",
    "def process(time, rdd):\n",
    "    if rdd.isEmpty():\n",
    "        return\n",
    "    \n",
    "    print(\"===================== %s =====================\" % str(time))\n",
    "    \n",
    "    ## Convert to data frame\n",
    "    df_pred = spark.read.json(rdd)\n",
    "    print(\"Incoming Dataframe: \")\n",
    "    df_pred.show()\n",
    "\n",
    "    \n",
    "    ## Preprocessing the incoming dataframe \n",
    "    df_pred_final , df_pred_label = preprocessing(df_pred)\n",
    "    print(\"Preprocessed Dataframe: \")\n",
    "    df_pred_final.show()\n",
    "    \n",
    "    \n",
    "    ## Load in the model if not yet loaded:\n",
    "    if not globals()['models_loaded']:\n",
    "        # load in your models here\n",
    "        globals()['my_model_nb'] = model_nb \n",
    "        globals()['models_loaded'] = True #Update the control to notify model is loaded\n",
    "        \n",
    "    # Predict using the loaded model: \n",
    "    df_result = globals()['my_model_nb'].transform(df_pred_label)\n",
    "    print(\"Predicted Result: \")\n",
    "    df_result.show()\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ssc = StreamingContext(sc, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lines = ssc.socketTextStream(\"seppe.net\", 7778)\n",
    "# lines.foreachRDD(process)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ssc_t = StreamingThread(ssc)\n",
    "# ssc_t.start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ssc_t.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
