{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part I: Preprocessing the Document Edits\n",
    "\n",
    "## Step 0: Import, Initialization and Loading\n",
    "\n",
    "IDEA: load all the part files into a single dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container { width:95% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import difflib\n",
    "import sys\n",
    "import os\n",
    "\n",
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:95% !important; }</style>\"))\n",
    "\n",
    "from threading import Thread\n",
    "\n",
    "class StreamingThread(Thread):\n",
    "    def __init__(self, ssc):\n",
    "        Thread.__init__(self)\n",
    "        self.ssc = ssc\n",
    "    def run(self):\n",
    "        ssc.start()\n",
    "        ssc.awaitTermination()\n",
    "    def stop(self):\n",
    "        print('----- Stopping... this may take a few seconds -----')\n",
    "        self.ssc.stop(stopSparkContext=False, stopGraceFully=True)\n",
    "\n",
    "from pyspark.ml import Pipeline\n",
    "        \n",
    "from pyspark.ml.feature import Tokenizer, RegexTokenizer, StopWordsRemover, HashingTF, IDF, OneHotEncoder, StringIndexer, VectorAssembler\n",
    "\n",
    "from pyspark.sql.types import IntegerType, StringType, ArrayType\n",
    "from pyspark.streaming import StreamingContext\n",
    "from pyspark.sql import Row\n",
    "from pyspark.sql.functions import udf, struct, array, col, lit, monotonically_increasing_id\n",
    "\n",
    "from pyspark.sql.types import StructType\n",
    "from pyspark.sql.types import StructField\n",
    "\n",
    "from pyspark.ml.classification import NaiveBayes, NaiveBayesModel\n",
    "from pyspark.ml.linalg import Vectors\n",
    "from pyspark.mllib.regression import LabeledPoint\n",
    "\n",
    "from nltk.stem.snowball import *\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://192.168.0.222:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v2.4.5</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>PySparkShell</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        "
      ],
      "text/plain": [
       "<SparkContext master=local[*] appName=PySparkShell>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - hive</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://192.168.0.222:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v2.4.5</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>PySparkShell</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x1142d1898>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading the data into a single dataframe\n",
    "\n",
    "The idea is to load all the saved partfiles into a single dataframe. Next this dataframe can be used to train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+------+--------------------+--------------------+--------------------+--------------------+--------------------+\n",
      "|             comment| label|           name_user|            text_new|            text_old|          title_page|            url_page|\n",
      "+--------------------+------+--------------------+--------------------+--------------------+--------------------+--------------------+\n",
      "|                    |  safe|            SnapSnap|{{Use dmy dates|d...|{{Use dmy dates|d...| Love Wedding Repeat|//en.wikipedia.or...|\n",
      "|                    |  safe|           Altin0000|{{short descripti...|{{short descripti...|2020 coronavirus ...|//en.wikipedia.or...|\n",
      "|        (→‎Episodes)|unsafe|2001:48f8:3028:d7...|{{Use American En...|{{Use American En...|SpongeBob SquareP...|//en.wikipedia.or...|\n",
      "|                    |  safe|         Maister1921|{{Infobox volleyb...|{{Infobox volleyb...|VERVA Warszawa OR...|//en.wikipedia.or...|\n",
      "|          (A9sjwisn)|unsafe|      114.124.241.64|{{Peacock|date=Fe...|{{Peacock|date=Fe...|        Erik Qualman|//en.wikipedia.or...|\n",
      "|    add default sort|  safe|           Lyndaship|{|{{Infobox ship ...|{|{{Infobox ship ...|Greek support shi...|//en.wikipedia.or...|\n",
      "| →‎Inshore Lifeboats|  safe|          Boatyboy03|{{Use dmy dates|d...|{{Use dmy dates|d...|Barry Dock Lifebo...|//en.wikipedia.or...|\n",
      "|       →‎Murders:c/e|  safe|     Gourami Watcher|{{short descripti...|{{short descripti...|Dr. No (serial ki...|//en.wikipedia.or...|\n",
      "|Changed a few of ...|  safe|       Yoselin Marin|{{short descripti...|{{short descripti...|The Story of an Hour|//en.wikipedia.or...|\n",
      "|Deleted a duplica...|  safe|         Ira Leviton|{{Infobox settlem...|{{Infobox settlem...|       Mapo District|//en.wikipedia.or...|\n",
      "|→‎top:Cleanup and...|  safe|          Iridescent|{{Orphan|date=Nov...|{{Orphan|date=Nov...|          Tyra Kleen|//en.wikipedia.or...|\n",
      "|                    |  safe|         Meandeminem|{{Infobox album\n",
      "|...|{{Infobox album\n",
      "|...|       Unfuccwitable|//en.wikipedia.or...|\n",
      "|expand templates ...|  safe|            Frietjes|{{Use dmy dates|d...|{{Use dmy dates|d...|     2017–18 EHF Cup|//en.wikipedia.or...|\n",
      "|→‎top:Update date...|  safe|     Rich Farmbrough|{{short descripti...|{{short descripti...|List of governors...|//en.wikipedia.or...|\n",
      "|       →‎-57kg Women|  safe|          Mohsen1248|{{main|Taekwondo ...|{{main|Taekwondo ...|Taekwondo at the ...|//en.wikipedia.or...|\n",
      "|→‎External links:...|  safe|     Rich Farmbrough|{{short descripti...|{{short descripti...|List of governors...|//en.wikipedia.or...|\n",
      "|         →‎Section 2|  safe|          Mohsen1248|{{short descripti...|{{short descripti...|2015 World Taekwo...|//en.wikipedia.or...|\n",
      "|add authority con...|  safe|Ser Amantio di Ni...|{{Use dmy dates|d...|{{Use dmy dates|d...|Brajendra Kishore...|//en.wikipedia.or...|\n",
      "|                    |unsafe|          Chindhakan|{{short descripti...|{{short descripti...| Antonio López Habas|//en.wikipedia.or...|\n",
      "|    typo; mnr cpy ed|  safe|            LilHelpa|'''Tejon Mountain...|'''Tejon Mountain...|Tejon Mountain Vi...|//en.wikipedia.or...|\n",
      "+--------------------+------+--------------------+--------------------+--------------------+--------------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def load_rdd(base_directory):\n",
    "    # Get all the directory names of the saved myoutput folders\n",
    "    foldernames = os.listdir(base_directory)\n",
    "    \n",
    "    # Create a dataframe from the rdds in the first folder of the list\n",
    "    first_directory = base_directory + \"/\" + foldernames[0]\n",
    "    rdd = sc.textFile(first_directory)\n",
    "    df = spark.read.json(rdd)\n",
    "    \n",
    "    # Remove this folder from the list to prevent it from being added twice\n",
    "    foldernames.remove(foldernames[0])\n",
    "    \n",
    "    for i in range(len(foldernames)):\n",
    "        \n",
    "        if foldernames[i] == '.DS_Store':\n",
    "            continue\n",
    "        \n",
    "        directory = base_directory + \"/\" + foldernames[i]\n",
    "        rdd = sc.textFile(directory)\n",
    "        df_temp = spark.read.json(rdd)\n",
    "        df = df.union(df_temp)\n",
    "\n",
    "    return df\n",
    "\n",
    "# base_directory = r'/Users/Simon/Documents/GitHub/adana_task3/Spark_Cleaned/myoutput-1586797640000/part-00003'\n",
    "# base_directory = r'/Users/Simon/Documents/GitHub/adana_task3/Spark_Cleaned/myoutput-1586797640000'\n",
    "base_directory = r'/Users/Simon/Documents/GitHub/adana_task3/Spark_Data_Limited'\n",
    "# base_directory = r'/Users/Simon/Documents/GitHub/adana_task3/Spark_Cleaned'\n",
    "\n",
    "df = load_rdd(base_directory)    \n",
    "df.show()\n",
    "\n",
    "# df.select('text_new').collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1a: Tokenization & Normalization\n",
    "\n",
    "The regexTokenizer is used because of its extra functionality compared to the standard Tokenizer built into spark. Also useful is that the tokens are normalized (decapitalized). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "rt_old = RegexTokenizer(inputCol=\"text_old\", outputCol=\"words_old\", toLowercase=True, pattern=(\"\\\\W\"))\n",
    "\n",
    "countTokens = udf(lambda words: len(words), IntegerType())\n",
    "\n",
    "regexTokenized_old = rt_old.transform(df)\n",
    "df_step1a = regexTokenized_old.withColumn(\"tokens_old\", countTokens(col(\"words_old\")))\n",
    "\n",
    "#########################################################################################\n",
    "\n",
    "rt_new = RegexTokenizer(inputCol=\"text_new\", outputCol=\"words_new\", toLowercase=True, pattern=(\"\\\\W\"))\n",
    "regexTokenized_new = rt_new.transform(df_step1a)\n",
    "df_step1b = regexTokenized_new.withColumn(\"tokens_new\", countTokens(col(\"words_new\")))\n",
    "\n",
    "# df_step1b.show(truncate=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_step1b.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1b: Delta Generator\n",
    "\n",
    "In this crucial step the difference between input and output text is determined. The difference is found using the unified_diff function accesible in through the difflib python library. The function takes two lists of strings as inputs and computes the deleted and inserted (replaced) words. This difference is used to later classify the text edit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_difference(text_old,text_new):\n",
    "#     text_old = df_step1b.select(\"words_old\").collect()[0][0]\n",
    "#     text_new = df_step1b.select(\"words_new\").collect()[0][0]\n",
    "\n",
    "    # print(text_old)\n",
    "\n",
    "    new_words = []\n",
    "    deleted_words = []\n",
    "\n",
    "    for line in difflib.unified_diff(text_old, text_new, fromfile='before.txt', tofile='after.txt'):\n",
    "    #     sys.stdout.write(line)\n",
    "\n",
    "        if \"-\" in line and \" \" not in line:\n",
    "            new_line = line.replace(\"-\", \"\")\n",
    "            deleted_words.append(new_line)\n",
    "        elif \"+\" in line and \" \" not in line:\n",
    "            new_line = line.replace(\"+\", \"\")\n",
    "            new_words.append(new_line)\n",
    "\n",
    "    #     print(line)\n",
    "\n",
    "\n",
    "    # print(\"Deleted words: \", deleted_words)\n",
    "    # print(\"Inserted words: \", new_words)\n",
    "\n",
    "    edited_words = deleted_words + new_words\n",
    "    print(edited_words)\n",
    "    \n",
    "    return edited_words\n",
    "\n",
    "# text_old = df_step1b.select(\"words_old\").collect()[0][0]\n",
    "# text_new = df_step1b.select(\"words_new\").collect()[0][0]\n",
    "# edited_words = text_difference(text_old,text_new)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def arrayUdf():\n",
    "#     return edited_words\n",
    "\n",
    "# # countTokens = udf(lambda words: len(words), IntegerType())\n",
    "# callArrayUdf = udf(larrayUdf, ArrayType(StringType()))\n",
    "\n",
    "# #calling udf function\n",
    "# df_step1c = df_step1b.withColumn(\"diff_text\", callArrayUdf())\n",
    "\n",
    "\n",
    "# df_step1c.select(\"diff_text\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### User Defined Function\n",
    "\n",
    "This code calculated the difference between the input and output text. This is accomplished by defining a UDF and a seperate function arrayUdf(). The udf is called on two columns *'words_old'* and *'words_new'*. Next a lambda function is defined to iterate over each row of the two input columns. Within the udf is refered to another function arrayUdf() which requires two inputs: the two tokenized lists of words which will be used to compute the difference. The arrayUdf() function acts as an itermediary to call on a different function: text_difference(). The text_difference() function uses the unified_diff generator from the difflib package to return the deltas between two lists of strings.\n",
    "\n",
    "Through experimentation with the unified_diff generator, we found that it was much easier to first tokenize the input and output text and then compute the difference between the two tokenized lists of words. This in contrast to passing the two texts (*'text_old'* and *'text_new'*) of the rdd's as input directly and then tokenizing this *'difference_text'*. Although the latter method might create less computational overhead due to less tokenization, the former method proves to be much more reliable to determine which words have been deleted and which words are new."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def arrayUdf(text_old,text_new):\n",
    "    edited_words = text_difference(text_old,text_new)\n",
    "    return edited_words\n",
    "\n",
    "# countTokens = udf(lambda words: len(words), IntegerType())\n",
    "callArrayUdf = udf(lambda row: arrayUdf(row[0],row[1]), ArrayType(StringType()))\n",
    "\n",
    "spark.udf.register(\"callArrayUdf\",callArrayUdf)\n",
    "#calling udf function\n",
    "\n",
    "\n",
    "df_step1c = df_step1b.withColumn(\"diff_text\", callArrayUdf(struct('words_old','words_new')))\n",
    "\n",
    "\n",
    "# df_step1c.select(\"diff_text\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cross validate the change by checking the difference in number of tokens created\n",
    "\n",
    "# tokens_old = df_step1b.select(\"tokens_old\").collect()[0][0]\n",
    "# tokens_new = df_step1b.select(\"tokens_new\").collect()[0][0]\n",
    "\n",
    "# diff_tokens = tokens_new - tokens_old\n",
    "# print(\"The difference in number of tokens for input and output text = \", diff_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Stop Word Removal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', 'her', 'hers', 'herself', 'it', 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', 'should', 'now', \"i'll\", \"you'll\", \"he'll\", \"she'll\", \"we'll\", \"they'll\", \"i'd\", \"you'd\", \"he'd\", \"she'd\", \"we'd\", \"they'd\", \"i'm\", \"you're\", \"he's\", \"she's\", \"it's\", \"we're\", \"they're\", \"i've\", \"we've\", \"you've\", \"they've\", \"isn't\", \"aren't\", \"wasn't\", \"weren't\", \"haven't\", \"hasn't\", \"hadn't\", \"don't\", \"doesn't\", \"didn't\", \"won't\", \"wouldn't\", \"shan't\", \"shouldn't\", \"mustn't\", \"can't\", \"couldn't\", 'cannot', 'could', \"here's\", \"how's\", \"let's\", 'ought', \"that's\", \"there's\", \"what's\", \"when's\", \"where's\", \"who's\", \"why's\", 'would', 'http', 'https', 'ref', 'www', 'com', 'org', 'url', 'web']\n"
     ]
    }
   ],
   "source": [
    "locale = sc._jvm.java.util.Locale\n",
    "locale.setDefault(locale.forLanguageTag(\"en-US\"))\n",
    "\n",
    "stopwords = StopWordsRemover.loadDefaultStopWords(\"english\")\n",
    "extra_stopwords = [\"http\",\"https\",\"ref\",\"www\",\"com\",\"org\",\"url\",\"web\"]\n",
    "stopwords = stopwords + extra_stopwords\n",
    "print(stopwords)\n",
    "\n",
    "remover = StopWordsRemover(inputCol=\"diff_text\", outputCol=\"words_clean\",stopWords=stopwords)\n",
    "stopwords = remover.getStopWords()\n",
    "df_step2 = remover.transform(df_step1c)\n",
    "# df_step2.select(\"words_clean\").show(truncate=False)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# (inputCol=\"words\", outputCol=\"filtered\",stopWords=StopWordsRemover.loadDefaultStopWords(\"english\"))\n",
    "# remover.transform(df_tokenized).show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Stemming\n",
    "\n",
    "The chosen algorithm for stemming is the snowball stemming algorithm (a variant of the Porter algorithm). The snowball stemmer was chosen because it is slightly more aggresive at stemming the tokenized words than the standard Porter algorithm while still being less aggresive than the Lancaster algorithm. It is a nice 'middle ground' between the two stemming variants."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# stemmer = SnowballStemmer('english')\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "stemmer_udf = udf(lambda tokens: [stemmer.stem(token) for token in tokens], ArrayType(StringType()))\n",
    "\n",
    "df_step3 = df_step2.withColumn(\"words_stemmed\", stemmer_udf(\"words_clean\"))\n",
    "\n",
    "# df_step3.select(\"words_stemmed\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Feature Vectorization (TF-IDF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf = HashingTF(inputCol=\"words_stemmed\", outputCol=\"tf\")#, numFeatures=20)\n",
    "\n",
    "df_step4a = tf.transform(df_step3)\n",
    "\n",
    "\n",
    "\n",
    "idf = IDF(inputCol=\"tf\", outputCol=\"tf_idf\")\n",
    "idfModel = idf.fit(df_step4a)\n",
    "df_step4b = idfModel.transform(df_step4a)\n",
    "\n",
    "# df_step4a.show(truncate=False)\n",
    "# df_step4b.select(\"words_stemmed\",\"tf_idf\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: String Indexer\n",
    "\n",
    "In this final step the labels (*Safe, Unsafe and Vandal*) are encoded to label indices. The most frequent label gets index 0 while the least frequent label gets the last index depending on the number of indices. In this case the least frequent label gets index 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----+\n",
      "|            features|label|\n",
      "+--------------------+-----+\n",
      "|      (262144,[],[])|  0.0|\n",
      "|(262144,[131,1040...|  0.0|\n",
      "|(262144,[21823,82...|  1.0|\n",
      "|(262144,[12060,21...|  0.0|\n",
      "|(262144,[29143,35...|  1.0|\n",
      "|(262144,[8215,127...|  0.0|\n",
      "|(262144,[21180,25...|  0.0|\n",
      "|(262144,[13471,13...|  0.0|\n",
      "|(262144,[65897,13...|  0.0|\n",
      "|      (262144,[],[])|  0.0|\n",
      "|(262144,[109320],...|  0.0|\n",
      "|(262144,[53159,12...|  0.0|\n",
      "|(262144,[215,2723...|  0.0|\n",
      "|(262144,[14898,36...|  0.0|\n",
      "|(262144,[78349,14...|  0.0|\n",
      "|(262144,[14898,36...|  0.0|\n",
      "|(262144,[78349,14...|  0.0|\n",
      "|(262144,[14898,21...|  0.0|\n",
      "|(262144,[97,3096,...|  1.0|\n",
      "|(262144,[220567,2...|  0.0|\n",
      "+--------------------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "label_indexer = StringIndexer(inputCol = \"label\", outputCol = \"label_index\")\n",
    "\n",
    "df_step5a = label_indexer.fit(df_step4b).transform(df_step4b)\n",
    "\n",
    "df_step5b = df_step5a.select(\"tf_idf\",\"label_index\")\n",
    "\n",
    "df_final = df_step5b.withColumnRenamed(\"tf_idf\",\"features\")\n",
    "df_final = df_final.withColumnRenamed(\"label_index\",\"label\")\n",
    "\n",
    "\n",
    "df_final.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part II: Multinomial Naive Bayes Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rdd_final = df_final.rdd\n",
    "(training_data, test_data) = df_final.randomSplit([0.7, 0.3], seed = 42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from pyspark.ml.classification import LogisticRegression\n",
    "\n",
    "# lr = LogisticRegression(maxIter=20, regParam=0.3, elasticNetParam=0)\n",
    "# lrModel = lr.fit(training_data)\n",
    "# predictions = lrModel.transform(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import NaiveBayes\n",
    "\n",
    "nb = NaiveBayes(smoothing=1.0, modelType=\"multinomial\")\n",
    "model = nb.fit(training_data)\n",
    "predictions = model.transform(test_data)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+------------------------------------------------------------------+----------+\n",
      "|label|probability                                                       |prediction|\n",
      "+-----+------------------------------------------------------------------+----------+\n",
      "|1.0  |[6.965922696119666E-9,0.9999999915451795,1.4888979822627257E-9]   |1.0       |\n",
      "|0.0  |[9.015248300964145E-11,0.9999999999098474,5.386297692386849E-18]  |1.0       |\n",
      "|0.0  |[1.2127528644099106E-8,0.9999999878724714,0.0]                    |1.0       |\n",
      "|0.0  |[3.182778185138302E-4,0.9995359695618484,1.457526196376846E-4]    |1.0       |\n",
      "|0.0  |[7.252809679245608E-118,1.0,0.0]                                  |1.0       |\n",
      "|0.0  |[1.0285580293569733E-11,0.9999999999897144,1.6691147036706362E-39]|1.0       |\n",
      "|0.0  |[2.243052649551445E-16,0.9999999999999998,1.1055806318743884E-30] |1.0       |\n",
      "|0.0  |[2.12936918635517E-4,0.9997761462855461,1.0916795818429364E-5]    |1.0       |\n",
      "|0.0  |[2.5998388706170613E-7,0.999999740016113,7.25866631605078E-24]    |1.0       |\n",
      "|0.0  |[1.8071922084079342E-5,0.999980988526486,9.39551430060615E-7]     |1.0       |\n",
      "|0.0  |[0.2996833285763914,0.7003131057337253,3.565689883217452E-6]      |1.0       |\n",
      "|1.0  |[2.0168634205501722E-38,1.0,7.782761536574368E-61]                |1.0       |\n",
      "|0.0  |[0.025304564750232177,0.9746954352497678,3.607443286232305E-23]   |1.0       |\n",
      "|1.0  |[3.8149309925146783E-4,0.9996185069007486,6.23E-322]              |1.0       |\n",
      "|0.0  |[3.65530347306467E-15,0.9999999999999962,1.8739916931993613E-16]  |1.0       |\n",
      "|1.0  |[6.961177125345584E-14,0.9999999999999303,3.5146952127662255E-19] |1.0       |\n",
      "|0.0  |[0.12912369916127042,0.8708763008386602,6.950548014727464E-14]    |1.0       |\n",
      "|1.0  |[1.0488800199938614E-12,0.9999999999988511,9.986795023689449E-14] |1.0       |\n",
      "|0.0  |[0.0,1.0,0.0]                                                     |1.0       |\n",
      "|0.0  |[8.849070498270482E-110,1.0,0.0]                                  |1.0       |\n",
      "+-----+------------------------------------------------------------------+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "predictions.filter(predictions['prediction'] == 1).select('label','probability','prediction').show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-------------------------------------------------------------+----------+\n",
      "|label|probability                                                  |prediction|\n",
      "+-----+-------------------------------------------------------------+----------+\n",
      "|0.0  |[0.10640016820806811,0.004932696899015449,0.8886671348929164]|2.0       |\n",
      "+-----+-------------------------------------------------------------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "predictions.filter(predictions['prediction'] == 2).select('label','probability','prediction').show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+------------------------------------------------------------------+----------+\n",
      "|label|probability                                                       |prediction|\n",
      "+-----+------------------------------------------------------------------+----------+\n",
      "|1.0  |[6.965922696119666E-9,0.9999999915451795,1.4888979822627257E-9]   |1.0       |\n",
      "|1.0  |[0.9720357165274826,0.027964222363647574,6.110886968682242E-8]    |0.0       |\n",
      "|1.0  |[1.0,3.697009398233063E-20,1.7862898835E-313]                     |0.0       |\n",
      "|1.0  |[0.9999956998551663,2.9465301587481647E-6,1.3536146749849673E-6]  |0.0       |\n",
      "|1.0  |[2.0168634205501722E-38,1.0,7.782761536574368E-61]                |1.0       |\n",
      "|1.0  |[3.8149309925146783E-4,0.9996185069007486,6.23E-322]              |1.0       |\n",
      "|1.0  |[6.961177125345584E-14,0.9999999999999303,3.5146952127662255E-19] |1.0       |\n",
      "|1.0  |[1.0488800199938614E-12,0.9999999999988511,9.986795023689449E-14] |1.0       |\n",
      "|1.0  |[0.9708454040498471,0.029154595950152823,1.8112190083535753E-27]  |0.0       |\n",
      "|1.0  |[0.8900757256438838,0.1099242743561162,1.8705538693731403E-24]    |0.0       |\n",
      "|1.0  |[1.9307223559294227E-294,1.0,0.0]                                 |1.0       |\n",
      "|1.0  |[0.862085727741862,0.11408125725938954,0.023833014998748396]      |0.0       |\n",
      "|1.0  |[1.3012634324239555E-9,0.9999999986975325,1.2039834948580214E-12] |1.0       |\n",
      "|1.0  |[4.124526233537537E-11,0.9999999999587545,2.0544445619254692E-16] |1.0       |\n",
      "|1.0  |[0.9999999797315333,1.4306193084153742E-8,5.962273625703043E-9]   |0.0       |\n",
      "|1.0  |[0.862085727741862,0.11408125725938954,0.023833014998748396]      |0.0       |\n",
      "|1.0  |[6.965922696119666E-9,0.9999999915451795,1.4888979822627257E-9]   |1.0       |\n",
      "|1.0  |[1.5461677989460604E-11,0.9999999999845384,1.1146948829037759E-41]|1.0       |\n",
      "|1.0  |[2.3858112574815544E-11,0.9999999999565612,1.9580829677761885E-11]|1.0       |\n",
      "|1.0  |[0.8806385060264756,0.11936131840594001,1.7556758446504973E-7]    |0.0       |\n",
      "+-----+------------------------------------------------------------------+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "predictions.filter(predictions['label'] == 1).select('label','probability','prediction').show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----------+----------+\n",
      "|label|probability|prediction|\n",
      "+-----+-----------+----------+\n",
      "+-----+-----------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "predictions.filter(predictions['label'] == 2).select('label','probability','prediction').show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# globals()['models_loaded'] = False\n",
    "\n",
    "# # the predict function will be registered as a udf!\n",
    "# # we use a df with a diff column\n",
    "# def predict(df):\n",
    "#     if any([x in df.diff.lower() for x in ['bad', 'lol', 'joke']]):\n",
    "#         return 'vandal'\n",
    "#     else:\n",
    "#         return 'safe'\n",
    "\n",
    "# predict_udf = udf(predict, StringType()) # user-defined-function (pyspark)\n",
    "\n",
    "# def process(time, rdd):\n",
    "#     if rdd.isEmpty():\n",
    "#         return\n",
    "    \n",
    "#     print(\"========= %s =========\" % str(time))\n",
    "    \n",
    "#     # Convert to data frame\n",
    "#     print(\"Show rdd\")\n",
    "#     rdd.show()\n",
    "#     print()\n",
    "#     df = spark.read.json(rdd)\n",
    "#     print(\"Show df\")\n",
    "#     df.show()\n",
    "    \n",
    "#     # Tip: making a diff will probably help a lot as a feature in any model:\n",
    "#     diff = make_diff(df.first().text_old, df.first().text_new)\n",
    "#     df_withdiff = df.withColumn(\"diff\", lit(diff))\n",
    "#     print(\"Show df_withdiff\")\n",
    "#     print(lit(diff))\n",
    "#     df_withdiff.select('diff').show()\n",
    "\n",
    "    \n",
    "#     # Utilize our predict function. Implementation of the udf!!!\n",
    "#     df_withpreds = df_withdiff.withColumn(\"pred\", predict_udf(\n",
    "#         struct([df_withdiff[x] for x in df_withdiff.columns])\n",
    "#     ))\n",
    "#     print(\"Show df_withpreds\")\n",
    "#     df_withpreds.show()\n",
    "    \n",
    "#     # Normally, you wouldn't use a UDF (User Defined Function) Python function to predict (you can)\n",
    "#     # But an MLlib model you've built and saved with Spark\n",
    "#     # In this case, you need to prevent loading your model in every call to \"process\" as follows:\n",
    "    \n",
    "#     # Load in the model if not yet loaded:\n",
    "#     if not globals()['models_loaded']:\n",
    "#         # load in your models here\n",
    "#         globals()['my_model'] = '***' # Replace '***' with:    [...].load('my_logistic_regression')\n",
    "#         globals()['models_loaded'] = True\n",
    "        \n",
    "#     # And then predict using the loaded model: \n",
    "#     # df_result = globals()['my_model'].transform(df)\n",
    "#     # df_result.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-21-788a2010daf9>, line 7)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-21-788a2010daf9>\"\u001b[0;36m, line \u001b[0;32m7\u001b[0m\n\u001b[0;31m    1. token:\u001b[0m\n\u001b[0m           ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "label: string\n",
    "comment: string\n",
    "text_old: string\n",
    "text_new: string\n",
    "user: string\n",
    "\n",
    "1. token:\n",
    "\n",
    "\n",
    "\n",
    "words_old: [string,string,string]\n",
    "words_new: [strings]\n",
    "    \n",
    "list1 = [words_old]\n",
    "list2 = [words_new]\n",
    "list3 = [diff_words]\n",
    "\n",
    "list3 = ['yes','no','hello','booooobs']\n",
    "\n",
    "def longest_char(list3):\n",
    "    \n",
    "    ...\n",
    "    \n",
    "    return longest_char = l, aantal = 2\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
