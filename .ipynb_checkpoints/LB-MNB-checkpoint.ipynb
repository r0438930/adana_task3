{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TO DO:\n",
    "\n",
    "- make an ensemble\n",
    "    - combine predictions of both\n",
    "- facebook vulgar words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part I: Preprocessing the Document Edits\n",
    "\n",
    "## Step 0: Import, Initialization and Loading\n",
    "\n",
    "IDEA: load all the part files into a single dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "//anaconda3/lib/python3.7/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.\n",
      "  warnings.warn(msg, category=FutureWarning)\n",
      "//anaconda3/lib/python3.7/site-packages/sklearn/base.py:318: UserWarning: Trying to unpickle estimator CountVectorizer from version 0.20.2 when using version 0.22.2.post1. This might lead to breaking code or invalid results. Use at your own risk.\n",
      "  UserWarning)\n",
      "//anaconda3/lib/python3.7/site-packages/sklearn/utils/deprecation.py:144: FutureWarning: The sklearn.svm.classes module is  deprecated in version 0.22 and will be removed in version 0.24. The corresponding classes / functions should instead be imported from sklearn.svm. Anything that cannot be imported from sklearn.svm is now part of the private API.\n",
      "  warnings.warn(message, FutureWarning)\n",
      "//anaconda3/lib/python3.7/site-packages/sklearn/base.py:318: UserWarning: Trying to unpickle estimator LinearSVC from version 0.20.2 when using version 0.22.2.post1. This might lead to breaking code or invalid results. Use at your own risk.\n",
      "  UserWarning)\n",
      "//anaconda3/lib/python3.7/site-packages/sklearn/utils/deprecation.py:144: FutureWarning: The sklearn.preprocessing.label module is  deprecated in version 0.22 and will be removed in version 0.24. The corresponding classes / functions should instead be imported from sklearn.preprocessing. Anything that cannot be imported from sklearn.preprocessing is now part of the private API.\n",
      "  warnings.warn(message, FutureWarning)\n",
      "//anaconda3/lib/python3.7/site-packages/sklearn/base.py:318: UserWarning: Trying to unpickle estimator LabelEncoder from version 0.20.2 when using version 0.22.2.post1. This might lead to breaking code or invalid results. Use at your own risk.\n",
      "  UserWarning)\n",
      "//anaconda3/lib/python3.7/site-packages/sklearn/base.py:318: UserWarning: Trying to unpickle estimator _SigmoidCalibration from version 0.20.2 when using version 0.22.2.post1. This might lead to breaking code or invalid results. Use at your own risk.\n",
      "  UserWarning)\n",
      "//anaconda3/lib/python3.7/site-packages/sklearn/base.py:318: UserWarning: Trying to unpickle estimator CalibratedClassifierCV from version 0.20.2 when using version 0.22.2.post1. This might lead to breaking code or invalid results. Use at your own risk.\n",
      "  UserWarning)\n",
      "//anaconda3/lib/python3.7/site-packages/fuzzywuzzy/fuzz.py:11: UserWarning: Using slow pure-python SequenceMatcher. Install python-Levenshtein to remove this warning\n",
      "  warnings.warn('Using slow pure-python SequenceMatcher. Install python-Levenshtein to remove this warning')\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>.container { width:95% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import emoji\n",
    "import re as re\n",
    "import difflib\n",
    "import shutil\n",
    "import sys\n",
    "import os\n",
    "\n",
    "import profanity_check as pc\n",
    "from fuzzywuzzy import fuzz\n",
    "\n",
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:95% !important; }</style>\"))\n",
    "\n",
    "from threading import Thread\n",
    "\n",
    "class StreamingThread(Thread):\n",
    "    def __init__(self, ssc):\n",
    "        Thread.__init__(self)\n",
    "        self.ssc = ssc\n",
    "    def run(self):\n",
    "        ssc.start()\n",
    "        ssc.awaitTermination()\n",
    "    def stop(self):\n",
    "        print('----- Stopping... this may take a few seconds -----')\n",
    "        self.ssc.stop(stopSparkContext=False, stopGraceFully=True)\n",
    "        \n",
    "# from pyspark import SparkContext\n",
    "# SparkContext.setSystemProperty('spark.executor.memory', '2g')\n",
    "\n",
    "from pyspark.ml import Pipeline\n",
    "        \n",
    "from pyspark.ml.feature import Tokenizer, RegexTokenizer, StopWordsRemover, HashingTF, IDF\n",
    "from pyspark.ml.feature import OneHotEncoder, StringIndexer, VectorAssembler,IndexToString\n",
    "\n",
    "from pyspark.sql.types import IntegerType, StringType, ArrayType, FloatType\n",
    "from pyspark.streaming import StreamingContext\n",
    "from pyspark.sql import Row\n",
    "from pyspark.sql.functions import udf, struct, array, col, lit, monotonically_increasing_id\n",
    "\n",
    "from pyspark.sql.types import StructType\n",
    "from pyspark.sql.types import StructField\n",
    "\n",
    "from pyspark.ml.classification import NaiveBayes, NaiveBayesModel, RandomForestClassifier\n",
    "from pyspark.ml.linalg import Vectors\n",
    "from pyspark.mllib.regression import LabeledPoint\n",
    "from pyspark.mllib.evaluation import MultilabelMetrics\n",
    "from pyspark.mllib.util import MLUtils\n",
    "\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "\n",
    "from nltk.stem.snowball import *\n",
    "\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "from sklearn.metrics import multilabel_confusion_matrix\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://192.168.0.222:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v2.4.5</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>PySparkShell</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        "
      ],
      "text/plain": [
       "<SparkContext master=local[*] appName=PySparkShell>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - hive</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://192.168.0.222:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v2.4.5</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>PySparkShell</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x11aaf7908>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading the data into a single dataframe\n",
    "\n",
    "The idea is to load all the saved partfiles into a single dataframe. Next this dataframe can be used to train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_rdd(base_directory):\n",
    "    # Get all the directory names of the saved myoutput folders\n",
    "    foldernames = os.listdir(base_directory)\n",
    "    \n",
    "    # Create list of full directorie names\n",
    "    full_directories = []\n",
    "    \n",
    "    for i in range(len(foldernames)):\n",
    "        \n",
    "        if foldernames[i] == '.DS_Store':\n",
    "            continue\n",
    "        \n",
    "        directory_temp = base_directory + \"/\" + foldernames[i]\n",
    "        full_directories.append(directory_temp)\n",
    "    \n",
    "    print(\"Number of directories included: \", len(full_directories))\n",
    "    \n",
    "    df = spark.read.format('json').load(path=full_directories)\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of directories included:  1120\n",
      "total nr of instances =   2703\n"
     ]
    }
   ],
   "source": [
    "# base_directory = r'/Users/Simon/Documents/GitHub/adana_task3/Spark_Cleaned/myoutput-1586797640000/part-00003'\n",
    "# base_directory = r'/Users/Simon/Documents/GitHub/adana_task3/Spark_Cleaned/myoutput-1586797640000'\n",
    "# base_directory = r'/Users/Simon/Documents/GitHub/adana_task3/Spark_Data_Limited'\n",
    "base_directory = r'/Users/Simon/Documents/GitHub/adana_task3/Spark_Cleaned'\n",
    "\n",
    "df = load_rdd(base_directory)\n",
    "print('total nr of instances =  ',len(df.toPandas()))\n",
    "# df.show()\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filtering the loaded data \n",
    "\n",
    "The goal of this step is to obtain a more balanced dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_data(dataframe): #input a spark dataframe\n",
    "    \n",
    "    df = dataframe.toPandas()\n",
    "    \n",
    "    df_unsafe = df[df['label'] == 'unsafe']\n",
    "    df_vandal = df[df['label'] == 'vandal']\n",
    "    \n",
    "    max_label = len(df_unsafe) + len(df_vandal)\n",
    "    \n",
    "    df_safe = df[df['label'] == 'safe'].sample(n = max_label,random_state=42)\n",
    "    \n",
    "    df = df_safe.append(df_unsafe)\n",
    "    df = df.append(df_vandal)\n",
    "    df = df.sample(frac=1).reset_index(drop=True)\n",
    "    df = sqlContext.createDataFrame(df)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total nr of instances =   692\n",
      "+--------------------+------+--------------------+--------------------+--------------------+--------------------+--------------------+\n",
      "|             comment| label|           name_user|            text_new|            text_old|          title_page|            url_page|\n",
      "+--------------------+------+--------------------+--------------------+--------------------+--------------------+--------------------+\n",
      "|Fix placename lin...|  safe|              Certes|{{About|the India...|{{Short descripti...|           Rajasthan|//en.wikipedia.or...|\n",
      "|→‎Geography and g...|  safe|            Bneu2013|{{Other uses|Copp...|{{Other uses|Copp...|Copper Basin (Ten...|//en.wikipedia.or...|\n",
      "|→‎Biography:that ...|  safe|        Lisapollison|{{short descripti...|{{short descripti...|     Marijohn Wilkin|//en.wikipedia.or...|\n",
      "|Update date forma...|  safe|     Rich Farmbrough|{{short descripti...|{{short descripti...|          Petr Málek|//en.wikipedia.or...|\n",
      "|    (→‎Plot summary)|unsafe|        73.25.240.42|{{refimprove|date...|{{refimprove|date...|Mrs. Frisby and t...|//en.wikipedia.or...|\n",
      "|(better now i think)|unsafe|     188.238.163.244|{{Other uses}}\n",
      "{{...|{{Other uses}}\n",
      "{{...|           Transport|//en.wikipedia.or...|\n",
      "|(Undid revision 9...|unsafe|           Zebuready|{{about|collision...|{{about|collision...|Collision avoidan...|//en.wikipedia.or...|\n",
      "|(→‎Famous residen...|unsafe|       107.242.113.9|{{Infobox settlem...|{{Infobox settlem...|Los Lunas, New Me...|//en.wikipedia.or...|\n",
      "|   Added Shippeitaro|  safe|         JasonFan123|{{About||folklore...|{{About||folklore...|  Japanese folktales|//en.wikipedia.or...|\n",
      "|→‎Correctional fa...|  safe|            JosephWC|{{short descripti...|{{short descripti...|List of locations...|//en.wikipedia.or...|\n",
      "|          (He is 32)|unsafe|        Nienkebakker|{{Use mdy dates|d...|{{Use mdy dates|d...|  Chris Wood (actor)|//en.wikipedia.or...|\n",
      "|                    |unsafe|2601:47:100:b020:...|{{short descripti...|{{short descripti...|   Cesaro (wrestler)|//en.wikipedia.or...|\n",
      "|                    |unsafe|       5.182.140.140|{{multiple issues...|{{multiple issues...|          Bread roll|//en.wikipedia.or...|\n",
      "|→‎Orders and meda...|  safe|  Twofingered Typist|{{short descripti...|{{short descripti...|James Morris Colq...|//en.wikipedia.or...|\n",
      "|             wording|  safe|     Interstellarity|[[Fiji]] is divid...|[[Fiji]] is divid...|Local government ...|//en.wikipedia.or...|\n",
      "|add authority con...|  safe|Ser Amantio di Ni...|[[File:Vinant Den...|[[File:Vinant Den...|Charles-Augustin ...|//en.wikipedia.or...|\n",
      "|             Reflist|  safe|              GünniX|{{Infobox scienti...|{{Infobox scienti...|Francisco José Vi...|//en.wikipedia.or...|\n",
      "|           (→‎Gooch)|unsafe|2a02:c7f:bc15:e00...|{{short descripti...|{{short descripti...|List of Henry Dan...|//en.wikipedia.or...|\n",
      "|    (→‎18th century)|unsafe|2605:a000:170a:48...|This is a list of...|This is a list of...|List of years in ...|//en.wikipedia.or...|\n",
      "|                    |  safe|             Cilidus|{{multiple issues...|{{Infobox Ambassa...|Lyndon Lowell Ols...|//en.wikipedia.or...|\n",
      "+--------------------+------+--------------------+--------------------+--------------------+--------------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "200"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_selection = filter_data(df)\n",
    "print('total nr of instances =  ',len(df_selection.toPandas()))\n",
    "df_selection.show()\n",
    "\n",
    "## To run faster we only take a selection of the data:\n",
    "# df_selection = df.toPandas().tail(2500)\n",
    "# df_selection = sqlContext.createDataFrame(df_selection)\n",
    "df_selection = df_selection.repartition(200)\n",
    "df_selection.rdd.getNumPartitions()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Checking the frequency of each label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-----+\n",
      "| label|count|\n",
      "+------+-----+\n",
      "|  safe|  346|\n",
      "|unsafe|  314|\n",
      "|vandal|   32|\n",
      "+------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_selection.groupBy(\"label\") \\\n",
    "    .count() \\\n",
    "    .orderBy(col(\"count\").desc()) \\\n",
    "    .show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1a: Tokenization & Normalization\n",
    "\n",
    "The regexTokenizer is used because of its extra functionality compared to the standard Tokenizer built into spark. Also useful is that the tokens are normalized (decapitalized). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(dataframe):\n",
    "    df = dataframe\n",
    "    \n",
    "    rt_old = RegexTokenizer(inputCol=\"text_old\", outputCol=\"words_old\", toLowercase=True, pattern=(\"\\\\W\"))\n",
    "    countTokens = udf(lambda words: len(words), IntegerType())\n",
    "\n",
    "    # regexTokenized_old = rt_old.transform(df)\n",
    "    # df_step1a = regexTokenized_old.withColumn(\"tokens_old\", countTokens(col(\"words_old\")))\n",
    "\n",
    "    regexTokenized_old = rt_old.transform(df)\n",
    "    df = regexTokenized_old.withColumn(\"tokens_old\", countTokens(col(\"words_old\")))\n",
    "\n",
    "    #########################################################################################\n",
    "\n",
    "    rt_new = RegexTokenizer(inputCol=\"text_new\", outputCol=\"words_new\", toLowercase=True, pattern=(\"\\\\W\"))\n",
    "    # regexTokenized_new = rt_new.transform(df_step1a)\n",
    "    # df_step1b = regexTokenized_new.withColumn(\"tokens_new\", countTokens(col(\"words_new\")))\n",
    "\n",
    "    regexTokenized_new = rt_new.transform(df)\n",
    "    df = regexTokenized_new.withColumn(\"tokens_new\", countTokens(col(\"words_new\")))\n",
    "\n",
    "    # df_step1b.show(truncate=False)\n",
    "    \n",
    "    print(\"Step 1a Done\")\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1b: Delta Generator\n",
    "\n",
    "In this crucial step the difference between input and output text is determined. The difference is found using the unified_diff function accesible in through the difflib python library. The function takes two lists of strings as inputs and computes the deleted and inserted (replaced) words. This difference is used to later classify the text edit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_difference(text_old,text_new):\n",
    "\n",
    "    new_words = []\n",
    "    deleted_words = []\n",
    "\n",
    "    for line in difflib.unified_diff(text_old, text_new, fromfile='before.txt', tofile='after.txt'):\n",
    "    #     sys.stdout.write(line)\n",
    "        if \"-\" in line and \" \" not in line:\n",
    "            new_line = line.replace(\"-\", \"\")\n",
    "            deleted_words.append(new_line)\n",
    "        elif \"+\" in line and \" \" not in line:\n",
    "            new_line = line.replace(\"+\", \"\")\n",
    "            new_words.append(new_line)\n",
    "\n",
    "    # print(\"Deleted words: \", deleted_words)\n",
    "    # print(\"Inserted words: \", new_words)\n",
    "\n",
    "    edited_words = deleted_words + new_words\n",
    "    \n",
    "    ## Need to built in a protection mechanism for some of the edits which are massive\n",
    "    ## Some pranksters copy the same sentence millions of times which breaks the vectorizer\n",
    "    \n",
    "    threshold_editsize = 300\n",
    "    massive_edit = 0.0\n",
    "    \n",
    "    if len(edited_words) >= threshold_editsize:\n",
    "        \n",
    "        massive_edit = 1.0\n",
    "        \n",
    "        if len(set(edited_words)) >= threshold_editsize:\n",
    "            edited_words = ['massive', 'edit']\n",
    "        \n",
    "        else:\n",
    "            edited_words = list(set(edited_words))\n",
    "            \n",
    "    \n",
    "    \n",
    "    return edited_words,massive_edit\n",
    "\n",
    "# text_old = df_step1b.select(\"words_old\").collect()[0][0]\n",
    "# text_new = df_step1b.select(\"words_new\").collect()[0][0]\n",
    "# edited_words = text_difference(text_old,text_new)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### User Defined Function\n",
    "\n",
    "This code calculated the difference between the input and output text. This is accomplished by defining a UDF and a seperate function arrayUdf(). The udf is called on two columns *'words_old'* and *'words_new'*. Next a lambda function is defined to iterate over each row of the two input columns. Within the udf is refered to another function arrayUdf() which requires two inputs: the two tokenized lists of words which will be used to compute the difference. The arrayUdf() function acts as an itermediary to call on a different function: text_difference(). The text_difference() function uses the unified_diff generator from the difflib package to return the deltas between two lists of strings.\n",
    "\n",
    "Through experimentation with the unified_diff generator, we found that it was much easier to first tokenize the input and output text and then compute the difference between the two tokenized lists of words. This in contrast to passing the two texts (*'text_old'* and *'text_new'*) of the rdd's as input directly and then tokenizing this *'difference_text'*. Although the latter method might create less computational overhead due to less tokenization, the former method proves to be much more reliable to determine which words have been deleted and which words are new."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def diff_text(dataframe):\n",
    "    df = dataframe\n",
    "    \n",
    "    def arrayUdf1(text_old,text_new):\n",
    "        edited_words = text_difference(text_old,text_new)[0]\n",
    "        return edited_words\n",
    "    \n",
    "    def arrayUdf2(text_old,text_new):\n",
    "        massive_edit = text_difference(text_old,text_new)[1]\n",
    "        return massive_edit\n",
    "\n",
    "    #calling udf function\n",
    "    callArrayUdf1 = udf(lambda row: arrayUdf1(row[0],row[1]), ArrayType(StringType()))\n",
    "    callArrayUdf2 = udf(lambda row: arrayUdf2(row[0],row[1]), FloatType())\n",
    "\n",
    "    #registering udf function\n",
    "    spark.udf.register(\"callArrayUdf1\",callArrayUdf1)\n",
    "    spark.udf.register(\"callArrayUdf2\",callArrayUdf2)\n",
    "\n",
    "    #results of udf function\n",
    "    df = df.withColumn(\"diff_text\", callArrayUdf1(struct('words_old','words_new')))\n",
    "    df = df.withColumn(\"massive_edit\", callArrayUdf2(struct('words_old','words_new')))\n",
    "    \n",
    "    print(\"Step 1b Done\")\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cross validate the change by checking the difference in number of tokens created\n",
    "\n",
    "# tokens_old = df_step1b.select(\"tokens_old\").collect()[0][0]\n",
    "# tokens_new = df_step1b.select(\"tokens_new\").collect()[0][0]\n",
    "\n",
    "# diff_tokens = tokens_new - tokens_old\n",
    "# print(\"The difference in number of tokens for input and output text = \", diff_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Stop Word Removal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def stop_words_removal(dataframe):\n",
    "\n",
    "    df = dataframe\n",
    "    \n",
    "    locale = sc._jvm.java.util.Locale\n",
    "    locale.setDefault(locale.forLanguageTag(\"en-US\"))\n",
    "\n",
    "    stopwords = StopWordsRemover.loadDefaultStopWords(\"english\")\n",
    "    extra_stopwords = [\"http\",\"https\",\"ref\",\"www\",\"com\",\"org\",\"url\",\"web\"]\n",
    "    stopwords = stopwords + extra_stopwords\n",
    "    # print(stopwords)\n",
    "\n",
    "    remover = StopWordsRemover(inputCol=\"diff_text\", outputCol=\"words_clean\",stopWords=stopwords)\n",
    "    stopwords = remover.getStopWords()\n",
    "\n",
    "\n",
    "    # df_step2 = remover.transform(df_step1c)\n",
    "\n",
    "    # df_step2.select(\"words_clean\").show(truncate=False)\n",
    "\n",
    "    df = remover.transform(df)\n",
    "\n",
    "    # (inputCol=\"words\", outputCol=\"filtered\",stopWords=StopWordsRemover.loadDefaultStopWords(\"english\"))\n",
    "    # remover.transform(df_tokenized).show(truncate=False)\n",
    "    \n",
    "    print(\"Step 2 Done\")\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Stemming\n",
    "\n",
    "The chosen algorithm for stemming is the snowball stemming algorithm (a variant of the Porter algorithm). The snowball stemmer was chosen because it is slightly more aggresive at stemming the tokenized words than the standard Porter algorithm while still being less aggresive than the Lancaster algorithm. It is a nice 'middle ground' between the two stemming variants."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stemming(dataframe):\n",
    "    \n",
    "    df = dataframe\n",
    "    \n",
    "    # stemmer = SnowballStemmer('english')\n",
    "    stemmer = PorterStemmer()\n",
    "\n",
    "    stemmer_udf = udf(lambda tokens: [stemmer.stem(token) for token in tokens], ArrayType(StringType()))\n",
    "\n",
    "    # df_step3 = df_step2.withColumn(\"words_stemmed\", stemmer_udf(\"words_clean\"))\n",
    "\n",
    "    df = df.withColumn(\"words_stemmed\", stemmer_udf(\"words_clean\"))\n",
    "\n",
    "    # df_step3.select(\"words_stemmed\").show(truncate=False)\n",
    "    \n",
    "    print(\"Step 3 Done\")\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Feature Vectorization (TF-IDF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vectorization(dataframe):\n",
    "    \n",
    "    df = dataframe\n",
    "    \n",
    "    tf = HashingTF(inputCol=\"words_stemmed\", outputCol=\"tf\")#, numFeatures=20)\n",
    "\n",
    "    # df_step4a = tf.transform(df_step3)\n",
    "    df = tf.transform(df)\n",
    "\n",
    "\n",
    "    idf = IDF(inputCol=\"tf\", outputCol=\"tf_idf\")\n",
    "    # idfModel = idf.fit(df_step4a)\n",
    "    # df_step4b = idfModel.transform(df_step4a)\n",
    "\n",
    "    idfModel = idf.fit(df)\n",
    "    df = idfModel.transform(df)\n",
    "\n",
    "    # df_step4a.show(truncate=False)\n",
    "    # df_step4b.select(\"words_stemmed\",\"tf_idf\").show(truncate=False)\n",
    "    \n",
    "    print(\"Step 4 Done\")\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: String Indexer\n",
    "\n",
    "In this final step the labels (*Safe, Unsafe and Vandal*) are encoded to label indices. The most frequent label gets index 0 while the least frequent label gets the last index depending on the number of indices. In this case the least frequent label gets index 2.\n",
    "\n",
    "In our data 0 corresponds to *Safe*, 1 corresponds to *Unsafe*, and 2 corresponds to *Vandal*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def string_indexer(dataframe):\n",
    "    \n",
    "    df = dataframe\n",
    "    \n",
    "    label_indexer = StringIndexer(inputCol = \"label\", outputCol = \"label_index\")\n",
    "    \n",
    "#     indexToLabel = label_indexer.labels\n",
    "    \n",
    "    # df_step5a = label_indexer.fit(df_step4b).transform(df_step4b)\n",
    "    # df_step5b = df_step5a.select(\"tf_idf\",\"label_index\")\n",
    "\n",
    "    #  # Renaming the columns\n",
    "    # df_final = df_step5b.withColumnRenamed(\"tf_idf\",\"features\")\n",
    "    # df_final = df_final.withColumnRenamed(\"label_index\",\"label\")\n",
    "\n",
    "    df = label_indexer.fit(df).transform(df)\n",
    "    \n",
    "\n",
    "    # # Renaming the columns\n",
    "    df_label = df.select(\"tf_idf\",\"label_index\")\n",
    "    df_label = df_label.withColumnRenamed(\"label_index\",\"actual_label\")\n",
    "\n",
    "    # df_final.show()\n",
    "    \n",
    "    print(\"Step 5 Done\")\n",
    "    return df , df_label #, indexToLabel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: Assembly of the preprocessing steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 1a Done\n",
      "Step 1b Done\n",
      "Step 2 Done\n",
      "Step 3 Done\n",
      "Step 4 Done\n",
      "Step 5 Done\n"
     ]
    }
   ],
   "source": [
    "def preprocessing(dataframe):\n",
    "    \n",
    "    \n",
    "    df = dataframe\n",
    "    \n",
    "    df = tokenize(df)\n",
    "    df = diff_text(df)\n",
    "    df = stop_words_removal(df)\n",
    "    df = stemming(df)\n",
    "    df = vectorization(df)\n",
    "    df , df_label = string_indexer(df)\n",
    "    \n",
    "    return df,df_label\n",
    "\n",
    "\n",
    "\n",
    "##### Preprocessing the training data #####\n",
    "df_final , df_label = preprocessing(df_selection)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part II: Ancillary Features\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# old_text = [\"car\", \"train\", \"boat\"]\n",
    "# new_text = [\"car\", \"train\", \"boat\", \"biiike\", \"pl4n3\"]\n",
    "# diff_text = [\"biiike\", \"pl4n3\", \"pl4n3\"]\n",
    "\n",
    "# word = 'biiike'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def maxRepeat(diff_text):\n",
    "    \n",
    "    h = len(diff_text)\n",
    "    count = 0\n",
    "    \n",
    "    for i in range(0, h):\n",
    "        l = len(diff_text[i])\n",
    "        \n",
    "        #Find the maximum repeating character\n",
    "                   \n",
    "        for j in range(0, l):\n",
    "            cur_count = 1\n",
    "            for k in range(j + 1, l):\n",
    "                if(diff_text[i][j] != diff_text[i][k]):\n",
    "                    break\n",
    "                cur_count += 1\n",
    "                                \n",
    "                #update result if required\n",
    "                if cur_count > count:\n",
    "                    count = cur_count\n",
    "                            \n",
    "    return count\n",
    "\n",
    "# print(maxRepeat(diff_text))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#empty revision\n",
    "##checks if edit is is empty or non empty\n",
    "\n",
    "def empty(text_list):\n",
    "    if len(text_list) == 0 or text_list[0] == \"empty\":\n",
    "        empty = 1\n",
    "    else:\n",
    "        empty = 0\n",
    "\n",
    "    return empty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ratio between old text and new text; if  > 1 new text is longer than old text\n",
    "#i would consider it vandal if there is a significant deviation from 1\n",
    "\n",
    "def size_ratio(old_text_list, new_text_list):\n",
    "    len_old_text = len(old_text_list)\n",
    "    len_new_text = len(new_text_list)\n",
    "    \n",
    "    if len_old_text == 0:\n",
    "        ratio = 0.0\n",
    "    else:\n",
    "        ratio = round(len_new_text / len_old_text,3)\n",
    "    \n",
    "    return ratio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#counts the alphanumberic strings in the diffrence list eg (dkfdj125kd,...) the strings with numbers and letters\n",
    "## Since these strings are likely to be vandal\n",
    "## Absolute count or ratio better?\n",
    "\n",
    "def alphanumeric_count(difference_list):\n",
    "    alpha_num = 0\n",
    "    for element in difference_list:\n",
    "        if element.isdigit():\n",
    "            continue\n",
    "        elif element.isalpha():\n",
    "            continue\n",
    "        else:\n",
    "            alpha_num += 1\n",
    "    \n",
    "    return alpha_num"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ratio of vulgar words in the edit\n",
    "\n",
    "def vulgar(old_text_list,difference_list):\n",
    "    if len(old_text_list) == 0:\n",
    "        old_text_list = ['empty']\n",
    "    \n",
    "    if len(difference_list) == 0:\n",
    "        difference_list = ['empty']\n",
    "    \n",
    "    vulgar_list_edit = pc.predict_prob(difference_list)\n",
    "    vulgar_list_old = pc.predict_prob(old_text_list)\n",
    "    count1 = 0\n",
    "    count2 = 0\n",
    "    \n",
    "    for i in vulgar_list_edit:\n",
    "        count1 += i\n",
    "        \n",
    "    for k in vulgar_list_old:\n",
    "        count2 += k\n",
    "        \n",
    "    ratio1 = count1 #/ len(difference_list)\n",
    "    \n",
    "    if count2 == 0:\n",
    "        ratio2 = count1\n",
    "    \n",
    "    else:\n",
    "        ratio2 = round(count1 / count2,3)\n",
    "        \n",
    "    return float(ratio2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Gives a similarity metric between original and new text\n",
    "#How less similar the more suspicous\n",
    "\n",
    "def similarity(old_text_list, new_text_list):\n",
    "    old = ''.join(old_text_list)\n",
    "    new = ''.join(new_text_list)\n",
    "    ratio = round(fuzz.token_set_ratio(old, new)/100,3)\n",
    "    \n",
    "    return ratio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_ancillary_features(dataframe):\n",
    "    df = dataframe\n",
    "    \n",
    "    ## UDF for computing longest repeated character\n",
    "    cafUdf1 = udf(lambda row: maxRepeat(row[2]), IntegerType())\n",
    "    spark.udf.register(\"cafUdf1\", cafUdf1)\n",
    "    df = df.withColumn(\"longest_repeated_char\", cafUdf1(struct('words_old','words_new','diff_text')))\n",
    "    \n",
    "    ## UDF for checking if edit is empty or not\n",
    "    cafUdf2 = udf(lambda row: empty(row[2]), IntegerType())\n",
    "    spark.udf.register(\"cafUdf2\", cafUdf2)\n",
    "    df = df.withColumn(\"empty_edit\", cafUdf2(struct('words_old','words_new','diff_text')))\n",
    "    \n",
    "    ## UDF to determine size ratio between input and output text\n",
    "    cafUdf3 = udf(lambda row: size_ratio(row[0],row[1]), FloatType())\n",
    "    spark.udf.register(\"cafUdf3\", cafUdf3)\n",
    "    df = df.withColumn(\"size_ratio\", cafUdf3(struct('words_old','words_new','diff_text')))\n",
    "    \n",
    "    ## UDF to determine number of alphanumeric words in an edit\n",
    "    cafUdf4 = udf(lambda row: alphanumeric_count(row[2]), IntegerType())\n",
    "    spark.udf.register(\"cafUdf4\", cafUdf4)\n",
    "    df = df.withColumn(\"alpha_count\", cafUdf4(struct('words_old','words_new','diff_text')))\n",
    "    \n",
    "    ## UDF to determine the ratio of vulgar words in the text\n",
    "    cafUdf5 = udf(lambda row: vulgar(row[0],row[2]), FloatType())\n",
    "    spark.udf.register(\"cafUdf5\", cafUdf5)\n",
    "    df = df.withColumn(\"vulgar_ratio\", cafUdf5(struct('words_old','words_new','diff_text')))\n",
    "    \n",
    "    ## UDF to determine the ratio of vulgar words in the text\n",
    "    cafUdf6 = udf(lambda row: similarity(row[0],row[1]), FloatType())\n",
    "    spark.udf.register(\"cafUdf6\", cafUdf6)\n",
    "    df = df.withColumn(\"similarity\", cafUdf6(struct('words_old','words_new','diff_text')))\n",
    "    \n",
    "    print(\"Ancillary Features Done\")\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ancillary Features Done\n",
      "+--------------------+------------+--------------------+---------------------+----------+----------+-----------+------------+----------+-----------+\n",
      "|           diff_text|massive_edit|              tf_idf|longest_repeated_char|empty_edit|size_ratio|alpha_count|vulgar_ratio|similarity|label_index|\n",
      "+--------------------+------------+--------------------+---------------------+----------+----------+-----------+------------+----------+-----------+\n",
      "|                  []|         0.0|      (262144,[],[])|                    0|         1|       1.0|          0|       0.002|       1.0|        0.0|\n",
      "|[i, realy, was, f...|         0.0|(262144,[125372,1...|                    0|         0|     1.002|          0|       0.002|       1.0|        1.0|\n",
      "|[entered, public,...|         0.0|(262144,[3268,605...|                    3|         0|     0.994|          0|       0.093|      0.94|        0.0|\n",
      "|[ford, 2011, ford...|         0.0|(262144,[31587,49...|                    2|         0|       1.0|          0|       0.002|       1.0|        1.0|\n",
      "|[br, buick, lacro...|         0.0|(262144,[77041,10...|                    2|         0|       1.0|          0|         0.0|       1.0|        1.0|\n",
      "|[still, gi, ni, n...|         1.0|(262144,[97,1769,...|                    3|         0|     0.964|          5|       0.014|      0.98|        1.0|\n",
      "|[multiple, issues...|         0.0|(262144,[50398,10...|                    2|         0|     1.003|          0|       0.003|       1.0|        0.0|\n",
      "|                  []|         0.0|      (262144,[],[])|                    0|         1|       1.0|          0|         0.0|       1.0|        0.0|\n",
      "|[and, the, showru...|         0.0|(262144,[5518,694...|                    2|         0|     1.003|          0|       0.003|       1.0|        1.0|\n",
      "|[yibo, aka, big, ...|         0.0|(262144,[30006,83...|                    2|         0|     1.001|          0|       0.001|       1.0|        1.0|\n",
      "|[camelcase, subpage]|         0.0|(262144,[111735,1...|                    0|         0|       1.0|          0|        0.32|      0.85|        0.0|\n",
      "|[website, publisher]|         0.0|(262144,[137153,1...|                    0|         0|       1.0|          0|       0.002|       1.0|        0.0|\n",
      "|[edward, tyree, i...|         0.0|(262144,[17252,17...|                    2|         0|     1.021|          0|       0.022|      0.99|        1.0|\n",
      "|[scope, row, styl...|         0.0|(262144,[28986,34...|                    2|         0|     1.023|          0|       0.022|      0.99|        1.0|\n",
      "|[greeville, green...|         0.0|(262144,[133329,2...|                    2|         0|       1.0|          0|       0.001|       1.0|        0.0|\n",
      "|[common, through,...|         0.0|(262144,[29945,13...|                    2|         0|     1.001|          0|       0.002|       1.0|        0.0|\n",
      "|[maintained, main...|         0.0|(262144,[9781,502...|                    2|         0|     1.001|          0|       0.003|       1.0|        1.0|\n",
      "|[k, preschool, gr...|         0.0|(262144,[128067,1...|                    2|         0|     1.001|          0|       0.002|       1.0|        0.0|\n",
      "|    [br, modernised]|         0.0|(262144,[40648,15...|                    0|         0|     1.001|          0|       0.001|       1.0|        0.0|\n",
      "|[nihongo, foot, s...|         0.0|(262144,[2325,486...|                    2|         0|     0.983|          0|       0.018|      0.99|        1.0|\n",
      "+--------------------+------------+--------------------+---------------------+----------+----------+-----------+------------+----------+-----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_ancillary = compute_ancillary_features(df_final)\n",
    "\n",
    "# pd_df_ancillary = df_ancillary.toPandas()\n",
    "# pd_df_ancillary = pd_df_ancillary[pd_df_ancillary['empty_edit'] == 0]\n",
    "# df_ancillary = sqlContext.createDataFrame(pd_df_ancillary)\n",
    "\n",
    "df_ancillary = df_ancillary.select('diff_text','massive_edit','longest_repeated_char','empty_edit','size_ratio','alpha_count',\\\n",
    "                                   'vulgar_ratio','similarity','label_index')\n",
    "df_ancillary.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>diff_text</th>\n",
       "      <th>massive_edit</th>\n",
       "      <th>longest_repeated_char</th>\n",
       "      <th>vulgar_ratio</th>\n",
       "      <th>label_index</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>73</th>\n",
       "      <td>[haemo, nowiki, lytic]</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.002</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>82</th>\n",
       "      <td>[and, there, is, an, actual, child, kyrie, ele...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2</td>\n",
       "      <td>0.021</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>104</th>\n",
       "      <td>[big, tucker, is, awesome, mom]</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.003</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>113</th>\n",
       "      <td>[wiktionary, card, card, or, the, card, may, r...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2</td>\n",
       "      <td>0.151</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>122</th>\n",
       "      <td>[jgomora, is, the, most, awesome, show, ever]</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.084</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>125</th>\n",
       "      <td>[any, of, the, people, who, smell, also, see, ...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2</td>\n",
       "      <td>0.200</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>155</th>\n",
       "      <td>[hey, hey, peopl, este, h, here]</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.018</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>193</th>\n",
       "      <td>[booty, juice]</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2</td>\n",
       "      <td>0.001</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>216</th>\n",
       "      <td>[natalia, aka, beanbear, loves, her, brother, ...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.026</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>251</th>\n",
       "      <td>[abby, lee, miller, is, famous, for, hit, danc...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2</td>\n",
       "      <td>0.006</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>259</th>\n",
       "      <td>[it, is, similar, to, the, arabic, andalusi, n...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2</td>\n",
       "      <td>0.352</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>261</th>\n",
       "      <td>[bang, me, daddy, oh, yeah, harder, by, mr, bi...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2</td>\n",
       "      <td>0.001</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>291</th>\n",
       "      <td>[il, est, un, musicien, bien, videmment, tr, s...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2</td>\n",
       "      <td>0.002</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>319</th>\n",
       "      <td>[to, help, people, if, they, dont, know, somet...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.051</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>323</th>\n",
       "      <td>[people, s, mums, have, big, hairy, fannys]</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2</td>\n",
       "      <td>0.001</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>329</th>\n",
       "      <td>[john, mahan, 2009, present, a, foster, kid, i...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2</td>\n",
       "      <td>0.080</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>330</th>\n",
       "      <td>[i, love, this, woman, she, is, an, icon]</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.001</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>341</th>\n",
       "      <td>[you, can, shove, it]</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.001</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>350</th>\n",
       "      <td>[his, mom, is, in, albuquerque, with, her, wif...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.034</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>365</th>\n",
       "      <td>[source, this, would, make, him, 12, when, he,...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2</td>\n",
       "      <td>0.066</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>370</th>\n",
       "      <td>[piiiiiiiinnnnnntttttttttoooooooo]</td>\n",
       "      <td>0.0</td>\n",
       "      <td>9</td>\n",
       "      <td>0.000</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>395</th>\n",
       "      <td>[a, religionislam, fake, is, islam, religion]</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>425</th>\n",
       "      <td>[rural]</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.001</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>446</th>\n",
       "      <td>[out, 18, births, name, was, purportedly, sue,...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4</td>\n",
       "      <td>0.378</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>460</th>\n",
       "      <td>[only, gypsy, boys, eat]</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.030</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>504</th>\n",
       "      <td>[who, is, the, publisher, where, is, the, pert...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.020</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>547</th>\n",
       "      <td>[brett, steven, lebda, rory, wilmshusrt, remem...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2</td>\n",
       "      <td>0.010</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>582</th>\n",
       "      <td>[its, nasty]</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.001</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>583</th>\n",
       "      <td>[i, hate, green, beans]</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2</td>\n",
       "      <td>0.007</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>626</th>\n",
       "      <td>[nowiki, farooqhoti, gulzar, khan, from, ppp]</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3</td>\n",
       "      <td>0.016</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>646</th>\n",
       "      <td>[did, you, know, that, barney, the, purple, di...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2</td>\n",
       "      <td>0.003</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>652</th>\n",
       "      <td>[hey, myrt, like, i, said, this, is, american,...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.020</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             diff_text  massive_edit  \\\n",
       "73                              [haemo, nowiki, lytic]           0.0   \n",
       "82   [and, there, is, an, actual, child, kyrie, ele...           0.0   \n",
       "104                    [big, tucker, is, awesome, mom]           0.0   \n",
       "113  [wiktionary, card, card, or, the, card, may, r...           0.0   \n",
       "122      [jgomora, is, the, most, awesome, show, ever]           0.0   \n",
       "125  [any, of, the, people, who, smell, also, see, ...           0.0   \n",
       "155                   [hey, hey, peopl, este, h, here]           0.0   \n",
       "193                                     [booty, juice]           0.0   \n",
       "216  [natalia, aka, beanbear, loves, her, brother, ...           0.0   \n",
       "251  [abby, lee, miller, is, famous, for, hit, danc...           0.0   \n",
       "259  [it, is, similar, to, the, arabic, andalusi, n...           0.0   \n",
       "261  [bang, me, daddy, oh, yeah, harder, by, mr, bi...           0.0   \n",
       "291  [il, est, un, musicien, bien, videmment, tr, s...           0.0   \n",
       "319  [to, help, people, if, they, dont, know, somet...           0.0   \n",
       "323        [people, s, mums, have, big, hairy, fannys]           0.0   \n",
       "329  [john, mahan, 2009, present, a, foster, kid, i...           0.0   \n",
       "330          [i, love, this, woman, she, is, an, icon]           0.0   \n",
       "341                              [you, can, shove, it]           0.0   \n",
       "350  [his, mom, is, in, albuquerque, with, her, wif...           0.0   \n",
       "365  [source, this, would, make, him, 12, when, he,...           0.0   \n",
       "370                 [piiiiiiiinnnnnntttttttttoooooooo]           0.0   \n",
       "395      [a, religionislam, fake, is, islam, religion]           1.0   \n",
       "425                                            [rural]           0.0   \n",
       "446  [out, 18, births, name, was, purportedly, sue,...           1.0   \n",
       "460                           [only, gypsy, boys, eat]           0.0   \n",
       "504  [who, is, the, publisher, where, is, the, pert...           0.0   \n",
       "547  [brett, steven, lebda, rory, wilmshusrt, remem...           0.0   \n",
       "582                                       [its, nasty]           0.0   \n",
       "583                            [i, hate, green, beans]           0.0   \n",
       "626      [nowiki, farooqhoti, gulzar, khan, from, ppp]           0.0   \n",
       "646  [did, you, know, that, barney, the, purple, di...           0.0   \n",
       "652  [hey, myrt, like, i, said, this, is, american,...           0.0   \n",
       "\n",
       "     longest_repeated_char  vulgar_ratio  label_index  \n",
       "73                       0         0.002          2.0  \n",
       "82                       2         0.021          2.0  \n",
       "104                      0         0.003          2.0  \n",
       "113                      2         0.151          2.0  \n",
       "122                      0         0.084          2.0  \n",
       "125                      2         0.200          2.0  \n",
       "155                      0         0.018          2.0  \n",
       "193                      2         0.001          2.0  \n",
       "216                      0         0.026          2.0  \n",
       "251                      2         0.006          2.0  \n",
       "259                      2         0.352          2.0  \n",
       "261                      2         0.001          2.0  \n",
       "291                      2         0.002          2.0  \n",
       "319                      0         0.051          2.0  \n",
       "323                      2         0.001          2.0  \n",
       "329                      2         0.080          2.0  \n",
       "330                      0         0.001          2.0  \n",
       "341                      0         0.001          2.0  \n",
       "350                      0         0.034          2.0  \n",
       "365                      2         0.066          2.0  \n",
       "370                      9         0.000          2.0  \n",
       "395                      0         0.000          2.0  \n",
       "425                      0         0.001          2.0  \n",
       "446                      4         0.378          2.0  \n",
       "460                      0         0.030          2.0  \n",
       "504                      0         0.020          2.0  \n",
       "547                      2         0.010          2.0  \n",
       "582                      0         0.001          2.0  \n",
       "583                      2         0.007          2.0  \n",
       "626                      3         0.016          2.0  \n",
       "646                      2         0.003          2.0  \n",
       "652                      0         0.020          2.0  "
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_temp = df_ancillary.toPandas()\n",
    "df_temp = df_temp[df_temp['label_index']==2.0]\n",
    "df_temp = df_temp[['diff_text','massive_edit','longest_repeated_char','vulgar_ratio','label_index']]\n",
    "df_temp.head(32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part III: Building Models\n",
    "\n",
    "## Multinomial Naive Bayes Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rdd_final = df_final.rdd\n",
    "(training_data_nb, test_data_nb) = df_label.randomSplit([0.7, 0.3], seed = 42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show the frequency of each label in the test set\n",
    "# training_data_nb.groupBy(\"actual_label\") \\\n",
    "#     .count() \\\n",
    "#     .orderBy(col(\"count\").desc()) \\\n",
    "#     .show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show the frequency of each label in the test set\n",
    "# test_data_nb.groupBy(\"actual_label\") \\\n",
    "#     .count() \\\n",
    "#     .orderBy(col(\"count\").desc()) \\\n",
    "#     .show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from pyspark.ml.classification import LogisticRegression\n",
    "\n",
    "# lr = LogisticRegression(maxIter=20, regParam=0.3, elasticNetParam=0)\n",
    "# lrModel = lr.fit(training_data)\n",
    "# predictions = lrModel.transform(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "nb = NaiveBayes(smoothing=1.0, modelType=\"multinomial\",featuresCol='tf_idf', labelCol='actual_label',thresholds = [0.99,0.99,0.05])\n",
    "model_nb = nb.fit(training_data_nb)\n",
    "predictions_nb = model_nb.transform(test_data_nb)\n",
    "\n",
    "# # Convert indexed labels back to original labels.\n",
    "# label_converter = IndexToString(inputCol=\"prediction\", outputCol=\"predictedLabel\",labels=indexToLabel)\n",
    "# predictions_nb = label_converter.fit(predictions_nb).transform(predictions_nb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predictions.filter(predictions['prediction'] == 1).select('label','probability','prediction').show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predictions.filter(predictions['prediction'] == 2).select('label','probability','prediction').show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predictions.filter(predictions['label'] == 1).select('label','probability','prediction').show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predictions.filter(predictions['label'] == 2).select('label','probability','prediction').show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Saving the Model\n",
    "Use pickle package to save a model to a file and load a model from a file.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# output_dir = r'/Users/Simon/Documents/GitHub/adana_task3'\n",
    "# shutil.rmtree(output_dir, ignore_errors=True)\n",
    "# model_nb.save(sc, output_dir)\n",
    "# sameModel = nb.load(sc, output_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forest Classisfier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----------+\n",
      "|            features|label_index|\n",
      "+--------------------+-----------+\n",
      "|[0.0,0.0,1.0,1.0,...|        0.0|\n",
      "|(7,[3,5,6],[1.001...|        1.0|\n",
      "|[0.0,3.0,0.0,0.99...|        0.0|\n",
      "|[0.0,2.0,0.0,1.0,...|        1.0|\n",
      "|(7,[1,3,6],[2.0,1...|        1.0|\n",
      "|[1.0,3.0,0.0,0.96...|        1.0|\n",
      "|[0.0,2.0,0.0,1.00...|        0.0|\n",
      "|(7,[2,3,6],[1.0,1...|        0.0|\n",
      "|[0.0,2.0,0.0,1.00...|        1.0|\n",
      "|[0.0,2.0,0.0,1.00...|        1.0|\n",
      "|(7,[3,5,6],[1.0,0...|        0.0|\n",
      "|(7,[3,5,6],[1.0,0...|        0.0|\n",
      "|[0.0,2.0,0.0,1.02...|        1.0|\n",
      "|[0.0,2.0,0.0,1.02...|        1.0|\n",
      "|[0.0,2.0,0.0,1.0,...|        0.0|\n",
      "|[0.0,2.0,0.0,1.00...|        0.0|\n",
      "|[0.0,2.0,0.0,1.00...|        1.0|\n",
      "|[0.0,2.0,0.0,1.00...|        0.0|\n",
      "|(7,[3,5,6],[1.001...|        0.0|\n",
      "|[0.0,2.0,0.0,0.98...|        1.0|\n",
      "+--------------------+-----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def vector_assembler(dataframe):\n",
    "    \n",
    "    df_ancillary = dataframe\n",
    "    assembler = VectorAssembler(inputCols=['massive_edit','longest_repeated_char','empty_edit','size_ratio','alpha_count','vulgar_ratio','similarity'],\\\n",
    "                                outputCol='features')\n",
    "\n",
    "    df_ancillary = assembler.transform(df_ancillary)\n",
    "    df_ancillary_vector = df_ancillary.select('features','label_index')\n",
    "    \n",
    "    return df_ancillary_vector\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----------+--------------------+--------------------+----------+\n",
      "|            features|label_index|       rawPrediction|         probability|prediction|\n",
      "+--------------------+-----------+--------------------+--------------------+----------+\n",
      "|[0.0,0.0,1.0,1.0,...|        0.0|[24.5447292525366...|[0.81815764175122...|       0.0|\n",
      "|[0.0,3.0,0.0,0.99...|        0.0|[8.66329371538427...|[0.28877645717947...|       1.0|\n",
      "|[1.0,3.0,0.0,0.96...|        1.0|[10.1696412642093...|[0.33898804214031...|       1.0|\n",
      "|(7,[3,5,6],[1.0,0...|        0.0|[17.2172226493089...|[0.57390742164363...|       0.0|\n",
      "|[0.0,2.0,0.0,1.0,...|        0.0|[8.73413083712336...|[0.29113769457077...|       1.0|\n",
      "|[0.0,2.0,0.0,1.00...|        1.0|[10.5573386590045...|[0.35191128863348...|       1.0|\n",
      "|(7,[1,3,6],[2.0,1...|        0.0|[8.73413083712336...|[0.29113769457077...|       1.0|\n",
      "|[0.0,2.0,0.0,0.98...|        1.0|[11.5818915382587...|[0.38606305127529...|       1.0|\n",
      "|(7,[3,5,6],[1.289...|        0.0|[21.6877058830691...|[0.72292352943563...|       0.0|\n",
      "|(7,[3,5,6],[1.014...|        0.0|[15.6839098677255...|[0.52279699559085...|       0.0|\n",
      "|(7,[3,5,6],[1.75,...|        0.0|[22.3543725497358...|[0.74514575165785...|       0.0|\n",
      "|[0.0,2.0,0.0,1.00...|        1.0|[9.19731236325816...|[0.30657707877527...|       1.0|\n",
      "|[0.0,4.0,0.0,0.85...|        1.0|[6.78683385579937...|[0.22622779519331...|       1.0|\n",
      "|(7,[3,5,6],[1.006...|        0.0|[15.7470399452112...|[0.52490133150704...|       0.0|\n",
      "|(7,[3,5,6],[1.006...|        0.0|[17.6190708310342...|[0.58730236103447...|       0.0|\n",
      "|(7,[3,5,6],[1.001...|        0.0|[17.9371314648420...|[0.59790438216140...|       0.0|\n",
      "| (7,[3,6],[1.0,1.0])|        1.0|[18.6355306731751...|[0.62118435577250...|       0.0|\n",
      "|(7,[3,5,6],[1.003...|        0.0|[18.1322472877465...|[0.60440824292488...|       0.0|\n",
      "|(7,[3,5,6],[1.016...|        0.0|[18.8206842109911...|[0.62735614036637...|       0.0|\n",
      "|(7,[3,5,6],[2.5,1...|        0.0|[22.3543725497358...|[0.74514575165785...|       0.0|\n",
      "+--------------------+-----------+--------------------+--------------------+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Split the data into training and test sets (30% held out for testing)\n",
    "(trainingData, testData) = df_ancillary_vector.randomSplit([0.7, 0.3], seed = 42)\n",
    "\n",
    "# Train a RandomForest model.\n",
    "rf = RandomForestClassifier(labelCol=\"label_index\", featuresCol=\"features\", numTrees=30)\n",
    "\n",
    "# Convert indexed labels back to original labels.\n",
    "# labelConverter = IndexToString(inputCol=\"prediction\", outputCol=\"predictedLabel\",\n",
    "#                                labels=labelIndexer.labels)\n",
    "\n",
    "# Chain indexers and forest in a Pipeline\n",
    "# pipeline = Pipeline(stages=[labelIndexer, featureIndexer, rf, labelConverter])\n",
    "\n",
    "# Train model.  This also runs the indexers.\n",
    "model_rf = rf.fit(trainingData)\n",
    "\n",
    "# Make predictions.\n",
    "predictions_rf = model_rf.transform(testData)\n",
    "predictions_rf.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part IV - A: Evaluation of the Naive Bayes Model\n",
    "\n",
    "## Accuracy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.48244730584541506\n"
     ]
    }
   ],
   "source": [
    "evaluator = MulticlassClassificationEvaluator(predictionCol=\"prediction\", labelCol='actual_label')\n",
    "accuracy_nb = evaluator.evaluate(predictions_nb)\n",
    "print(accuracy_nb)\n",
    "# Result = 0.7410821256831497\n",
    "# Result = 0.8053634745632435\n",
    "# Result = 0.8838408650684599\n",
    "# Result = 0.911560434636496"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Precision and Recall\n",
    "\n",
    "For the precision and recall measures we fall back on Sklearn packages because their implementation is much more straight forward as compared to the mllib packages. We are mostly interested in the unsafe and vandal isntances!\n",
    "\n",
    "**Binary Classification:**\n",
    "\n",
    "- Precision = TruePositives / (TruePositives + FalsePositives)\n",
    "- Recall = TruePositives / (TruePositives + FalseNegatives)\n",
    "\n",
    "**Multilabel Classification:**\n",
    "\n",
    "In an imbalanced classification problem with more than two classes, precision is calculated as the sum of true positives across all classes divided by the sum of true positives and false positives across all classes.\n",
    "\n",
    "- Precision = Sum c in C TruePositives_c / Sum c in C (TruePositives_c + FalsePositives_c)\n",
    "\n",
    "\n",
    "In an imbalanced classification problem with more than two classes, recall is calculated as the sum of true positives across all classes divided by the sum of true positives and false negatives across all classes.\n",
    "\n",
    "- Recall = Sum c in C TruePositives_c / Sum c in C (TruePositives_c + FalseNegatives_c)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.6208214103838786, 0.4642857142857143, 0.482447305845415, None)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd_predictions_nb = predictions_nb.toPandas()\n",
    "# print(pd_predictions_nb[pd_predictions_nb.prediction == 2.0].count())\n",
    "\n",
    "\n",
    "y_true = pd_predictions_nb['actual_label'].to_list()\n",
    "y_pred = pd_predictions_nb['prediction'].to_list()\n",
    "\n",
    "# precision_recall_fscore_support(y_true, y_pred, average='micro')\n",
    "# precision_recall_fscore_support(y_true, y_pred, average='macro')\n",
    "precision_recall_fscore_support(y_true, y_pred, average='weighted')\n",
    "# precision_recall_fscore_support(y_true, y_pred, average=None, labels=[0.0, 1.0, 2.0])\n",
    "# pd_predictions_nb.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Confusion Matrices and Manual Calculations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Pred_Other</th>\n",
       "      <th>Pred_2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Act_Other</th>\n",
       "      <td>170</td>\n",
       "      <td>44</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Act_2</th>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           Pred_Other  Pred_2\n",
       "Act_Other         170      44\n",
       "Act_2              10       0"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cm_arr = multilabel_confusion_matrix(y_true, y_pred,labels=[0.0, 1.0, 2.0])\n",
    "cm_arr_safe = cm_arr[0,:,:]\n",
    "cm_arr_unsafe = cm_arr[1,:,:]\n",
    "cm_arr_vandal = cm_arr[2,:,:]\n",
    "\n",
    "cm_safe = pd.DataFrame({'Pred_Other': cm_arr_safe[:,0], 'Pred_0': cm_arr_safe[:,1]})\n",
    "cm_safe = cm_safe.rename(index={0: 'Act_Other', 1: 'Act_0'})\n",
    "\n",
    "cm_unsafe = pd.DataFrame({'Pred_Other': cm_arr_unsafe[:,0], 'Pred_1': cm_arr_unsafe[:,1]})\n",
    "cm_unsafe = cm_unsafe.rename(index={0: 'Act_Other', 1: 'Act_1'})\n",
    "\n",
    "cm_vandal = pd.DataFrame({'Pred_Other': cm_arr_vandal[:,0], 'Pred_2': cm_arr_vandal[:,1]})\n",
    "cm_vandal = cm_vandal.rename(index={0: 'Act_Other', 1: 'Act_2'})\n",
    "\n",
    "cm_vandal.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision for Label 0:  0.7674418604651163\n",
      "Precision for Label 1:  0.5182481751824818\n",
      "Precision for Label 2:  0.0\n",
      "Recall for Label 0:  0.2920353982300885\n",
      "Recall for Label 1:  0.7029702970297029\n",
      "Recall for Label 2:  0.0\n",
      "Total Precision:  0.4642857142857143\n",
      "Total Recall:  0.4642857142857143\n"
     ]
    }
   ],
   "source": [
    "precision_safe = cm_arr_safe[1,1]/(cm_arr_safe[1,1] + cm_arr_safe[0,1])\n",
    "precision_unsafe = cm_arr_unsafe[1,1]/(cm_arr_unsafe[1,1] + cm_arr_unsafe[0,1])\n",
    "precision_vandal = cm_arr_vandal[1,1]/(cm_arr_vandal[1,1] + cm_arr_vandal[0,1])\n",
    "\n",
    "recall_safe = cm_arr_safe[1,1]/(cm_arr_safe[1,1] + cm_arr_safe[1,0])\n",
    "recall_unsafe = cm_arr_unsafe[1,1]/(cm_arr_unsafe[1,1] + cm_arr_unsafe[1,0])\n",
    "recall_vandal = cm_arr_vandal[1,1]/(cm_arr_vandal[1,1] + cm_arr_vandal[1,0])\n",
    "\n",
    "print(\"Precision for Label 0: \", precision_safe)\n",
    "print(\"Precision for Label 1: \", precision_unsafe)\n",
    "print(\"Precision for Label 2: \", precision_vandal)\n",
    "\n",
    "print(\"Recall for Label 0: \", recall_safe)\n",
    "print(\"Recall for Label 1: \", recall_unsafe)\n",
    "print(\"Recall for Label 2: \", recall_vandal)\n",
    "\n",
    "tp_tot = cm_arr_safe[1,1]+cm_arr_unsafe[1,1]+cm_arr_vandal[1,1]\n",
    "fp_tot = cm_arr_safe[0,1]+cm_arr_unsafe[0,1]+cm_arr_vandal[0,1]\n",
    "fn_tot = cm_arr_safe[1,0]+cm_arr_unsafe[1,0]+cm_arr_vandal[1,0]\n",
    "\n",
    "precision_tot = (tp_tot / (tp_tot + fp_tot))\n",
    "recall_tot = ( tp_tot / (tp_tot + fn_tot ))\n",
    "\n",
    "print(\"Total Precision: \", precision_tot)\n",
    "print(\"Total Recall: \", recall_tot)\n",
    "              "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part IV - B: Evaluation of the Random Forest Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6005569643385751\n"
     ]
    }
   ],
   "source": [
    "evaluator2 = MulticlassClassificationEvaluator(predictionCol=\"prediction\", labelCol='label_index')\n",
    "accuracy_rf = evaluator2.evaluate(predictions_rf)\n",
    "print(accuracy_rf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.5858528919776506, 0.6160714285714286, 0.6005569643385751, None)"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd_predictions_rf = predictions_rf.toPandas()\n",
    "# print(pd_predictions_nb[pd_predictions_nb.prediction == 2.0].count())\n",
    "\n",
    "\n",
    "y_true = pd_predictions_rf['label_index'].to_list()\n",
    "y_pred = pd_predictions_rf['prediction'].to_list()\n",
    "\n",
    "# precision_recall_fscore_support(y_true, y_pred, average='micro')\n",
    "# precision_recall_fscore_support(y_true, y_pred, average='macro')\n",
    "precision_recall_fscore_support(y_true, y_pred, average='weighted')\n",
    "# precision_recall_fscore_support(y_true, y_pred, average=None, labels=[0.0, 1.0, 2.0])\n",
    "# pd_predictions_nb.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Pred_Other</th>\n",
       "      <th>Pred_2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Act_Other</th>\n",
       "      <td>211</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Act_2</th>\n",
       "      <td>12</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           Pred_Other  Pred_2\n",
       "Act_Other         211       1\n",
       "Act_2              12       0"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cm_arr = multilabel_confusion_matrix(y_true, y_pred,labels=[0.0, 1.0, 2.0])\n",
    "cm_arr_safe = cm_arr[0,:,:]\n",
    "cm_arr_unsafe = cm_arr[1,:,:]\n",
    "cm_arr_vandal = cm_arr[2,:,:]\n",
    "\n",
    "cm_safe = pd.DataFrame({'Pred_Other': cm_arr_safe[:,0], 'Pred_0': cm_arr_safe[:,1]})\n",
    "cm_safe = cm_safe.rename(index={0: 'Act_Other', 1: 'Act_0'})\n",
    "\n",
    "cm_unsafe = pd.DataFrame({'Pred_Other': cm_arr_unsafe[:,0], 'Pred_1': cm_arr_unsafe[:,1]})\n",
    "cm_unsafe = cm_unsafe.rename(index={0: 'Act_Other', 1: 'Act_1'})\n",
    "\n",
    "cm_vandal = pd.DataFrame({'Pred_Other': cm_arr_vandal[:,0], 'Pred_2': cm_arr_vandal[:,1]})\n",
    "cm_vandal = cm_vandal.rename(index={0: 'Act_Other', 1: 'Act_2'})\n",
    "\n",
    "cm_vandal.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision for Label 0:  0.6052631578947368\n",
      "Precision for Label 1:  0.6330275229357798\n",
      "Precision for Label 2:  0.0\n",
      "Recall for Label 0:  0.6448598130841121\n",
      "Recall for Label 1:  0.6571428571428571\n",
      "Recall for Label 2:  0.0\n",
      "Total Precision:  0.6160714285714286\n",
      "Total Recall:  0.6160714285714286\n"
     ]
    }
   ],
   "source": [
    "precision_safe = cm_arr_safe[1,1]/(cm_arr_safe[1,1] + cm_arr_safe[0,1])\n",
    "precision_unsafe = cm_arr_unsafe[1,1]/(cm_arr_unsafe[1,1] + cm_arr_unsafe[0,1])\n",
    "precision_vandal = cm_arr_vandal[1,1]/(cm_arr_vandal[1,1] + cm_arr_vandal[0,1])\n",
    "\n",
    "recall_safe = cm_arr_safe[1,1]/(cm_arr_safe[1,1] + cm_arr_safe[1,0])\n",
    "recall_unsafe = cm_arr_unsafe[1,1]/(cm_arr_unsafe[1,1] + cm_arr_unsafe[1,0])\n",
    "recall_vandal = cm_arr_vandal[1,1]/(cm_arr_vandal[1,1] + cm_arr_vandal[1,0])\n",
    "\n",
    "print(\"Precision for Label 0: \", precision_safe)\n",
    "print(\"Precision for Label 1: \", precision_unsafe)\n",
    "print(\"Precision for Label 2: \", precision_vandal)\n",
    "\n",
    "print(\"Recall for Label 0: \", recall_safe)\n",
    "print(\"Recall for Label 1: \", recall_unsafe)\n",
    "print(\"Recall for Label 2: \", recall_vandal)\n",
    "\n",
    "tp_tot = cm_arr_safe[1,1]+cm_arr_unsafe[1,1]+cm_arr_vandal[1,1]\n",
    "fp_tot = cm_arr_safe[0,1]+cm_arr_unsafe[0,1]+cm_arr_vandal[0,1]\n",
    "fn_tot = cm_arr_safe[1,0]+cm_arr_unsafe[1,0]+cm_arr_vandal[1,0]\n",
    "\n",
    "precision_tot = (tp_tot / (tp_tot + fp_tot))\n",
    "recall_tot = ( tp_tot / (tp_tot + fn_tot ))\n",
    "\n",
    "print(\"Total Precision: \", precision_tot)\n",
    "print(\"Total Recall: \", recall_tot)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part V: Running the Models in a Streaming Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "globals()['models_loaded'] = False\n",
    "\n",
    "def process(time, rdd):\n",
    "    if rdd.isEmpty():\n",
    "        return\n",
    "    \n",
    "    print(\"===================== %s =====================\" % str(time))\n",
    "    \n",
    "    ## Convert to data frame\n",
    "    df_pred = spark.read.json(rdd)\n",
    "    print(\"Incoming Dataframe: \")\n",
    "    df_pred.show()\n",
    "\n",
    "    \n",
    "    ## Preprocessing the incoming dataframe \n",
    "    df_pred_final , df_pred_label = preprocessing(df_pred)\n",
    "    print(\"Preprocessed Dataframe: \")\n",
    "    df_pred_final.show()\n",
    "    \n",
    "    ## Computing ancillary features\n",
    "    df_pred_ancillary = compute_ancillary_features(df_final)\n",
    "    df_ancillary_vector = vector_assembler(df_pred_ancillary)\n",
    "    \n",
    "    ## Load in the model if not yet loaded:\n",
    "    if not globals()['models_loaded']:\n",
    "        # load in your models here\n",
    "        globals()['my_model_nb'] = model_nb\n",
    "        globals()['my_model_rf'] = model_rf\n",
    "        globals()['models_loaded'] = True #Update the control to notify model is loaded\n",
    "        \n",
    "    # Predict using the loaded model: \n",
    "    df_result_nb = globals()['my_model_nb'].transform(df_pred_label)\n",
    "    df_result_rf = globals()['my_model_rf'].transform(df_ancillary_vector)\n",
    "    \n",
    "    print(\"Predicted Result for Naive Bayes Classifier: \")\n",
    "    df_result_nb.show()\n",
    "    \n",
    "    print(\"Predicted Result for Random Forest Classifier: \")\n",
    "    df_result_rf.show()\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ssc = StreamingContext(sc, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lines = ssc.socketTextStream(\"seppe.net\", 7778)\n",
    "# lines.foreachRDD(process)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ssc_t = StreamingThread(ssc)\n",
    "# ssc_t.start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ssc_t.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
