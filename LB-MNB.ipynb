{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Advanced Analytics - Assignment 3\n",
    "\n",
    "This assignment aims to predict the validity of Wikipedia Edits. The goal is to build a model which can classify incoming (stream) Wikipedia edits as Safe, Unsafe or Vandal. This notebook starts off by giving an overview of the contents. \n",
    "\n",
    "## Overview of the Notebook\n",
    "\n",
    "> **Part 0.** Problem Description <br>\n",
    "> **Part 1.** Data Loading and Filtering <br>\n",
    "> **Part 2.** Preprocessing Data <br>\n",
    "> **Part 3.** Computing Ancillary Features <br>\n",
    "> **Part 4.** Training Models <br>\n",
    "> **Part 5.** Evaluating Models <br>\n",
    "> **Part 6.** Comparing Model Performance <br>\n",
    "> **Part 7.** Employing Models in Streaming Setup<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 0: Problem Description\n",
    "\n",
    "The goal of this assignment is to classify incoming wikipedia edits. The classifier should be able to differentiate between three classes: *safe, unsafe, vandal*. The most challenging part in this assignment is the type of data which is used namely text (semi-structured data). This makes model implementation extra tough because preprocissing is very essential here. Another challenge is the way in which predictions must be made. The predictions must be such that incoming data can be immediatly classified i.s.o. a collected dataset without labels. \n",
    "\n",
    "To start, we will take a closer look at the data and how it is constructed. \n",
    "The data that has been streamed is text-based and formatted as a JSON dictionary with the following keys:\n",
    "\n",
    "- <b>Title page:</b> Title of the Wikipedia page.\n",
    "\n",
    "- <b>Text_new:</b> Text after the edit.\n",
    "\n",
    "- <b>Text_old:</b> Text before the edit.\n",
    "\n",
    "- <b>Name_user:</b> The user that edited the page. If the user is registered, their user name will show. In case of an anonymous edit, the user will be identified by his/her IP address at the time of editing.\n",
    "\n",
    "- <b>Label:</b> Label is the target feature which we aim to predict. The possible values are 'safe', 'unsafe' or 'vandal'.\n",
    "\n",
    "- <b>Comment:</b> The editor is asked to summarize the changes that have been made to the page. It is considered good practice to provide a summary about the edit, however, the user is free to leave this field blank, therefore empty values can occur.\n",
    "\n",
    "- <b>URL_page:</b> The URL of the Wikipedia page that was edited."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part I: Data Loading and Filtering\n",
    "\n",
    "##  Import & Initialization\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "//anaconda3/lib/python3.7/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.\n",
      "  warnings.warn(msg, category=FutureWarning)\n",
      "//anaconda3/lib/python3.7/site-packages/sklearn/base.py:318: UserWarning: Trying to unpickle estimator CountVectorizer from version 0.20.2 when using version 0.22.2.post1. This might lead to breaking code or invalid results. Use at your own risk.\n",
      "  UserWarning)\n",
      "//anaconda3/lib/python3.7/site-packages/sklearn/utils/deprecation.py:144: FutureWarning: The sklearn.svm.classes module is  deprecated in version 0.22 and will be removed in version 0.24. The corresponding classes / functions should instead be imported from sklearn.svm. Anything that cannot be imported from sklearn.svm is now part of the private API.\n",
      "  warnings.warn(message, FutureWarning)\n",
      "//anaconda3/lib/python3.7/site-packages/sklearn/base.py:318: UserWarning: Trying to unpickle estimator LinearSVC from version 0.20.2 when using version 0.22.2.post1. This might lead to breaking code or invalid results. Use at your own risk.\n",
      "  UserWarning)\n",
      "//anaconda3/lib/python3.7/site-packages/sklearn/utils/deprecation.py:144: FutureWarning: The sklearn.preprocessing.label module is  deprecated in version 0.22 and will be removed in version 0.24. The corresponding classes / functions should instead be imported from sklearn.preprocessing. Anything that cannot be imported from sklearn.preprocessing is now part of the private API.\n",
      "  warnings.warn(message, FutureWarning)\n",
      "//anaconda3/lib/python3.7/site-packages/sklearn/base.py:318: UserWarning: Trying to unpickle estimator LabelEncoder from version 0.20.2 when using version 0.22.2.post1. This might lead to breaking code or invalid results. Use at your own risk.\n",
      "  UserWarning)\n",
      "//anaconda3/lib/python3.7/site-packages/sklearn/base.py:318: UserWarning: Trying to unpickle estimator _SigmoidCalibration from version 0.20.2 when using version 0.22.2.post1. This might lead to breaking code or invalid results. Use at your own risk.\n",
      "  UserWarning)\n",
      "//anaconda3/lib/python3.7/site-packages/sklearn/base.py:318: UserWarning: Trying to unpickle estimator CalibratedClassifierCV from version 0.20.2 when using version 0.22.2.post1. This might lead to breaking code or invalid results. Use at your own risk.\n",
      "  UserWarning)\n",
      "//anaconda3/lib/python3.7/site-packages/fuzzywuzzy/fuzz.py:11: UserWarning: Using slow pure-python SequenceMatcher. Install python-Levenshtein to remove this warning\n",
      "  warnings.warn('Using slow pure-python SequenceMatcher. Install python-Levenshtein to remove this warning')\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>.container { width:95% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import emoji\n",
    "import re as re\n",
    "import difflib\n",
    "import shutil\n",
    "import sys\n",
    "import os\n",
    "\n",
    "import profanity_check as pc\n",
    "from fuzzywuzzy import fuzz\n",
    "\n",
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:95% !important; }</style>\"))\n",
    "\n",
    "from threading import Thread\n",
    "\n",
    "class StreamingThread(Thread):\n",
    "    def __init__(self, ssc):\n",
    "        Thread.__init__(self)\n",
    "        self.ssc = ssc\n",
    "    def run(self):\n",
    "        ssc.start()\n",
    "        ssc.awaitTermination()\n",
    "    def stop(self):\n",
    "        print('----- Stopping... this may take a few seconds -----')\n",
    "        self.ssc.stop(stopSparkContext=False, stopGraceFully=True)\n",
    "        \n",
    "# from pyspark import SparkContext\n",
    "# SparkContext.setSystemProperty('spark.executor.memory', '2g')\n",
    "\n",
    "from pyspark.ml import Pipeline\n",
    "        \n",
    "from pyspark.ml.feature import Tokenizer, RegexTokenizer, StopWordsRemover, HashingTF, IDF\n",
    "from pyspark.ml.feature import OneHotEncoder, StringIndexer, VectorAssembler,IndexToString\n",
    "\n",
    "from pyspark.sql.types import IntegerType, StringType, ArrayType, FloatType\n",
    "from pyspark.streaming import StreamingContext\n",
    "from pyspark.sql import Row\n",
    "from pyspark.sql.functions import udf, struct, array, col, lit, monotonically_increasing_id\n",
    "\n",
    "from pyspark.sql.types import StructType\n",
    "from pyspark.sql.types import StructField\n",
    "\n",
    "from pyspark.ml.classification import NaiveBayes, NaiveBayesModel, RandomForestClassifier\n",
    "from pyspark.ml.linalg import Vectors\n",
    "from pyspark.mllib.regression import LabeledPoint\n",
    "from pyspark.mllib.evaluation import MultilabelMetrics\n",
    "from pyspark.mllib.util import MLUtils\n",
    "\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "\n",
    "from nltk.stem.snowball import *\n",
    "\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "from sklearn.metrics import multilabel_confusion_matrix\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://192.168.0.222:4042\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v2.4.5</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>PySparkShell</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        "
      ],
      "text/plain": [
       "<SparkContext master=local[*] appName=PySparkShell>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - hive</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://192.168.0.222:4042\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v2.4.5</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>PySparkShell</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x116953978>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the data into a single dataframe\n",
    "\n",
    "First, data was captured from the streaming server. The data that will be used for training and testing was collected over a period of a couple of weeks (with some break in between due to spark time-out problems).\n",
    "\n",
    "Secondly, the streamed data should be filtered. Many folders out of the streamed data do not contain any 'part files'. It is easiest to delete these empty folders before preprocessing the data and training the model, since they do not contain any useful data. This was done in a seperate python file (see python file: *'DeleteEmptyDirectory'*)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_rdd(base_directory):\n",
    "    # Get all the directory names of the saved myoutput folders\n",
    "    foldernames = os.listdir(base_directory)\n",
    "    \n",
    "    # Create list of full directorie names\n",
    "    full_directories = []\n",
    "    \n",
    "    for i in range(len(foldernames)):\n",
    "        \n",
    "        if foldernames[i] == '.DS_Store':\n",
    "            continue\n",
    "        \n",
    "        directory_temp = base_directory + \"/\" + foldernames[i]\n",
    "        full_directories.append(directory_temp)\n",
    "    \n",
    "    print(\"Number of directories included: \", len(full_directories))\n",
    "    \n",
    "    df = spark.read.format('json').load(path=full_directories)\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of directories included:  2845\n",
      "total nr of instances =   7841\n"
     ]
    }
   ],
   "source": [
    "# base_directory = r'/Users/Simon/Documents/GitHub/adana_task3/Spark_Cleaned/myoutput-1586797640000/part-00003'\n",
    "# base_directory = r'/Users/Simon/Documents/GitHub/adana_task3/Spark_Cleaned/myoutput-1586797640000'\n",
    "# base_directory = r'/Users/Simon/Documents/GitHub/adana_task3/Spark_Data_Limited'\n",
    "# base_directory = r'/Users/Simon/Documents/GitHub/adana_task3/Spark_Cleaned'\n",
    "base_directory = r'/Users/Simon/Documents/GitHub/adana_task3/AllData'\n",
    "\n",
    "df = load_rdd(base_directory)\n",
    "print('total nr of instances =  ',len(df.toPandas()))\n",
    "# df.show()\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Filtering the loaded data \n",
    "\n",
    "After loading all the data, the data is filtered to create a more balanced dataset. Because the vandal and unsafe instances are rather infrequent, all those instances are optained. Next triple the amount of safe instances are sampled as compared to vandal instances from the loaded data. For the unsafe instances only double the instances as compared to the vandal instances are sampled from the data. This is done so that the natural frequenty of labels occuring still remains the same (safe > unsafe > vandal) but this difference is more balanced. Finally the data is repartitioned because spark places a task limit of 100kb. This helps spark with work overloads."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_data(dataframe): #input a spark dataframe\n",
    "    \n",
    "    df = dataframe.toPandas()\n",
    "    \n",
    "    df_vandal = df[df['label'] == 'vandal']\n",
    "    df_unsafe = df[df['label'] == 'unsafe']\n",
    "    \n",
    "    max_label_unsafe = int(min(3*len(df_vandal),len(df_unsafe))) # + len(df_unsafe) \n",
    "    max_label_safe = 4*len(df_vandal)\n",
    "    \n",
    "    df_unsafe = df[df['label'] == 'unsafe'].sample(n = max_label_unsafe,random_state=42)\n",
    "    df_safe = df[df['label'] == 'safe'].sample(n = max_label_safe,random_state=42)\n",
    "    \n",
    "    df = df_safe.append(df_vandal)\n",
    "    df = df.append(df_unsafe)\n",
    "    df = df.sample(frac=1).reset_index(drop=True)\n",
    "    df = sqlContext.createDataFrame(df)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total nr of instances =   728\n"
     ]
    }
   ],
   "source": [
    "df_selection = filter_data(df)\n",
    "print('total nr of instances =  ',len(df_selection.toPandas()))\n",
    "df_selection.toPandas().head()\n",
    "\n",
    "## To run faster we only take a selection of the data:\n",
    "# df_selection = df.toPandas().tail(2500)\n",
    "# df_selection = sqlContext.createDataFrame(df_selection)\n",
    "df_selection = df_selection.repartition(200)\n",
    "# df_selection.rdd.getNumPartitions()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Checking the frequency of each label\n",
    "\n",
    "It is useful to know how many instances of each type are included in the dataset. This amount is restricted by the number of vandal labels but can be adjusted to include more unsafe and vandal labels. Note that in a later process these labels will be converted to an index (0,1 and 2) ordered from most frequent to least frequent (to save memory). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-----+\n",
      "| label|count|\n",
      "+------+-----+\n",
      "|  safe|  364|\n",
      "|unsafe|  273|\n",
      "|vandal|   91|\n",
      "+------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_selection.groupBy(\"label\") \\\n",
    "    .count() \\\n",
    "    .orderBy(col(\"count\").desc()) \\\n",
    "    .show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part II: Preprocessing the Document Edits\n",
    "\n",
    "For a better accuracy of the wikipedie text edit classification, our goal is to combine two approaches. The first approach is to use text vectorization and a naive bayes model to predict the type of edit. The second approach is to classify the text edit based on ancillary features. These features can be described as *meta-data* of the text. They do not check the actual words in the edit but rather look at more abstract features of the edit. These two approaches are compared and we define a way to combine these results in an ensemble."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1a: Tokenization & Normalization\n",
    "\n",
    "The values for 'new_text' and 'old_text' are still plain text. Therefore, a tokenizer is needed which converts the text into (text_old, text_new) into a list of words (words_old, words_new). \n",
    "<br>\n",
    "\n",
    "The regexTokenizer is used because of its additional functionality compared to the standard Tokenizer that is built into Spark. One of these additional functionalities is that the tokens will automatically be normalized (decapitalized).\n",
    "<br>\n",
    "\n",
    "An extra functionality included is to also calculate the number of tokens created. This could be used further in the ancillary feature computations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(dataframe):\n",
    "    df = dataframe\n",
    "    \n",
    "    rt_old = RegexTokenizer(inputCol=\"text_old\", outputCol=\"words_old\", toLowercase=True, pattern=(\"\\\\W\"))\n",
    "    countTokens = udf(lambda words: len(words), IntegerType())\n",
    "\n",
    "    regexTokenized_old = rt_old.transform(df)\n",
    "    df = regexTokenized_old.withColumn(\"tokens_old\", countTokens(col(\"words_old\")))\n",
    "\n",
    "    #########################################################################################\n",
    "\n",
    "    rt_new = RegexTokenizer(inputCol=\"text_new\", outputCol=\"words_new\", toLowercase=True, pattern=(\"\\\\W\"))\n",
    "\n",
    "    regexTokenized_new = rt_new.transform(df)\n",
    "    df = regexTokenized_new.withColumn(\"tokens_new\", countTokens(col(\"words_new\")))\n",
    "    \n",
    "    print(\"Step 1a Done\")\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1b: Delta Generator\n",
    "\n",
    "The goal is to determine the difference between old_text and new_text, in order to find what has been modified on the Wikipedia webpage. The new feature 'diff_text' will be created. This feature shows the exact changes made to the text, omitting the part of the text that is the same before and after the edit (resp. old_text and new_text). \n",
    "\n",
    "Several types of changes could occur:\n",
    " - Spelling changes: <i> misteries --> mysteries</i>\n",
    " - Grammar changes: <i> On the country side, I ride my bike --> I ride my bike on the country side </i>\n",
    " - Drastic changes, such as completely new text, deleted text, ... <br>\n",
    "\n",
    "So the essence of a change lies in either deleted words or newly added words. Replaced words are a combination of a deleted word and a newly added word.\n",
    "\n",
    "### User defined function\n",
    "\n",
    "The code calculated the difference between the input and output text. This is accomplished by defining a UDF and a seperate function arrayUdf(). The udf is called on two columns *'words_old'* and *'words_new'*. Next a lambda function is defined to iterate over each row of the two input columns. Within the udf is refered to another function arrayUdf() which requires two inputs: the two tokenized lists of words which will be used to compute the difference. The arrayUdf() function acts as an itermediary to call on a different function: text_difference(). The text_difference() function uses the unified_diff generator from the difflib package to return the deltas between two lists of strings. \n",
    "\n",
    "Through experimentation with the unified_diff generator, we found that it was much easier to first tokenize the input and output text and then compute the difference between the two tokenized lists of words. This in contrast to passing the two texts (*'text_old'* and *'text_new'*) of the rdd's as input directly and then tokenizing this *'diff_text'*. Although the latter method might create less computational overhead due to less tokenization, the former method proves to be much more reliable (and less complex) to determine which words have been deleted and which words are new.\n",
    "\n",
    "### text_difference() function - extra essential functionality\n",
    "\n",
    "As mentioned above this function computes the deleted and newly added words by comparing two tokenized lists of words of the text input and output. This function is essential for another reason as well. Through experimentation it was found that some vandal edits include the repitition of the same words millions of times. The difference in text, at first, did not identify repeated (or unique) words in the edit. This caused problems later on in the process when a vectorization of the difference was computed. Because of the large edit with repeated words, the vectorization broke down and the process halted. Therefore this function also identifies these truely huge edits and either returns a list of unique words or fills in the edit with two words: *'massive', 'edit'*. This was essential to prevent the vectorization from breaking down during training but also in the employment of the pipeline. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_difference(text_old,text_new):\n",
    "\n",
    "    new_words = []\n",
    "    deleted_words = []\n",
    "\n",
    "    for line in difflib.unified_diff(text_old, text_new, fromfile='before.txt', tofile='after.txt'):\n",
    "    #     sys.stdout.write(line)\n",
    "        if \"-\" in line and \" \" not in line:\n",
    "            new_line = line.replace(\"-\", \"\")\n",
    "            deleted_words.append(new_line)\n",
    "        elif \"+\" in line and \" \" not in line:\n",
    "            new_line = line.replace(\"+\", \"\")\n",
    "            new_words.append(new_line)\n",
    "\n",
    "    # print(\"Deleted words: \", deleted_words)\n",
    "    # print(\"Inserted words: \", new_words)\n",
    "\n",
    "    edited_words = deleted_words + new_words\n",
    "    \n",
    "    ## Need to built in a protection mechanism for some of the edits which are massive\n",
    "    ## Some pranksters copy the same sentence millions of times which breaks the vectorizer\n",
    "    \n",
    "    threshold_editsize = 300\n",
    "    massive_edit = 0.0\n",
    "    \n",
    "    if len(edited_words) >= threshold_editsize:\n",
    "        \n",
    "        massive_edit = 1.0\n",
    "        \n",
    "        if len(set(edited_words)) >= threshold_editsize:\n",
    "            edited_words = ['massive', 'edit']\n",
    "        \n",
    "        else:\n",
    "            edited_words = list(set(edited_words))\n",
    "            \n",
    "    \n",
    "    \n",
    "    return edited_words,massive_edit\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def diff_text(dataframe):\n",
    "    df = dataframe\n",
    "    \n",
    "    def arrayUdf1(text_old,text_new):\n",
    "        edited_words = text_difference(text_old,text_new)[0]\n",
    "        return edited_words\n",
    "    \n",
    "    def arrayUdf2(text_old,text_new):\n",
    "        massive_edit = text_difference(text_old,text_new)[1]\n",
    "        return massive_edit\n",
    "\n",
    "    #calling udf function\n",
    "    callArrayUdf1 = udf(lambda row: arrayUdf1(row[0],row[1]), ArrayType(StringType()))\n",
    "    callArrayUdf2 = udf(lambda row: arrayUdf2(row[0],row[1]), FloatType())\n",
    "\n",
    "    #registering udf function\n",
    "    spark.udf.register(\"callArrayUdf1\",callArrayUdf1)\n",
    "    spark.udf.register(\"callArrayUdf2\",callArrayUdf2)\n",
    "\n",
    "    #results of udf function\n",
    "    df = df.withColumn(\"diff_text\", callArrayUdf1(struct('words_old','words_new')))\n",
    "    df = df.withColumn(\"massive_edit\", callArrayUdf2(struct('words_old','words_new')))\n",
    "    \n",
    "    print(\"Step 1b Done\")\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Stop Word Removal\n",
    "\n",
    "Stop-words should be filtered out before any training can happen. These stop-words are words that are very common in natural language. nltk.org provides a useful package of different stemmers. Ontop of this, it is possible to add custom stopwords like: *http, web, com, www, ...* . This allows for a smaller set of words which should be considered by the vectorization. Furthermore, these words occur in many text edits and therefore have only limited power in predicting the class of the edit. \n",
    "\n",
    "<br>\n",
    "This procedure will leave us with a new column namely '<i>words_clean</i>'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def stop_words_removal(dataframe):\n",
    "\n",
    "    df = dataframe\n",
    "    \n",
    "    locale = sc._jvm.java.util.Locale\n",
    "    locale.setDefault(locale.forLanguageTag(\"en-US\"))\n",
    "\n",
    "    stopwords = StopWordsRemover.loadDefaultStopWords(\"english\")\n",
    "    extra_stopwords = [\"http\",\"https\",\"ref\",\"www\",\"com\",\"org\",\"url\",\"web\"]\n",
    "    stopwords = stopwords + extra_stopwords\n",
    "    # print(stopwords)\n",
    "\n",
    "    remover = StopWordsRemover(inputCol=\"diff_text\", outputCol=\"words_clean\",stopWords=stopwords)\n",
    "    stopwords = remover.getStopWords()\n",
    "\n",
    "\n",
    "    # df_step2 = remover.transform(df_step1c)\n",
    "\n",
    "    # df_step2.select(\"words_clean\").show(truncate=False)\n",
    "\n",
    "    df = remover.transform(df)\n",
    "\n",
    "    # (inputCol=\"words\", outputCol=\"filtered\",stopWords=StopWordsRemover.loadDefaultStopWords(\"english\"))\n",
    "    # remover.transform(df_tokenized).show(truncate=False)\n",
    "    \n",
    "    print(\"Step 2 Done\")\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Stemming\n",
    "\n",
    "Several stemming algorithms exist, of which the Porter algorithm is one of the least aggressive ones. The Snowball stemmer is slightly more aggressive at stemming the tokenized words, while being less aggressive than the Lancaster algorithm. <br>\n",
    "We started off by trying the Snowball stemmer since it seemed like a nice 'middle ground' between the other two stemming variants. However, after inspecting the results, this stemmer still turned out to be too aggressive, stemming words as 'country' to 'countr' or 'thing' to 'th'. Therefore, the chosen algorithm for stemming is the Porter algorithm.<br>\n",
    "\n",
    "<br>\n",
    "This will result in the dataset '<i>words_stemmed</i>'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stemming(dataframe):\n",
    "    \n",
    "    df = dataframe\n",
    "    \n",
    "    # stemmer = SnowballStemmer('english')\n",
    "    stemmer = PorterStemmer()\n",
    "\n",
    "    stemmer_udf = udf(lambda tokens: [stemmer.stem(token) for token in tokens], ArrayType(StringType()))\n",
    "\n",
    "    # df_step3 = df_step2.withColumn(\"words_stemmed\", stemmer_udf(\"words_clean\"))\n",
    "\n",
    "    df = df.withColumn(\"words_stemmed\", stemmer_udf(\"words_clean\"))\n",
    "\n",
    "    # df_step3.select(\"words_stemmed\").show(truncate=False)\n",
    "    \n",
    "    print(\"Step 3 Done\")\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Feature Vectorization (TF-IDF)\n",
    "\n",
    "In this step, the list of stemmed words is converted to a vector representation. First the TF is calculated which is required as input to compute IDF. The result is a column named TF-IDF which is the input column for the Naive Bayes model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vectorization(dataframe,pred):\n",
    "    \n",
    "    df = dataframe\n",
    "    \n",
    "    tf = HashingTF(inputCol=\"words_stemmed\", outputCol=\"tf\")#, numFeatures=20)\n",
    "\n",
    "    # df_step4a = tf.transform(df_step3)\n",
    "    df = tf.transform(df)\n",
    "\n",
    "\n",
    "    idf = IDF(inputCol=\"tf\", outputCol=\"tf_idf\")\n",
    "    # idfModel = idf.fit(df_step4a)\n",
    "    # df_step4b = idfModel.transform(df_step4a)\n",
    "\n",
    "    if pred == False:\n",
    "        globals()['idfModel'] = idf.fit(df)\n",
    "        \n",
    "#     idfModel = idf.fit(df)\n",
    "    df = globals()['idfModel'].transform(df)\n",
    "\n",
    "    # df_step4a.show(truncate=False)\n",
    "    # df_step4b.select(\"words_stemmed\",\"tf_idf\").show(truncate=False)\n",
    "    \n",
    "    print(\"Step 4 Done\")\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: String Indexer\n",
    "\n",
    "In this final step the labels (*Safe, Unsafe and Vandal*) are encoded to label indices. The most frequent label gets index 0 while the least frequent label gets the last index depending on the number of indices. In this case the least frequent label gets index 2.\n",
    "\n",
    "In our data 0 corresponds to *Safe*, 1 corresponds to *Unsafe*, and 2 corresponds to *Vandal*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def string_indexer(dataframe,pred):\n",
    "    \n",
    "    df_train = dataframe\n",
    "    \n",
    "    label_indexer = StringIndexer(inputCol = \"label\", outputCol = \"label_index\")\n",
    "    \n",
    "#     indexToLabel = label_indexer.labels\n",
    "    \n",
    "    # df_step5a = label_indexer.fit(df_step4b).transform(df_step4b)\n",
    "    # df_step5b = df_step5a.select(\"tf_idf\",\"label_index\")\n",
    "\n",
    "    #  # Renaming the columns\n",
    "    # df_final = df_step5b.withColumnRenamed(\"tf_idf\",\"features\")\n",
    "    # df_final = df_final.withColumnRenamed(\"label_index\",\"label\")\n",
    "    \n",
    "    if pred == False:\n",
    "        globals()['label_indexer_model'] = label_indexer.fit(df_train)\n",
    "        \n",
    "    df_train = globals()['label_indexer_model'].transform(df_train)\n",
    "\n",
    "    ## Renaming the columns\n",
    "    df_label = df_train.select(\"tf_idf\",\"label_index\")\n",
    "    df_label = df_label.withColumnRenamed(\"label_index\",\"actual_label\")\n",
    "    \n",
    "    print(\"Step 5 Done\")\n",
    "    return df_train, df_label #, df_pred, df_label_pred #, indexToLabel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: Assembly of the preprocessing steps\n",
    "\n",
    "Finally this step combines all the steps so that they can be performed by calling a single function. The function returns two dataframes: a dataframe including all columns and a dataframe only including the columns *'tf_idf'* and *'label_index'*. Note the boolean entry as *False* which indicates that the input dataframe is being used as training data and not for prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 1a Done\n",
      "Step 1b Done\n",
      "Step 2 Done\n",
      "Step 3 Done\n",
      "Step 4 Done\n",
      "Step 5 Done\n"
     ]
    }
   ],
   "source": [
    "def preprocessing(dataframe,pred):\n",
    "    \n",
    "    df = dataframe\n",
    "    \n",
    "    df = tokenize(df)\n",
    "    df = diff_text(df)\n",
    "    df = stop_words_removal(df)\n",
    "    df = stemming(df)\n",
    "    df = vectorization(df,pred)\n",
    "    \n",
    "    df, df_label = string_indexer(df,pred)\n",
    "    \n",
    "    return df , df_label \n",
    "\n",
    "\n",
    "\n",
    "##### Preprocessing the training data #####\n",
    "df_final , df_label = preprocessing(df_selection,False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part III: Ancillary Features\n",
    "\n",
    "In total 7 ancillary features are computed to be used in a random forest classifier. Below the features are briefly discussed.\n",
    "\n",
    "- <b>Massive Edit:</b> This feature was computed when computing the difference in text. It return 0 for small edits and 1 for large edits according to a threshold value. This is saved in the column *'massive_edit'*.\n",
    "\n",
    "- <b>Longest repeated character:</b> The function 'maxRepeat' identifies the length of the longest repeated character. Anything larger than 3 could be considered an unsafe or vandal edit. This is saved in the column *'longest_repeated_char'*.\n",
    "\n",
    "- <b>Empty Edit:</b> The function checks if the edit was empty or not. This is saved in the column *'empty_edit'*.\n",
    "\n",
    "- <b>Size Ratio:</b> This function computes the size difference between input and output text. It is important to note that this is already limited by the threshold used to identify massive edits. Results larger than 1 say that the number of words have increased. This is saved in the column *'size_ratio'*.\n",
    "\n",
    "- <b>Alphanumeric Count:</b> This function returns a integer value. A value larger than 0 means that the edit includes at least one word which has both letters and numbers. These are usualy invalid words which could prove a vandal edit. This is saved in the column *'alpha_count'*.\n",
    "\n",
    "- <b>Vulgar Language:</b> Vulgar language is a key indicator of vandalism on wikipedia. This function identifies the presence of these words. The function compares the vulgar words in the edit with the vulgar words in the original text and computes a ratio. If the number of words are the same than this could indicate that the page might just be about something vulgar assuming that an original text before the edit is always safe. If the number of vandal words increases then this could be considered as vandal. This is saved in the column *'vulgar_ratio'*.\n",
    "\n",
    "- <b>Similarity:</b> Last but not least the similarity between old and new texts are compared. Very similar text might point to grammatical edits or small factual changes which are likely safe. This is saved in the column *'similarity'*.\n",
    "\n",
    "All these functions are combined in a single function, *compute_ancillary_features()*, to make calling upon the functions easier. This again requires the use of user defined functions to pass an instance (a row of the dataframe) to each function. \n",
    "\n",
    "### Important Note:\n",
    "\n",
    "Many more features could be computed which cover more types of vandalism. The first part of improving the scores for the random forest model would be to define extra ancillary features. Some examples are: the user name, whether a comment was filled in, vulgar words in the comment, etc.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def maxRepeat(diff_text):\n",
    "    h = len(diff_text)\n",
    "    count = 0\n",
    "    for i in range(0, h):\n",
    "        l = len(diff_text[i])\n",
    "        #Find the maximum repeating character    \n",
    "        for j in range(0, l):\n",
    "            cur_count = 1\n",
    "            for k in range(j + 1, l):\n",
    "                if(diff_text[i][j] != diff_text[i][k]):\n",
    "                    break\n",
    "                cur_count += 1\n",
    "                                \n",
    "                #update result if required\n",
    "                if cur_count > count:\n",
    "                    count = cur_count\n",
    "                            \n",
    "    return count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#empty revision\n",
    "##checks if edit is is empty or non empty\n",
    "\n",
    "def empty(text_list):\n",
    "    if len(text_list) == 0 or text_list[0] == \"empty\":\n",
    "        empty = 1\n",
    "    else:\n",
    "        empty = 0\n",
    "\n",
    "    return empty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ratio between old text and new text; if  > 1 new text is longer than old text\n",
    "#i would consider it vandal if there is a significant deviation from 1\n",
    "\n",
    "def size_ratio(old_text_list, new_text_list):\n",
    "    len_old_text = len(old_text_list)\n",
    "    len_new_text = len(new_text_list)\n",
    "    \n",
    "    if len_old_text == 0:\n",
    "        ratio = 0.0\n",
    "    else:\n",
    "        ratio = round(len_new_text / len_old_text,3)\n",
    "    \n",
    "    return ratio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#counts the alphanumberic strings in the diffrence list eg (dkfdj125kd,...) the strings with numbers and letters\n",
    "## Since these strings are likely to be vandal\n",
    "## Absolute count or ratio better?\n",
    "\n",
    "def alphanumeric_count(difference_list):\n",
    "    alpha_num = 0\n",
    "    for element in difference_list:\n",
    "        if element.isdigit():\n",
    "            continue\n",
    "        elif element.isalpha():\n",
    "            continue\n",
    "        else:\n",
    "            alpha_num += 1\n",
    "    \n",
    "    return alpha_num"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ratio of vulgar words in the edit\n",
    "\n",
    "def vulgar(old_text_list,difference_list):\n",
    "    if len(old_text_list) == 0:\n",
    "        old_text_list = ['empty']\n",
    "    \n",
    "    if len(difference_list) == 0:\n",
    "        difference_list = ['empty']\n",
    "    \n",
    "    vulgar_list_edit = pc.predict_prob(difference_list)\n",
    "    vulgar_list_old = pc.predict_prob(old_text_list)\n",
    "    count1 = 0\n",
    "    count2 = 0\n",
    "    \n",
    "    for i in vulgar_list_edit:\n",
    "        count1 += i\n",
    "        \n",
    "    for k in vulgar_list_old:\n",
    "        count2 += k\n",
    "        \n",
    "    ratio1 = count1 #/ len(difference_list)\n",
    "    \n",
    "    if count2 == 0:\n",
    "        ratio2 = count1\n",
    "    \n",
    "    else:\n",
    "        ratio2 = round(count1 / count2,3)\n",
    "        \n",
    "    return float(ratio2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Gives a similarity metric between original and new text\n",
    "#How less similar the more suspicous\n",
    "\n",
    "def similarity(old_text_list, new_text_list):\n",
    "    old = ''.join(old_text_list)\n",
    "    new = ''.join(new_text_list)\n",
    "    ratio = round(fuzz.token_set_ratio(old, new)/100,3)\n",
    "    \n",
    "    return ratio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_ancillary_features(dataframe):\n",
    "    df = dataframe\n",
    "    \n",
    "    ## UDF for computing longest repeated character\n",
    "    cafUdf1 = udf(lambda row: maxRepeat(row[2]), IntegerType())\n",
    "    spark.udf.register(\"cafUdf1\", cafUdf1)\n",
    "    df = df.withColumn(\"longest_repeated_char\", cafUdf1(struct('words_old','words_new','diff_text')))\n",
    "    \n",
    "    ## UDF for checking if edit is empty or not\n",
    "    cafUdf2 = udf(lambda row: empty(row[2]), IntegerType())\n",
    "    spark.udf.register(\"cafUdf2\", cafUdf2)\n",
    "    df = df.withColumn(\"empty_edit\", cafUdf2(struct('words_old','words_new','diff_text')))\n",
    "    \n",
    "    ## UDF to determine size ratio between input and output text\n",
    "    cafUdf3 = udf(lambda row: size_ratio(row[0],row[1]), FloatType())\n",
    "    spark.udf.register(\"cafUdf3\", cafUdf3)\n",
    "    df = df.withColumn(\"size_ratio\", cafUdf3(struct('words_old','words_new','diff_text')))\n",
    "    \n",
    "    ## UDF to determine number of alphanumeric words in an edit\n",
    "    cafUdf4 = udf(lambda row: alphanumeric_count(row[2]), IntegerType())\n",
    "    spark.udf.register(\"cafUdf4\", cafUdf4)\n",
    "    df = df.withColumn(\"alpha_count\", cafUdf4(struct('words_old','words_new','diff_text')))\n",
    "    \n",
    "    ## UDF to determine the ratio of vulgar words in the text\n",
    "    cafUdf5 = udf(lambda row: vulgar(row[0],row[2]), FloatType())\n",
    "    spark.udf.register(\"cafUdf5\", cafUdf5)\n",
    "    df = df.withColumn(\"vulgar_ratio\", cafUdf5(struct('words_old','words_new','diff_text')))\n",
    "    \n",
    "    ## UDF to determine the ratio of vulgar words in the text\n",
    "    cafUdf6 = udf(lambda row: similarity(row[0],row[1]), FloatType())\n",
    "    spark.udf.register(\"cafUdf6\", cafUdf6)\n",
    "    df = df.withColumn(\"similarity\", cafUdf6(struct('words_old','words_new','diff_text')))\n",
    "    \n",
    "    print(\"Ancillary Features Done\")\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ancillary Features Done\n",
      "+--------------------+------------+---------------------+----------+----------+-----------+------------+----------+-----------+------+\n",
      "|           diff_text|massive_edit|longest_repeated_char|empty_edit|size_ratio|alpha_count|vulgar_ratio|similarity|label_index| label|\n",
      "+--------------------+------------+---------------------+----------+----------+-----------+------------+----------+-----------+------+\n",
      "|[trying, to, kill...|         0.0|                    2|         0|     1.001|          0|       0.001|       1.0|        2.0|vandal|\n",
      "|[https, www, wash...|         0.0|                    3|         0|     0.987|          6|       0.027|      0.99|        1.0|unsafe|\n",
      "|[collapsible, sta...|         0.0|                    2|         0|     0.999|          0|       0.007|      0.99|        0.0|  safe|\n",
      "|[new, super, luig...|         0.0|                    2|         0|     0.999|          0|       0.008|      0.99|        1.0|unsafe|\n",
      "|                  []|         0.0|                    0|         1|       1.0|          0|         0.0|       1.0|        0.0|  safe|\n",
      "|      [i, miss, you]|         0.0|                    2|         0|       1.0|          0|         0.0|       1.0|        1.0|unsafe|\n",
      "|[october, 2019, a...|         0.0|                    0|         0|       1.0|          0|       0.005|       1.0|        0.0|  safe|\n",
      "|[cendol, file, ce...|         0.0|                    2|         0|     0.988|          4|       0.011|      0.98|        0.0|  safe|\n",
      "|[a, sandwich, an,...|         0.0|                    0|         0|     1.003|          1|       0.009|      0.99|        1.0|unsafe|\n",
      "|[persecution, of,...|         0.0|                    2|         0|       1.0|          0|       0.001|       1.0|        1.0|unsafe|\n",
      "|[columns, list, c...|         0.0|                    0|         0|       1.0|          1|         0.0|       1.0|        1.0|unsafe|\n",
      "|     [massive, edit]|         1.0|                    2|         0|     0.473|          0|       0.002|      0.62|        1.0|unsafe|\n",
      "|[17, name, chatur...|         1.0|                    3|         0|     0.767|          4|       0.115|      0.68|        1.0|unsafe|\n",
      "|[she, is, in, lov...|         0.0|                    2|         0|     1.002|          0|       0.001|       1.0|        1.0|unsafe|\n",
      "|                  []|         0.0|                    0|         1|       1.0|          0|         0.0|       1.0|        0.0|  safe|\n",
      "|[holland, holland...|         0.0|                    2|         0|       1.0|          0|         0.0|       1.0|        1.0|unsafe|\n",
      "|             [small]|         0.0|                    2|         0|       1.0|          0|         0.0|       1.0|        0.0|  safe|\n",
      "|[https, www, norr...|         0.0|                    3|         0|     1.001|          0|       0.001|       1.0|        1.0|unsafe|\n",
      "|[george, h, w, bu...|         0.0|                    3|         0|     1.001|          0|       0.001|       1.0|        0.0|  safe|\n",
      "|                  []|         0.0|                    0|         1|       1.0|          0|       0.002|       1.0|        0.0|  safe|\n",
      "+--------------------+------------+---------------------+----------+----------+-----------+------------+----------+-----------+------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_ancillary = compute_ancillary_features(df_final)\n",
    "\n",
    "df_ancillary = df_ancillary.select('diff_text','massive_edit','longest_repeated_char','empty_edit','size_ratio','alpha_count',\\\n",
    "                                   'vulgar_ratio','similarity','label_index','label')\n",
    "df_ancillary.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part IV: Building Models\n",
    "\n",
    "## Multinomial Naive Bayes Classifier\n",
    "\n",
    "In this section a Naive Bayes Classifier is trained. The reason for using the naive bayes classifier is that this has been a typical choice in literature for text classification. Its accuracy is not necessarily the best but it does perform quite fast. A paper was found which described a logit boosted naive bayes model which proved very powerful for vandalism detection in text (see pdf included with this file). Due to the complexity of implementing this model in spark and time restrictions this goal was replaced by a simpler naive bayes classifier. \n",
    "\n",
    "To improve our accuracy of correctly predicting unsafe and vandal instances the threshold is of these probabilities are adapted. We place more importance on misclassifying a vandal instances as non-vandal than a safe instance as non-safe. There are two ways this can be communicated to the model. Either through a weights column which places weights on each instances or more easily by changing the probability threshold which was done here with promising results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "(training_data_nb, test_data_nb) = df_label.randomSplit([0.7, 0.3], seed = 8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "nb = NaiveBayes(smoothing=1.0, modelType=\"multinomial\",featuresCol='tf_idf', labelCol='actual_label')#,thresholds = [0.90,0.90,0.1])\n",
    "model_nb = nb.fit(training_data_nb)\n",
    "predictions_nb = model_nb.transform(test_data_nb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+------------+--------------------+--------------------+----------+\n",
      "|              tf_idf|actual_label|       rawPrediction|         probability|prediction|\n",
      "+--------------------+------------+--------------------+--------------------+----------+\n",
      "|(262144,[70684,92...|         0.0|[-332.52900393838...|[0.00585986948510...|       1.0|\n",
      "|      (262144,[],[])|         0.0|[-0.6597780393853...|[0.51696606786427...|       0.0|\n",
      "|(262144,[37834,41...|         0.0|[-112.78421136106...|[0.99969648172287...|       0.0|\n",
      "|(262144,[56267,92...|         1.0|[-296.36196046527...|[0.52087504590657...|       0.0|\n",
      "|(262144,[116287,1...|         1.0|[-85.501579696501...|[0.21160592435479...|       1.0|\n",
      "|(262144,[103513,1...|         1.0|[-222.43641485880...|[0.52919615926166...|       0.0|\n",
      "|(262144,[15554,16...|         0.0|[-1375.3708107105...|[3.90932356075996...|       1.0|\n",
      "|(262144,[16657,17...|         0.0|[-1042.1645805325...|[6.26278142615098...|       1.0|\n",
      "|(262144,[40782,11...|         1.0|[-143.42922232240...|[6.29797909864210...|       1.0|\n",
      "|(262144,[12512,15...|         0.0|[-1845.8115126504...|[1.15594602625712...|       1.0|\n",
      "|(262144,[15664,21...|         0.0|[-1053.1203963251...|[1.40286494453537...|       1.0|\n",
      "|(262144,[14531,28...|         0.0|[-2095.3669684024...|[1.0,1.1430804255...|       0.0|\n",
      "|(262144,[133631],...|         2.0|[-74.585323645858...|[0.52681269574992...|       0.0|\n",
      "|(262144,[17252,17...|         1.0|[-1561.0560145091...|[0.25819096432008...|       1.0|\n",
      "|(262144,[34957,50...|         0.0|[-176.51352421712...|[0.99750130139102...|       0.0|\n",
      "|(262144,[20055],[...|         0.0|[-74.585323645858...|[0.52681269574992...|       0.0|\n",
      "|(262144,[2548,934...|         1.0|[-385.41540938364...|[4.63525452098692...|       1.0|\n",
      "|(262144,[4977,605...|         1.0|[-2470.0000632488...|[0.99999999051524...|       0.0|\n",
      "|(262144,[188167],...|         0.0|[-43.828429637431...|[0.04226379956815...|       1.0|\n",
      "|(262144,[6665,249...|         1.0|[-1673.4395087251...|[2.18904851860880...|       1.0|\n",
      "+--------------------+------------+--------------------+--------------------+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "predictions_nb.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forest Classisfier\n",
    "\n",
    "The random forest classifier is used on the ancillary features. For these features to work with a spark RFC, the ancillary features must be combined into a single feature vector. This is accomplished by the VectorAssembler built into spark. This returns a new dataframe with two columns: *'features'* and *'label_index'*. This dataframe is used to train the RFC. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vector_assembler(dataframe):\n",
    "    \n",
    "    df_ancillary = dataframe\n",
    "    assembler = VectorAssembler(inputCols=['massive_edit','longest_repeated_char','empty_edit','size_ratio','alpha_count',\\\n",
    "                                           'vulgar_ratio','similarity'], outputCol='features')\n",
    "\n",
    "    df_ancillary = assembler.transform(df_ancillary)\n",
    "    df_ancillary_vector = df_ancillary.select('features','label_index')\n",
    "    \n",
    "    return df_ancillary_vector\n",
    "\n",
    "df_ancillary_vector = vector_assembler(df_ancillary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----------+--------------------+--------------------+----------+\n",
      "|            features|label_index|       rawPrediction|         probability|prediction|\n",
      "+--------------------+-----------+--------------------+--------------------+----------+\n",
      "|[0.0,3.0,0.0,0.98...|        1.0|[9.77091537248531...|[0.19541830744970...|       1.0|\n",
      "|(7,[1,3,6],[2.0,1...|        1.0|[21.2811061675233...|[0.42562212335046...|       1.0|\n",
      "|[0.0,0.0,0.0,1.00...|        1.0|[32.6312673498640...|[0.65262534699728...|       0.0|\n",
      "|[0.0,2.0,0.0,0.98...|        0.0|[12.0799333724671...|[0.24159866744934...|       1.0|\n",
      "|[1.0,2.0,0.0,0.47...|        1.0|[9.70356686717197...|[0.19407133734343...|       1.0|\n",
      "|(7,[1,3,6],[2.0,1...|        1.0|[21.2811061675233...|[0.42562212335046...|       1.0|\n",
      "|[0.0,0.0,1.0,1.0,...|        0.0|[44.5113085202883...|[0.89022617040576...|       0.0|\n",
      "|(7,[3,5,6],[1.001...|        1.0|[25.3860851896026...|[0.50772170379205...|       0.0|\n",
      "|(7,[3,5,6],[1.080...|        2.0|[36.1378010551152...|[0.72275602110230...|       0.0|\n",
      "|[0.0,2.0,0.0,1.07...|        0.0|[28.5142565648294...|[0.57028513129658...|       0.0|\n",
      "|[0.0,3.0,0.0,1.01...|        0.0|[23.2448397205691...|[0.46489679441138...|       0.0|\n",
      "|[0.0,2.0,0.0,1.05...|        0.0|[23.7393636130692...|[0.47478727226138...|       0.0|\n",
      "|[0.0,4.0,0.0,0.85...|        1.0|[7.83771983327955...|[0.15675439666559...|       1.0|\n",
      "|(7,[3,5,6],[1.011...|        0.0|[32.4017106299977...|[0.64803421259995...|       0.0|\n",
      "|[0.0,0.0,0.0,1.0,...|        0.0|[30.2449685760750...|[0.60489937152150...|       0.0|\n",
      "|(7,[3,5,6],[0.999...|        0.0|[27.0219179913685...|[0.54043835982737...|       0.0|\n",
      "| (7,[3,6],[1.0,1.0])|        0.0|[28.2222402143958...|[0.56444480428791...|       0.0|\n",
      "|[0.0,2.0,0.0,0.99...|        1.0|[17.4809167146325...|[0.34961833429265...|       1.0|\n",
      "|[0.0,2.0,0.0,1.21...|        1.0|[14.4083801321230...|[0.28816760264246...|       1.0|\n",
      "|(7,[3,5,6],[1.006...|        0.0|[32.0714455271075...|[0.64142891054215...|       0.0|\n",
      "+--------------------+-----------+--------------------+--------------------+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Split the data into training and test sets (30% held out for testing)\n",
    "(trainingData, testData) = df_ancillary_vector.randomSplit([0.7, 0.3], seed = 8)\n",
    "\n",
    "# Train a RandomForest model.\n",
    "rf = RandomForestClassifier(labelCol=\"label_index\", featuresCol=\"features\", numTrees=50)\n",
    "\n",
    "# Train model. \n",
    "model_rf = rf.fit(trainingData)\n",
    "\n",
    "# Make predictions.\n",
    "predictions_rf = model_rf.transform(testData)\n",
    "predictions_rf.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part V - A: Evaluation of the Naive Bayes Model\n",
    "\n",
    "## Accuracy\n",
    "\n",
    "The accuracy is an important metric to determine the number of mistakes made by the model. In our application, accuracy is much less important than some of the other metrics because of two reasons:\n",
    "\n",
    "1. The Dataset, before filtering, was highly unbalanced. Predicting all labels as safe would already result in a high accuracy but does not mean the classifier is performing well. \n",
    "\n",
    "2. We are more interested in the classification of a vandal edit compared to the other labels.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5088142292490119\n"
     ]
    }
   ],
   "source": [
    "evaluator = MulticlassClassificationEvaluator(predictionCol=\"prediction\", labelCol='actual_label')\n",
    "accuracy_nb = evaluator.evaluate(predictions_nb)\n",
    "print(accuracy_nb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Precision and Recall\n",
    "\n",
    "For the precision and recall measures we fall back on Sklearn packages because their implementation is much more straight forward as compared to the mllib packages. We are mostly interested in the unsafe and vandal isntances!\n",
    "\n",
    "**Binary Classification:**\n",
    "\n",
    "- Precision = TruePositives / (TruePositives + FalsePositives)\n",
    "- Recall = TruePositives / (TruePositives + FalseNegatives)\n",
    "\n",
    "**Multilabel Classification:**\n",
    "\n",
    "In an imbalanced classification problem with more than two classes, precision is calculated as the sum of true positives across all classes divided by the sum of true positives and false positives across all classes.\n",
    "\n",
    "- Precision = Sum c in C TruePositives_c / Sum c in C (TruePositives_c + FalsePositives_c)\n",
    "\n",
    "\n",
    "In an imbalanced classification problem with more than two classes, recall is calculated as the sum of true positives across all classes divided by the sum of true positives and false negatives across all classes.\n",
    "\n",
    "- Recall = Sum c in C TruePositives_c / Sum c in C (TruePositives_c + FalseNegatives_c)\n",
    "\n",
    "**Key Insight:**\n",
    "\n",
    "In this classification problem, we are mostly interested in finding all vandal edits. Recall is therefore our most important metric since it shows the percentage of vandals that were identified. A higher recall means that less of the vandals were falsly labeled safe (less vandals *'slip through'*). The results below show that for the vandal edits, a recall of ~0.77 was reached. This is a good result. Note however that to reach this high recall, the thresholds of the naive bayes classifier were edited. This comes at a cost of predicting more safe labels as unsafe and vandal. The model can be tuned to the user's wishes. If all vandals should be identified than this will come at a cost of lower precision and lower recall for the other labels. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.5117742181540809, 0.5173913043478261, 0.5088142292490118, None)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd_predictions_nb = predictions_nb.toPandas()\n",
    "\n",
    "y_true = pd_predictions_nb['actual_label'].to_list()\n",
    "y_pred = pd_predictions_nb['prediction'].to_list()\n",
    "\n",
    "precision_recall_fscore_support(y_true, y_pred, average='weighted')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Confusion Matrices and Manual Calculations\n",
    "\n",
    "In this section we manually compute a confusion matrix for the different types of labels. Below the confusion matrix for vandal labels is shown. The rows show the actual labels of each instance and the columns show the predicted labels. As a last step, the precision and recall are computed as if the classification were binary by comparing: safe vs (unsafe + vandal), unsafe vs (safe + vandal) and vandal vs (safe + unsafe)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Pred_Other</th>\n",
       "      <th>Pred_2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Act_Other</th>\n",
       "      <td>189</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Act_2</th>\n",
       "      <td>25</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           Pred_Other  Pred_2\n",
       "Act_Other         189       9\n",
       "Act_2              25       7"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cm_arr = multilabel_confusion_matrix(y_true, y_pred,labels=[0.0, 1.0, 2.0])\n",
    "cm_arr_safe = cm_arr[0,:,:]\n",
    "cm_arr_unsafe = cm_arr[1,:,:]\n",
    "cm_arr_vandal = cm_arr[2,:,:]\n",
    "\n",
    "cm_safe = pd.DataFrame({'Pred_Other': cm_arr_safe[:,0], 'Pred_0': cm_arr_safe[:,1]})\n",
    "cm_safe = cm_safe.rename(index={0: 'Act_Other', 1: 'Act_0'})\n",
    "\n",
    "cm_unsafe = pd.DataFrame({'Pred_Other': cm_arr_unsafe[:,0], 'Pred_1': cm_arr_unsafe[:,1]})\n",
    "cm_unsafe = cm_unsafe.rename(index={0: 'Act_Other', 1: 'Act_1'})\n",
    "\n",
    "cm_vandal = pd.DataFrame({'Pred_Other': cm_arr_vandal[:,0], 'Pred_2': cm_arr_vandal[:,1]})\n",
    "cm_vandal = cm_vandal.rename(index={0: 'Act_Other', 1: 'Act_2'})\n",
    "\n",
    "cm_vandal.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision for Label 0 (safe):  0.5964912280701754\n",
      "Precision for Label 1 (unsafe):  0.44\n",
      "Precision for Label 2 (vandal):  0.4375\n",
      "\n",
      "Recall for Label 0 (safe):  0.6415094339622641\n",
      "Recall for Label 1 (unsafe):  0.4782608695652174\n",
      "Recall for Label 2 (vandal):  0.21875\n",
      "\n"
     ]
    }
   ],
   "source": [
    "precision_safe = cm_arr_safe[1,1]/(cm_arr_safe[1,1] + cm_arr_safe[0,1])\n",
    "precision_unsafe = cm_arr_unsafe[1,1]/(cm_arr_unsafe[1,1] + cm_arr_unsafe[0,1])\n",
    "precision_vandal = cm_arr_vandal[1,1]/(cm_arr_vandal[1,1] + cm_arr_vandal[0,1])\n",
    "\n",
    "recall_safe = cm_arr_safe[1,1]/(cm_arr_safe[1,1] + cm_arr_safe[1,0])\n",
    "recall_unsafe = cm_arr_unsafe[1,1]/(cm_arr_unsafe[1,1] + cm_arr_unsafe[1,0])\n",
    "recall_vandal = cm_arr_vandal[1,1]/(cm_arr_vandal[1,1] + cm_arr_vandal[1,0])\n",
    "\n",
    "print(\"Precision for Label 0 (safe): \", precision_safe)\n",
    "print(\"Precision for Label 1 (unsafe): \", precision_unsafe)\n",
    "print(\"Precision for Label 2 (vandal): \", precision_vandal)\n",
    "print()\n",
    "print(\"Recall for Label 0 (safe): \", recall_safe)\n",
    "print(\"Recall for Label 1 (unsafe): \", recall_unsafe)\n",
    "print(\"Recall for Label 2 (vandal): \", recall_vandal)\n",
    "print()\n",
    "tp_tot = cm_arr_safe[1,1]+cm_arr_unsafe[1,1]+cm_arr_vandal[1,1]\n",
    "fp_tot = cm_arr_safe[0,1]+cm_arr_unsafe[0,1]+cm_arr_vandal[0,1]\n",
    "fn_tot = cm_arr_safe[1,0]+cm_arr_unsafe[1,0]+cm_arr_vandal[1,0]\n",
    "\n",
    "precision_tot = (tp_tot / (tp_tot + fp_tot))\n",
    "recall_tot = ( tp_tot / (tp_tot + fn_tot ))\n",
    "\n",
    "# print(\"Total Precision: \", precision_tot)\n",
    "# print(\"Total Recall: \", recall_tot)\n",
    "              "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part V - B: Evaluation of the Random Forest Model\n",
    "\n",
    "*For a detailed explanation of the metrics see part V - A*\n",
    "\n",
    "## Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.548993933985727\n"
     ]
    }
   ],
   "source": [
    "evaluator2 = MulticlassClassificationEvaluator(predictionCol=\"prediction\", labelCol='label_index')\n",
    "accuracy_rf = evaluator2.evaluate(predictions_rf)\n",
    "print(accuracy_rf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Precision and Recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.5515940573889762, 0.5869565217391305, 0.548993933985727, None)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd_predictions_rf = predictions_rf.toPandas()\n",
    "\n",
    "y_true = pd_predictions_rf['label_index'].to_list()\n",
    "y_pred = pd_predictions_rf['prediction'].to_list()\n",
    "\n",
    "precision_recall_fscore_support(y_true, y_pred, average='weighted')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Confusion Matrices and Manual Calculations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Pred_Other</th>\n",
       "      <th>Pred_2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Act_Other</th>\n",
       "      <td>197</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Act_2</th>\n",
       "      <td>30</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           Pred_Other  Pred_2\n",
       "Act_Other         197       2\n",
       "Act_2              30       1"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cm_arr = multilabel_confusion_matrix(y_true, y_pred,labels=[0.0, 1.0, 2.0])\n",
    "cm_arr_safe = cm_arr[0,:,:]\n",
    "cm_arr_unsafe = cm_arr[1,:,:]\n",
    "cm_arr_vandal = cm_arr[2,:,:]\n",
    "\n",
    "cm_safe = pd.DataFrame({'Pred_Other': cm_arr_safe[:,0], 'Pred_0': cm_arr_safe[:,1]})\n",
    "cm_safe = cm_safe.rename(index={0: 'Act_Other', 1: 'Act_0'})\n",
    "\n",
    "cm_unsafe = pd.DataFrame({'Pred_Other': cm_arr_unsafe[:,0], 'Pred_1': cm_arr_unsafe[:,1]})\n",
    "cm_unsafe = cm_unsafe.rename(index={0: 'Act_Other', 1: 'Act_1'})\n",
    "\n",
    "cm_vandal = pd.DataFrame({'Pred_Other': cm_arr_vandal[:,0], 'Pred_2': cm_arr_vandal[:,1]})\n",
    "cm_vandal = cm_vandal.rename(index={0: 'Act_Other', 1: 'Act_2'})\n",
    "\n",
    "cm_vandal.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision for Label 0 (safe):  0.6319444444444444\n",
      "Precision for Label 1 (unsafe):  0.5180722891566265\n",
      "Precision for Label 2 (vandal):  0.3333333333333333\n",
      "\n",
      "Recall for Label 0 (safe):  0.7711864406779662\n",
      "Recall for Label 1 (unsafe):  0.5308641975308642\n",
      "Recall for Label 2 (vandal):  0.03225806451612903\n",
      "\n"
     ]
    }
   ],
   "source": [
    "precision_safe = cm_arr_safe[1,1]/(cm_arr_safe[1,1] + cm_arr_safe[0,1])\n",
    "precision_unsafe = cm_arr_unsafe[1,1]/(cm_arr_unsafe[1,1] + cm_arr_unsafe[0,1])\n",
    "precision_vandal = cm_arr_vandal[1,1]/(cm_arr_vandal[1,1] + cm_arr_vandal[0,1])\n",
    "\n",
    "recall_safe = cm_arr_safe[1,1]/(cm_arr_safe[1,1] + cm_arr_safe[1,0])\n",
    "recall_unsafe = cm_arr_unsafe[1,1]/(cm_arr_unsafe[1,1] + cm_arr_unsafe[1,0])\n",
    "recall_vandal = cm_arr_vandal[1,1]/(cm_arr_vandal[1,1] + cm_arr_vandal[1,0])\n",
    "\n",
    "print(\"Precision for Label 0 (safe): \", precision_safe)\n",
    "print(\"Precision for Label 1 (unsafe): \", precision_unsafe)\n",
    "print(\"Precision for Label 2 (vandal): \", precision_vandal)\n",
    "print()\n",
    "print(\"Recall for Label 0 (safe): \", recall_safe)\n",
    "print(\"Recall for Label 1 (unsafe): \", recall_unsafe)\n",
    "print(\"Recall for Label 2 (vandal): \", recall_vandal)\n",
    "print()\n",
    "tp_tot = cm_arr_safe[1,1]+cm_arr_unsafe[1,1]+cm_arr_vandal[1,1]\n",
    "fp_tot = cm_arr_safe[0,1]+cm_arr_unsafe[0,1]+cm_arr_vandal[0,1]\n",
    "fn_tot = cm_arr_safe[1,0]+cm_arr_unsafe[1,0]+cm_arr_vandal[1,0]\n",
    "\n",
    "precision_tot = (tp_tot / (tp_tot + fp_tot))\n",
    "recall_tot = ( tp_tot / (tp_tot + fn_tot ))\n",
    "\n",
    "# print(\"Total Precision: \", precision_tot)\n",
    "# print(\"Total Recall: \", recall_tot)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part VI: Comparison of the Models - A Discussion\n",
    "\n",
    "In this section the results of Part V are discussed for both models and an approach is defined on combining the results of both models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparing Accuracy\n",
    "\n",
    "The accuracy of the Naive Bayes classifiers (NBC) is lower than that of the Random Forest classifier (RFC). The accuracies of both models seem on the low side. This mostly to do with the limited amount of data. In the full dataset, almost 8000 instances were included but this dataset is highly unbalanced. To balance the dataset, only a selection of the safe and unsafe instances are chosen in proportion to the total number of vandal instances (91). Simply by increasing this proportion a higher accuracy can be obtained. This means that accuracy is a rather poor metric to define classifier performance.\n",
    "\n",
    "The reason for the difference in accuracy is mostly because the NBC allows different threshold values for the probabilities to determine the predicted label. In general, by deviating from the standard threshold of 0.5, a lower accuracy will be obtained. The question one asks is then what would be the benefit of changing the threshold values. This is a segway to the next section: Recall and Precision.\n",
    "\n",
    "\n",
    "\n",
    "## Comparing Recall and Precision\n",
    "\n",
    "The Recall and Precision metrics show how well each label is identified. While the recall focusses on the number of labels missed in the prediction (eg. number of vandals which were labeled safe or unsafe), the precision focusses on the number of labels falsly identified (eg. number of safe and unsafe which were labeled vandal). It does not come as a suprise that a higher recall results in a lower precision and vice versa. \n",
    "\n",
    "The NBC shows a much higher recall than the RFC mostly because of the changed threshold values. One can plot the Precision and Recall for different values of threshold to obtain the PR-curve. Because of the multiple labels, a PR-curve can be obtained for each label or a weighted average can be computed. The latter is less revealing than the former since the focus of the problem setting is on identifying vandal instances. \n",
    "\n",
    "\n",
    "## Discussion: How to implement a combined approach.\n",
    "\n",
    "There are two approaches to combining the results of the classifiers:\n",
    "\n",
    "#### A. Probabilities\n",
    "\n",
    "The average probability for each label of each instance can be computed and the label can be redefined as that label with the highest probability. This will in general result in a more accurate model only if the two models are complementary. This means that the NBC is finding different vandals than the vandals found by the RFC.\n",
    "\n",
    "\n",
    "#### B. Labels\n",
    "\n",
    "Another approach is to only focus on the labels. If both classifiers predict the same label there is no conflict. When the classifiers predict different labels, the most 'severe' outcome is chosen. The idea is that missclassifying a safe edit as an unsafe or vandal edit is not as significant as the reverse mistake. \n",
    "\n",
    "**An example:**\n",
    "\n",
    "- NBC predictions: [0,0,1,1,0,2,2]\n",
    "- RFC predictions: [0,1,0,2,0,1,0]\n",
    "- CB  predictions: [0,1,1,2,0,2,2]\n",
    "\n",
    "In this way, more vandals will be identified compared to the NBC and RFC seperate. Using this method it is not possible to lower the recall for the vandal edits but this could lower the other metrics. Also the recall of both safe and unsafe edits may decrease."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def combine_prediction(df1,df2):\n",
    "    \n",
    "    df_nb = df1.toPandas()\n",
    "    df_rf = df2.toPandas()\n",
    "    column_names = [\"actual_label\",\"pred_nb\", \"pred_rf\", \"pred_combined\"]\n",
    "    df_combined = pd.DataFrame(columns = column_names)\n",
    "    \n",
    "    df_combined[\"actual_label\"] = df_nb[\"actual_label\"]\n",
    "    df_combined[\"pred_nb\"] = df_nb[\"prediction\"]\n",
    "    df_combined[\"pred_rf\"] = df_rf[\"prediction\"]\n",
    "    \n",
    "    df_combined = sqlContext.createDataFrame(df_combined)\n",
    "    \n",
    "    def combine(val_nb,val_rf):\n",
    "        if val_nb == val_rf:\n",
    "            return val_nb\n",
    "        elif val_nb > val_rf:\n",
    "            return val_nb\n",
    "        elif val_rf > val_nb:\n",
    "            return val_rf\n",
    "    \n",
    "    combineUdf = udf(lambda row: combine(row[0],row[1]), FloatType())\n",
    "    spark.udf.register(\"combineUdf\", combineUdf)\n",
    "    df_combined = df_combined.withColumn(\"pred_combined\", combineUdf(struct('pred_nb','pred_rf')))\n",
    "    \n",
    "    return df_combined"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part VII: Employing the Models in a Streaming Setup\n",
    "\n",
    "In this last part, the two models are used to predict labels of the incoming stream of data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "globals()['models_loaded'] = False\n",
    "\n",
    "def process(time, rdd):\n",
    "    if rdd.isEmpty():\n",
    "        return\n",
    "    \n",
    "    print(\"===================== %s =====================\" % str(time))\n",
    "    \n",
    "    ## Convert to data frame\n",
    "    df_pred = spark.read.json(rdd)\n",
    "    print(\"Incoming Dataframe: \")\n",
    "    df_pred.show()\n",
    "\n",
    "    \n",
    "    ## Preprocessing the incoming dataframe \n",
    "    df_pred_final , df_pred_label = preprocessing(df_pred,True)\n",
    "    print(\"Preprocessed Dataframe: \")\n",
    "    df_pred_final.show()\n",
    "    \n",
    "    ## Computing ancillary features\n",
    "    df_pred_ancillary = compute_ancillary_features(df_pred_final)\n",
    "    df_ancillary_vector = vector_assembler(df_pred_ancillary)\n",
    "    print(\"Ancillary Features Dataframe: \")\n",
    "    df_pred_ancillary.show()\n",
    "    \n",
    "    ## Load in the model if not yet loaded:\n",
    "    if not globals()['models_loaded']:\n",
    "        # load in your models here\n",
    "        globals()['my_model_nb'] = model_nb\n",
    "        globals()['my_model_rf'] = model_rf\n",
    "        globals()['models_loaded'] = True #Update the control to notify model is loaded\n",
    "        \n",
    "    # Predict using the loaded model: \n",
    "    df_result_nb = globals()['my_model_nb'].transform(df_pred_label)\n",
    "    df_result_rf = globals()['my_model_rf'].transform(df_ancillary_vector)\n",
    "    df_result_combined = combine_prediction(df_result_nb,df_result_rf)\n",
    "    \n",
    "    print(\"Predicted Result for Naive Bayes Classifier: \")\n",
    "    df_result_nb.show()\n",
    "    \n",
    "    print(\"Predicted Result for Random Forest Classifier: \")\n",
    "    df_result_rf.show()\n",
    "    \n",
    "    print(\"Predicted Result for the combined_prediction function: \")\n",
    "    df_result_combined.show()\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "ssc = StreamingContext(sc, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "lines = ssc.socketTextStream(\"seppe.net\", 7778)\n",
    "lines.foreachRDD(process)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "ssc_t = StreamingThread(ssc)\n",
    "ssc_t.start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ssc_t.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----- Stopping... this may take a few seconds -----\n",
      "===================== 2020-05-19 23:35:10 =====================\n",
      "Incoming Dataframe: \n",
      "+--------------------+-----+-------------+--------------------+--------------------+--------------+--------------------+\n",
      "|             comment|label|    name_user|            text_new|            text_old|    title_page|            url_page|\n",
      "+--------------------+-----+-------------+--------------------+--------------------+--------------+--------------------+\n",
      "|born born->born -...| safe|Bellowhead678|{{Distinguish|Lis...|{{Distinguish|Lis...|Lisandro López|//en.wikipedia.or...|\n",
      "+--------------------+-----+-------------+--------------------+--------------------+--------------+--------------------+\n",
      "\n",
      "Step 1a Done\n",
      "Step 1b Done\n",
      "Step 2 Done\n",
      "Step 3 Done\n",
      "Step 4 Done\n",
      "Step 5 Done\n",
      "Preprocessed Dataframe: \n",
      "+--------------------+-----+-------------+--------------------+--------------------+--------------+--------------------+--------------------+----------+--------------------+----------+---------+------------+-----------+-------------+--------------------+--------------------+-----------+\n",
      "|             comment|label|    name_user|            text_new|            text_old|    title_page|            url_page|           words_old|tokens_old|           words_new|tokens_new|diff_text|massive_edit|words_clean|words_stemmed|                  tf|              tf_idf|label_index|\n",
      "+--------------------+-----+-------------+--------------------+--------------------+--------------+--------------------+--------------------+----------+--------------------+----------+---------+------------+-----------+-------------+--------------------+--------------------+-----------+\n",
      "|born born->born -...| safe|Bellowhead678|{{Distinguish|Lis...|{{Distinguish|Lis...|Lisandro López|//en.wikipedia.or...|[distinguish, lis...|      5907|[distinguish, lis...|      5906|   [born]|         0.0|     [born]|       [born]|(262144,[107481],...|(262144,[107481],...|        0.0|\n",
      "+--------------------+-----+-------------+--------------------+--------------------+--------------+--------------------+--------------------+----------+--------------------+----------+---------+------------+-----------+-------------+--------------------+--------------------+-----------+\n",
      "\n",
      "Ancillary Features Done\n",
      "Ancillary Features Dataframe: \n",
      "+--------------------+-----+-------------+--------------------+--------------------+--------------+--------------------+--------------------+----------+--------------------+----------+---------+------------+-----------+-------------+--------------------+--------------------+-----------+---------------------+----------+----------+-----------+------------+----------+\n",
      "|             comment|label|    name_user|            text_new|            text_old|    title_page|            url_page|           words_old|tokens_old|           words_new|tokens_new|diff_text|massive_edit|words_clean|words_stemmed|                  tf|              tf_idf|label_index|longest_repeated_char|empty_edit|size_ratio|alpha_count|vulgar_ratio|similarity|\n",
      "+--------------------+-----+-------------+--------------------+--------------------+--------------+--------------------+--------------------+----------+--------------------+----------+---------+------------+-----------+-------------+--------------------+--------------------+-----------+---------------------+----------+----------+-----------+------------+----------+\n",
      "|born born->born -...| safe|Bellowhead678|{{Distinguish|Lis...|{{Distinguish|Lis...|Lisandro López|//en.wikipedia.or...|[distinguish, lis...|      5907|[distinguish, lis...|      5906|   [born]|         0.0|     [born]|       [born]|(262144,[107481],...|(262144,[107481],...|        0.0|                    0|         0|       1.0|          0|         0.0|       1.0|\n",
      "+--------------------+-----+-------------+--------------------+--------------------+--------------+--------------------+--------------------+----------+--------------------+----------+---------+------------+-----------+-------------+--------------------+--------------------+-----------+---------------------+----------+----------+-----------+------------+----------+\n",
      "\n",
      "Predicted Result for Naive Bayes Classifier: \n",
      "+--------------------+------------+--------------------+--------------------+----------+\n",
      "|              tf_idf|actual_label|       rawPrediction|         probability|prediction|\n",
      "+--------------------+------------+--------------------+--------------------+----------+\n",
      "|(262144,[107481],...|         0.0|[-52.379047574345...|[0.00142191720451...|       1.0|\n",
      "+--------------------+------------+--------------------+--------------------+----------+\n",
      "\n",
      "Predicted Result for Random Forest Classifier: \n",
      "+-------------------+-----------+--------------------+--------------------+----------+\n",
      "|           features|label_index|       rawPrediction|         probability|prediction|\n",
      "+-------------------+-----------+--------------------+--------------------+----------+\n",
      "|(7,[3,6],[1.0,1.0])|        0.0|[28.2222402143958...|[0.56444480428791...|       0.0|\n",
      "+-------------------+-----------+--------------------+--------------------+----------+\n",
      "\n",
      "Predicted Result for the combined_prediction function: \n",
      "+------------+-------+-------+-------------+\n",
      "|actual_label|pred_nb|pred_rf|pred_combined|\n",
      "+------------+-------+-------+-------------+\n",
      "|         0.0|    1.0|    0.0|          1.0|\n",
      "+------------+-------+-------+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "ssc_t.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
