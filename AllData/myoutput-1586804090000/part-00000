{"title_page": "Spearman's rank correlation coefficient", "text_new": "[[File:spearman fig1.svg|300px|thumb|A Spearman correlation of 1 results when the two variables being compared are monotonically related, even if their relationship is not linear. This means that all data points with greater ''x'' values than that of a given data point will have greater ''y'' values as well. In contrast, this does not give a perfect Pearson correlation.]][[File:spearman fig2.svg|300px|thumb|When the data are roughly elliptically distributed and there are no prominent outliers, the Spearman correlation and Pearson correlation give similar values.]]\n[[File:spearman fig3.svg|300px|thumb|The Spearman correlation is less sensitive than the Pearson correlation to strong outliers that are in the tails of both samples. That is because Spearman's \u03c1 limits the outlier to the value of its rank.]]\nIn [[statistics]], '''Spearman's rank correlation coefficient''' or '''Spearman's \u03c1''', named after [[Charles Spearman]] and often denoted by the Greek letter [[rho (letter)|<math>\\rho</math>]] (rho) or as <math>r_s</math>, is a [[Nonparametric statistics|nonparametric]] measure of [[rank correlation]] ([[correlation and dependence|statistical dependence]] between the [[ranking]]s of two [[Variable (mathematics)#Applied statistics|variables]]). It assesses how well the relationship between two variables can be described using a [[monotonic]] function.\n\nThe Spearman correlation between two variables is equal to the [[Pearson product-moment correlation coefficient|Pearson correlation]] between the rank values of those two variables; while Pearson's correlation assesses linear relationships, Spearman's correlation assesses monotonic relationships (whether linear or not). If there are no repeated data values, a perfect Spearman correlation of +1 or \u22121 occurs when each of the variables is a perfect monotone function of the other.\n\nIntuitively, the Spearman correlation between two variables will be high when observations have a similar (or identical for a correlation of 1) [[Ranking (statistics)|rank]] (i.e. relative position label of the observations within the variable: 1st, 2nd, 3rd, etc.) between the two variables, and low when observations have a dissimilar (or fully opposed for a correlation of \u22121) rank between the two variables.\n\nSpearman's coefficient is appropriate for both [[continuous variable|continuous]] and discrete [[ordinal variable]]s.<ref>[[Level of measurement#Typology|Scale types]].</ref><ref>{{cite book |title=Jmp For Basic Univariate And Multivariate Statistics: A Step-by-step Guide |last=Lehman |first=Ann |publisher=SAS Press |year=2005 |isbn=978-1-59047-576-8 |location=Cary, NC |page=123}}</ref> Both Spearman's <math>\\rho</math> and [[Kendall tau rank correlation coefficient|Kendall's <math>\\tau</math>]] can be formulated as special cases of a more [[general correlation coefficient]].\n\n==Definition and calculation==\nThe Spearman correlation coefficient is defined as the [[Pearson product-moment correlation coefficient|Pearson correlation coefficient]] between the [[Ranking|rank variables]].<ref name=\"myers2003\">{{Cite book | last1=Myers | first1=Jerome L. | first2=Arnold D. |last2= Well | title=Research Design and Statistical Analysis | publisher=Lawrence Erlbaum | year=2003 | edition=2nd | isbn=978-0-8058-4037-7 | pages=508}}</ref>\n\nFor a sample of size ''n'', the ''n'' [[raw score]]s <math>X_i, Y_i</math> are converted to ranks <math>\\operatorname{rg} X_i, \\operatorname{rg} Y_i</math>, and <math>r_s</math> is computed as\n\n: <math>\n r_s =\n \\rho_{\\operatorname{rg}_X,\\operatorname{rg}_Y} =\n \\frac{\\operatorname{cov}(\\operatorname{rg}_X, \\operatorname{rg}_Y)}\n      {\\sigma_{\\operatorname{rg}_X} \\sigma_{\\operatorname{rg}_Y}},\n</math>\n\nwhere\n: <math>\\rho</math> denotes the usual [[Pearson product-moment correlation coefficient|Pearson correlation coefficient]], but applied to the rank variables,\n: <math>\\operatorname{cov}(\\operatorname{rg}_X, \\operatorname{rg}_Y)</math> is the [[covariance]] of the rank variables,\n: <math>\\sigma_{\\operatorname{rg}_X}</math> and <math>\\sigma_{\\operatorname{rg}_Y}</math> are the [[standard deviation]]s of the rank variables.\n\nOnly if all ''n'' ranks are ''distinct integers'', it can be computed using the popular formula\n\n: <math>r_s = 1 - \\frac{6 \\sum d_i^2}{n(n^2 - 1)},</math>\n\nwhere\n: <math>d_i = \\operatorname{rg}(X_i) - \\operatorname{rg}(Y_i)</math> is the difference between the two ranks of each observation,\n: ''n'' is the number of observations.\n\nIdentical values are usually<ref>{{cite book | last=Dodge | first=Yadolah | date=2010 | title=The Concise Encyclopedia of Statistics | publisher=Springer-Verlag New York  | page=502 | isbn=978-0-387-31742-7 }}</ref> each assigned [[Ranking#Fractional ranking .28.221 2.5 2.5 4.22 ranking.29|fractional ranks]] equal to the average of their positions in the ascending order of the values, which is equivalent to averaging over all possible permutations.\n\nIf ties are present in the data set, the simplified formula above yields incorrect results: Only if in both variables all ranks are distinct, then <math>\\sigma_{\\operatorname{rg}_X} \\sigma_{\\operatorname{rg}_Y} = \\operatorname{Var}{\\operatorname{rg}_X} = \\operatorname{Var}{\\operatorname{rg}_Y} = (n^2 - 1)/12</math> (calculated according to biased variance).\nThe first equation \u2014 normalizing by the standard deviation \u2014 may be used even when ranks are normalized to [0,&nbsp;1] (\"relative ranks\") because it is insensitive both to translation and linear scaling.\n<!-- For example, if [1,2,3,4,5] vs. [1,3,3,3,5] has r_s=0.894, but the simplified formula yields 0.877. -->\n\nThe simplified method should also not be used in cases where the data set is truncated; that is, when the Spearman's correlation coefficient is desired for the top ''X'' records (whether by pre-change rank or post-change rank, or both), the user should use the Pearson correlation coefficient formula given above.<ref name=\"Jaber\">{{Cite book | last1=Al Jaber | first1=Ahmed Odeh | first2=Haifaa Omar  |last2=Elayyan | title=Toward Quality Assurance and Excellence in Higher Education | publisher=River Publishers | year=2018 | isbn=978-87-93609-54-9 | pages=284}}</ref>\n\nThe standard error of the coefficient (''\u03c3'') was determined by Pearson in 1907{{cn|date=January 2020}} and Gosset in 1920.{{cn|date=January 2020}} It is\n\n: <math>\\sigma_{r_s} = \\frac{0.6325}{\\sqrt{n - 1}}.</math>\n\n==Related quantities==\n{{Main|Correlation and dependence}}\n\nThere are several other numerical measures that quantify the extent of [[statistical dependence]] between pairs of observations. The most common of these is the [[Pearson product-moment correlation coefficient]], which is a similar correlation method to Spearman's rank, that measures the \u201clinear\u201d relationships between the raw numbers rather than between their ranks.\n\nAn alternative name for the Spearman [[rank correlation]] is the \u201cgrade correlation\u201d;<ref name=\"Yule and Kendall\">{{cite book |last=Yule |first=G. U. |last2=Kendall |first2=M. G. |orig-year=1950 |title=An Introduction to the Theory of Statistics |edition=14th |year=1968 |publisher=Charles Griffin & Co. |page=268 }}</ref> in this, the \u201crank\u201d of an observation is replaced by the \u201cgrade\u201d. In continuous distributions, the grade of an observation is, by convention, always one half less than the rank, and hence the grade and rank correlations are the same in this case. More generally, the \u201cgrade\u201d of an observation is proportional to an estimate of the fraction of a population less than a given value, with the half-observation adjustment at observed values. Thus this corresponds to one possible treatment of tied ranks. While unusual, the term \u201cgrade correlation\u201d is still in use.<ref>{{cite journal |last=Piantadosi |first=J. |last2=Howlett |first2=P. |last3=Boland |first3=J. |year=2007 |title=Matching the grade correlation coefficient using a copula with maximum disorder |journal=Journal of Industrial and Management Optimization |volume=3 |issue=2 |pages=305\u2013312 |doi= 10.3934/jimo.2007.3.305|url=http://aimsciences.org/journals/pdfs.jsp?paperID=2265&mode=abstract |doi-access=free }}</ref>\n\n==Interpretation==\n{| style=\"float: right;\"\n|+ '''Positive and negative Spearman rank correlations'''\n|- \n| [[File:spearman fig5.svg|300px|left|thumb|A positive Spearman correlation coefficient corresponds to an increasing monotonic trend between ''X'' and ''Y''.]]\n| [[File:spearman fig4.svg|300px|thumb|A negative Spearman correlation coefficient corresponds to a decreasing monotonic trend between ''X'' and ''Y''.]]\n|}\n\nThe sign of the Spearman correlation indicates the direction of association between ''X'' (the independent variable) and ''Y'' (the dependent variable).  If  ''Y'' tends to increase when ''X'' increases, the Spearman correlation coefficient is positive.  If ''Y'' tends to decrease when ''X'' increases, the Spearman correlation coefficient is negative.  A Spearman correlation of zero indicates that there is no tendency for ''Y'' to either increase or decrease when ''X'' increases.  The Spearman correlation increases in magnitude as ''X'' and ''Y'' become closer to being perfectly monotone functions of each other.  When ''X'' and ''Y'' are perfectly monotonically related, the Spearman correlation coefficient becomes 1.  A perfectly monotone increasing relationship implies that for any two pairs of data values {{math|''X''<sub>''i''</sub>, ''Y''<sub>''i''</sub>}} and {{math|''X''<sub>''j''</sub>, ''Y''<sub>''j''</sub>}}, that {{math|''X''<sub>''i''</sub> \u2212 ''X''<sub>''j''</sub>}} and {{math|''Y''<sub>''i''</sub> \u2212 ''Y''<sub>''j''</sub>}} always have the same sign.  A perfectly monotone decreasing relationship implies that these differences always have opposite signs.\n\nThe Spearman correlation coefficient is often described as being \"nonparametric\".  This can have two meanings.  First, a perfect Spearman correlation results when ''X'' and ''Y'' are related by any [[monotonic function]]. Contrast this with the Pearson correlation, which only gives a perfect value when ''X'' and ''Y'' are related by a ''linear'' function. The other sense in which the Spearman correlation is nonparametric in that its exact sampling distribution can be obtained without requiring knowledge (i.e., knowing the parameters) of the [[joint probability distribution]] of ''X'' and ''Y''.\n\n==Example==\nIn this example, the raw data in the table below is used to calculate the correlation between the [[IQ]] of a person with the number of hours spent in front of [[TV]] per week. {{Citation needed|reason=Is it actual data?|date=February 2018}}\n{| class=\"wikitable sortable\" style=\"text-align:right;\"\n|-\n![[IQ]], <math>X_i</math>\n!Hours of [[TV]] per week, <math>Y_i</math>\n|-\n|106\n|7\n|-\n|100\n|27\n|-\n|86\n|2\n|-\n|101\n|50\n|-\n|99\n|28\n|-\n|103\n|29\n|-12\n|97\n|20\n|-\n|113\n|12\n|-\n|112\n|6\n|-\n|110\n|17\n|}\n\nFirstly, evaluate <math>d^2_i</math>. To do so use the following steps, reflected in the table below.\n# Sort the data by the first column (<math>X_i</math>). Create a new column <math>x_i</math> and assign it the ranked values 1, 2, 3, ..., ''n''.\n# Next, sort the data by the second column (<math>Y_i</math>). Create a fourth column <math>y_i</math> and similarly assign it the ranked values 1, 2, 3, ..., ''n''.\n# Create a fifth column <math>d_i</math> to hold the differences between the two rank columns (<math>x_i</math> and <math>y_i</math>).\n# Create one final column <math>d^2_i</math> to hold the value of column <math>d_i</math> squared.\n\n{| class=\"wikitable sortable\" style=\"text-align:right;\"\n|-\n![[IQ]], <math>X_i</math>\n!Hours of [[TV]] per week, <math>Y_i</math>\n!rank <math>x_i</math>\n!rank <math>y_i</math>\n!<math>d_i</math>\n!<math>d^2_i</math>\n|-\n|86\n|2\n|1\n|1\n|0\n|0\n|-\n|97\n|20\n|2\n|6\n| \u22124\n|16\n|-\n|99\n|28\n|3\n|8\n| \u22125\n|25\n|-\n|100\n|27\n|4\n|7\n| \u22123\n|9\n|-\n|101\n|50\n|5\n|10\n| \u22125\n|25\n|-\n|103\n|29\n|6\n|9\n| \u22123\n|9\n|-\n|106\n|7\n|7\n|3\n|4\n|16\n|-\n|110\n|17\n|8\n|5\n|3\n|9\n|-\n|112\n|6\n|9\n|2\n|7\n|49\n|-\n|113\n|12\n|10\n|4\n|6\n|36\n|}\n\nWith <math>d^2_i</math> found, add them to find <math>\\sum d_i^2 = 194</math>. The value of ''n'' is 10. These values can now be substituted back into the equation\n\n: <math>\\rho = 1 - \\frac{6 \\sum d_i^2}{n(n^2 - 1)}</math>\n\nto give\n\n: <math>\\rho = 1 - \\frac{6 \\times 194}{10(10^2 - 1)},</math>\n\nwhich evaluates to {{math|1=''\u03c1'' = \u221229/165 = \u22120.175757575...}} with a [[p-value|''p''-value]] = 0.627188 (using the [[Student's t-distribution|''t''-distribution]]).\n\n[[File:Spearman's Rank chart.png|thumb|Chart of the data presented. It can be seen that there might be a negative correlation, but that the relationship does not appear definitive.]]\nThat the value is close to zero shows that the correlation between IQ and hours spent watching TV is very low, although the negative value suggests that the longer the time spent watching television the lower the IQ. In the case of ties in the original values, this formula should not be used; instead, the Pearson correlation coefficient should be calculated on the ranks (where ties are given ranks, as described above {{Where|date=February 2018}}).\n\n==Determining significance==\nOne approach to test whether an observed value of ''\u03c1'' is significantly different from zero (''r'' will always maintain {{nowrap|\u22121 \u2264 ''r'' \u2264 1}}) is to calculate the probability that it would be greater than or equal to the observed ''r'', given the [[null hypothesis]], by using a [[Resampling (statistics)#Permutation tests|permutation test]]. An advantage of this approach is that it automatically takes into account the number of tied data values in the sample and the way they are treated in computing the rank correlation.\n\nAnother approach parallels the use of the [[Fisher transformation]] in the case of the Pearson product-moment correlation coefficient. That is, [[confidence intervals]] and [[hypothesis test]]s relating to the population value ''\u03c1'' can be carried out using the Fisher transformation:\n\n: <math>F(r) = \\frac{1}{2} \\ln\\frac{1 + r}{1 - r} = \\operatorname{arctanh} r.</math>\n\nIf ''F''(''r'') is the Fisher transformation of ''r'', the sample Spearman rank correlation coefficient, and ''n'' is the sample size, then\n\n: <math>z = \\sqrt{\\frac{n - 3}{1.06}} F(r)</math>\n\nis a [[standard score|''z''-score]] for ''r'', which approximately follows a standard [[normal distribution]] under the [[null hypothesis]] of [[statistical independence]] ({{math|1=''\u03c1'' = 0}}).<ref>{{cite journal |last=Choi |first=S. C. |year=1977 |title=Tests of Equality of Dependent Correlation Coefficients |journal=[[Biometrika]] |volume=64 |issue=3 |pages=645\u2013647 |doi=10.1093/biomet/64.3.645 }}</ref><ref>{{cite journal |last=Fieller |first=E. C. |last2=Hartley |first2=H. O. |last3=Pearson |first3=E. S. |year=1957 |title=Tests for rank correlation coefficients. I |journal=Biometrika |volume=44 |issue=3\u20134 |pages=470\u2013481 |doi=10.1093/biomet/44.3-4.470 |citeseerx=10.1.1.474.9634 }}</ref>\n\nOne can also test for significance using\n\n: <math>t = r \\sqrt{\\frac{n - 2}{1 - r^2}},</math>\n\nwhich is distributed approximately as [[Student's t-distribution|Student's ''t''-distribution]] with {{math|''n'' \u2212 2}} degrees of freedom under the [[null hypothesis]].<ref>{{cite book |last=Press |last2=Vettering |last3=Teukolsky |last4=Flannery |year=1992 |title=Numerical Recipes in C: The Art of Scientific Computing |url=https://archive.org/details/numericalrecipes00pres_0 |url-access=registration |edition=2nd |page=640 |publisher=Cambridge University Press }}</ref> A justification for this result relies on a permutation argument.<ref>{{cite book |last=Kendall |first=M. G. |last2=Stuart |first2=A. |year=1973 |title=The Advanced Theory of Statistics, Volume 2: Inference and Relationship |publisher=Griffin |isbn=978-0-85264-215-3 |url-access=registration |url=https://archive.org/details/advancedtheoryof0001kend |section=Sections 31.19, 31.21}}</ref>\n\nA generalization of the Spearman coefficient is useful in the situation where there are three or more conditions, a number of subjects are all observed in each of them, and it is predicted that the observations will have a particular order.  For example, a number of subjects might each be given three trials at the same task, and it is predicted that performance will improve from trial to trial.  A test of the significance of the trend between conditions in this situation was developed by E. B. Page<ref>{{cite journal |author=Page, E. B. |title=Ordered hypotheses for multiple treatments: A significance test for linear ranks |journal=Journal of the American Statistical Association |volume=58 |pages=216\u2013230 |year=1963 |doi=10.2307/2282965 |issue=301 |jstor=2282965 }}</ref> and is usually referred to as [[Page's trend test]] for ordered alternatives.\n\n==Correspondence analysis based on Spearman's \u03c1==\nClassic [[correspondence analysis]] is a statistical method that gives a score to every value of two nominal variables. In this way the Pearson [[Pearson product-moment correlation coefficient|correlation coefficient]] between them is maximized.\n\nThere exists an equivalent of this method, called [[grade correspondence analysis]], which maximizes Spearman's \u03c1 or [[Kendall's \u03c4]].<ref>{{cite book |editor1-last=Kowalczyk |editor1-first=T. |editor2-last=Pleszczy\u0144ska |editor2-first=E. |editor3-last=Ruland |editor3-first=F. | year=2004 |title=Grade Models and Methods for Data Analysis with Applications for the Analysis of Data Populations |series=Studies in Fuzziness and Soft Computing |volume=151 |publisher=Springer Verlag |location=Berlin Heidelberg New York |isbn=978-3-540-21120-4}}</ref>\n\n==Software Implementations==\n\n* [[R (programming language)|R]]'s statistics base-package implements the test [http://stat.ethz.ch/R-manual/R-patched/library/stats/html/cor.test.html <code>cor.test(x, y, method = \"spearman\")</code>] in its \"stats\" package (also <code>cor(x, y, method = \"spearman\")</code> will work, but without returning the ''p''-value).\n\n==See also==\n{{Portal|Mathematics}}\n* [[Kendall tau rank correlation coefficient]]\n* [[Chebyshev's sum inequality]], [[rearrangement inequality]] (These two articles may shed light on the mathematical properties of Spearman's \u03c1.)\n*[[Distance correlation]]\n* [[Polychoric correlation]]\n\n==References==\n{{Reflist|30em}}\n\n==Further reading==\n* Corder, G.\u00a0W. & Foreman, D.\u00a0I. (2014). Nonparametric Statistics: A Step-by-Step Approach, Wiley. {{isbn|978-1118840313}}.\n* {{cite book |last=Daniel |first=Wayne W. |chapter=Spearman rank correlation coefficient |title=Applied Nonparametric Statistics |location=Boston |publisher=PWS-Kent |edition=2nd |year=1990 |isbn=978-0-534-91976-4 |pages=358\u2013365 |chapterurl=https://books.google.com/books?id=0hPvAAAAMAAJ&pg=PA358 }}\n* {{Cite journal |author=Spearman C. |title=The proof and measurement of association between two things |journal=American Journal of Psychology |volume=15 |issue=1 |year=1904 |pages=72\u2013101 |doi=10.2307/1412159 |jstor=1412159 }}\n* {{Cite journal |author=Bonett D.\u00a0G., Wright, T.\u00a0A. |title=Sample size requirements for Pearson, Kendall, and Spearman correlations |journal=Psychometrika |volume=65 |year=2000 |pages=23\u201328 |doi=10.1007/bf02294183}}\n* {{Cite book |author=Kendall M.\u00a0G. |title=Rank correlation methods |location=London |publisher=Griffin |year=1970 |edition=4th |isbn=978-0-852-6419-96 |oclc=136868}}\n* {{Cite book |authors=Hollander M., Wolfe D.\u00a0A. |title=Nonparametric statistical methods |location=New York |publisher=Wiley |year=1973 |isbn=978-0-471-40635-8 |oclc=520735 |url-access=registration |url=https://archive.org/details/nonparametricsta00holl }}\n* {{Cite journal |authors=Caruso J.\u00a0C., Cliff N. |title=Empirical size, coverage, and power of confidence intervals for Spearman's Rho |journal=Educational and Psychological Measurement |volume=57 |issue=4 |year=1997 |pages=637\u2013654 |doi=10.1177/0013164497057004009}}\n\n==External links==\n{{Wikiversity}}\n*[http://www.sussex.ac.uk/Users/grahamh/RM1web/Rhotable.htm Table of critical values of \u03c1 for significance with small samples]\n* [https://www.rgs.org/NR/rdonlyres/4844E3AB-B36D-4B14-8A20-3A3C28FAC087/0/OASpearmansRankExcelGuidePDF.pdf Spearman\u2019s Rank Correlation Coefficient \u2013 Excel Guide]: sample data and formulae for Excel, developed by the [[Royal Geographical Society]].\n{{Statistics|descriptive}}\n\n{{DEFAULTSORT:Spearman's Rank Correlation Coefficient}}\n[[Category:Covariance and correlation]]\n[[Category:Information retrieval evaluation]]\n[[Category:Nonparametric statistics]]\n[[Category:Statistical tests]]\n", "text_old": "[[File:spearman fig1.svg|300px|thumb|A Spearman correlation of 1 results when the two variables being compared are monotonically related, even if their relationship is not linear. This means that all data points with greater ''x'' values than that of a given data point will have greater ''y'' values as well. In contrast, this does not give a perfect Pearson correlation.]][[File:spearman fig2.svg|300px|thumb|When the data are roughly elliptically distributed and there are no prominent outliers, the Spearman correlation and Pearson correlation give similar values.]]\n[[File:spearman fig3.svg|300px|thumb|The Spearman correlation is less sensitive than the Pearson correlation to strong outliers that are in the tails of both samples. That is because Spearman's \u03c1 limits the outlier to the value of its rank.]]\nIn [[statistics]], '''Spearman's rank correlation coefficient''' or '''Spearman's \u03c1''', named after [[Charles Spearman]] and often denoted by the Greek letter [[rho (letter)|<math>\\rho</math>]] (rho) or as <math>r_s</math>, is a [[Nonparametric statistics|nonparametric]] measure of [[rank correlation]] ([[correlation and dependence|statistical dependence]] between the [[ranking]]s of two [[Variable (mathematics)#Applied statistics|variables]]). It assesses how well the relationship between two variables can be described using a [[monotonic]] function.\n\nThe Spearman correlation between two variables is equal to the [[Pearson product-moment correlation coefficient|Pearson correlation]] between the rank values of those two variables; while Pearson's correlation assesses linear relationships, Spearman's correlation assesses monotonic relationships (whether linear or not). If there are no repeated data values, a perfect Spearman correlation of +1 or \u22121 occurs when each of the variables is a perfect monotone function of the other.\n\nIntuitively, the Spearman correlation between two variables will be high when observations have a similar (or identical for a correlation of 1) [[Ranking (statistics)|rank]] (i.e. relative position label of the observations within the variable: 1st, 2nd, 3rd, etc.) between the two variables, and low when observations have a dissimilar (or fully opposed for a correlation of \u22121) rank between the two variables.\n\nSpearman's coefficient is appropriate for both [[continuous variable|continuous]] and discrete [[ordinal variable]]s.<ref>[[Level of measurement#Typology|Scale types]].</ref><ref>{{cite book |title=Jmp For Basic Univariate And Multivariate Statistics: A Step-by-step Guide |last=Lehman |first=Ann |publisher=SAS Press |year=2005 |isbn=978-1-59047-576-8 |location=Cary, NC |page=123}}</ref> Both Spearman's <math>\\rho</math> and [[Kendall tau rank correlation coefficient|Kendall's <math>\\tau</math>]] can be formulated as special cases of a more [[general correlation coefficient]].\n\n==Definition and calculation==\nThe Spearman correlation coefficient is defined as the [[Pearson product-moment correlation coefficient|Pearson correlation coefficient]] between the [[Ranking|rank variables]].<ref name=\"myers2003\">{{Cite book | last1=Myers | first1=Jerome L. | first2=Arnold D. |last2= Well | title=Research Design and Statistical Analysis | publisher=Lawrence Erlbaum | year=2003 | edition=2nd | isbn=978-0-8058-4037-7 | pages=508}}</ref>\n\nFor a sample of size ''n'', the ''n'' [[raw score]]s <math>X_i, Y_i</math> are converted to ranks <math>\\operatorname{rg} X_i, \\operatorname{rg} Y_i</math>, and <math>r_s</math> is computed as\n\n: <math>\n r_s =\n \\rho_{\\operatorname{rg}_X,\\operatorname{rg}_Y} =\n \\frac{\\operatorname{cov}(\\operatorname{rg}_X, \\operatorname{rg}_Y)}\n      {\\sigma_{\\operatorname{rg}_X} \\sigma_{\\operatorname{rg}_Y}},\n</math>\n\nwhere\n: <math>\\rho</math> denotes the usual [[Pearson product-moment correlation coefficient|Pearson correlation coefficient]], but applied to the rank variables,\n: <math>\\operatorname{cov}(\\operatorname{rg}_X, \\operatorname{rg}_Y)</math> is the [[covariance]] of the rank variables,\n: <math>\\sigma_{\\operatorname{rg}_X}</math> and <math>\\sigma_{\\operatorname{rg}_Y}</math> are the [[standard deviation]]s of the rank variables.\n\nOnly if all ''n'' ranks are ''distinct integers'', it can be computed using the popular formula\n\n: <math>r_s = 1 - \\frac{6 \\sum d_i^2}{n(n^2 - 1)},</math>\n\nwhere\n: <math>d_i = \\operatorname{rg}(X_i) - \\operatorname{rg}(Y_i)</math> is the difference between the two ranks of each observation,\n: ''n'' is the number of observations.\n\nIdentical values are usually<ref>{{cite book | last=Dodge | first=Yadolah | date=2010 | title=The Concise Encyclopedia of Statistics | publisher=Springer-Verlag New York  | page=502 | isbn=978-0-387-31742-7 }}</ref> each assigned [[Ranking#Fractional ranking .28.221 2.5 2.5 4.22 ranking.29|fractional ranks]] equal to the average of their positions in the ascending order of the values, which is equivalent to averaging over all possible permutations.\n\nIf ties are present in the data set, the simplified formula above yields incorrect results: Only if in both variables all ranks are distinct, then <math>\\sigma_{\\operatorname{rg}_X} \\sigma_{\\operatorname{rg}_Y} = \\operatorname{Var}{\\operatorname{rg}_X} = \\operatorname{Var}{\\operatorname{rg}_Y} = (n^2 - 1)/12</math> (calculated according to biased variance).\nThe first equation \u2014 normalizing by the standard deviation \u2014 may be used even when ranks are normalized to [0,&nbsp;1] (\"relative ranks\") because it is insensitive both to translation and linear scaling.\n<!-- For example, if [1,2,3,4,5] vs. [1,3,3,3,5] has r_s=0.894, but the simplified formula yields 0.877. -->\n\nThe simplified method should also not be used in cases where the data set is truncated; that is, when the Spearman's correlation coefficient is desired for the top ''X'' records (whether by pre-change rank or post-change rank, or both), the user should use the Pearson correlation coefficient formula given above.<ref name=\"Jaber\">{{Cite book | last1=Al Jaber | first1=Ahmed Odeh | first2=Haifaa Omar  |last2=Elayyan | title=Toward Quality Assurance and Excellence in Higher Education | publisher=River Publishers | year=2018 | isbn=978-87-93609-54-9 | pages=284}}</ref>\n\nThe standard error of the coefficient (''\u03c3'') was determined by Pearson in 1907{{cn|date=January 2020}} and Gosset in 1920.{{cn|date=January 2020}} It is\n\n: <math>\\sigma_{r_s} = \\frac{0.6325}{\\sqrt{n - 1}}.</math>\n\n==Related quantities==\n{{Main|Correlation and dependence}}\n\nThere are several other numerical measures that quantify the extent of [[statistical dependence]] between pairs of observations. The most common of these is the [[Pearson product-moment correlation coefficient]], which is a similar correlation method to Spearman's rank, that measures the \u201clinear\u201d relationships between the raw numbers rather than between their ranks.\n\nAn alternative name for the Spearman [[rank correlation]] is the \u201cgrade correlation\u201d;<ref name=\"Yule and Kendall\">{{cite book |last=Yule |first=G. U. |last2=Kendall |first2=M. G. |orig-year=1950 |title=An Introduction to the Theory of Statistics |edition=14th |year=1968 |publisher=Charles Griffin & Co. |page=268 }}</ref> in this, the \u201crank\u201d of an observation is replaced by the \u201cgrade\u201d. In continuous distributions, the grade of an observation is, by convention, always one half less than the rank, and hence the grade and rank correlations are the same in this case. More generally, the \u201cgrade\u201d of an observation is proportional to an estimate of the fraction of a population less than a given value, with the half-observation adjustment at observed values. Thus this corresponds to one possible treatment of tied ranks. While unusual, the term \u201cgrade correlation\u201d is still in use.<ref>{{cite journal |last=Piantadosi |first=J. |last2=Howlett |first2=P. |last3=Boland |first3=J. |year=2007 |title=Matching the grade correlation coefficient using a copula with maximum disorder |journal=Journal of Industrial and Management Optimization |volume=3 |issue=2 |pages=305\u2013312 |doi= 10.3934/jimo.2007.3.305|url=http://aimsciences.org/journals/pdfs.jsp?paperID=2265&mode=abstract |doi-access=free }}</ref>\n\n==Interpretation==\n{| style=\"float: right;\"\n|+ '''Positive and negative Spearman rank correlations'''\n|- \n| [[File:spearman fig5.svg|300px|left|thumb|A positive Spearman correlation coefficient corresponds to an increasing monotonic trend between ''X'' and ''Y''.]]\n| [[File:spearman fig4.svg|300px|thumb|A negative Spearman correlation coefficient corresponds to a decreasing monotonic trend between ''X'' and ''Y''.]]\n|}\n\nThe sign of the Spearman correlation indicates the direction of association between ''X'' (the independent variable) and ''Y'' (the dependent variable).  If  ''Y'' tends to increase when ''X'' increases, the Spearman correlation coefficient is positive.  If ''Y'' tends to decrease when ''X'' increases, the Spearman correlation coefficient is negative.  A Spearman correlation of zero indicates that there is no tendency for ''Y'' to either increase or decrease when ''X'' increases.  The Spearman correlation increases in magnitude as ''X'' and ''Y'' become closer to being perfectly monotone functions of each other.  When ''X'' and ''Y'' are perfectly monotonically related, the Spearman correlation coefficient becomes 1.  A perfectly monotone increasing relationship implies that for any two pairs of data values {{math|''X''<sub>''i''</sub>, ''Y''<sub>''i''</sub>}} and {{math|''X''<sub>''j''</sub>, ''Y''<sub>''j''</sub>}}, that {{math|''X''<sub>''i''</sub> \u2212 ''X''<sub>''j''</sub>}} and {{math|''Y''<sub>''i''</sub> \u2212 ''Y''<sub>''j''</sub>}} always have the same sign.  A perfectly monotone decreasing relationship implies that these differences always have opposite signs.\n\nThe Spearman correlation coefficient is often described as being \"nonparametric\".  This can have two meanings.  First, a perfect Spearman correlation results when ''X'' and ''Y'' are related by any [[monotonic function]]. Contrast this with the Pearson correlation, which only gives a perfect value when ''X'' and ''Y'' are related by a ''linear'' function. The other sense in which the Spearman correlation is nonparametric in that its exact sampling distribution can be obtained without requiring knowledge (i.e., knowing the parameters) of the [[joint probability distribution]] of ''X'' and ''Y''.\n\n==Example==\nIn this example, the raw data in the table below is used to calculate the correlation between the [[IQ]] of a person with the number of hours spent in front of [[TV]] per week. {{Citation needed|reason=Is it actual data?|date=February 2018}}\n{| class=\"wikitable sortable\" style=\"text-align:right;\"\n|-\n![[IQ]], <math>X_i</math>\n!Hours of [[TV]] per week, <math>Y_i</math>\n|-\n|106\n|7\n|-\n|100\n|27\n|-\n|86\n|2\n|-\n|101\n|50\n|-\n|99\n|28\n|-\n|103\n|29\n|-12\n|97\n|20\n|-\n|113\n|12\n|-\n|112\n|6\n|-\n|110\n|17\n|}\n\nFirstly, evaluate <math>d^2_i</math>. To do so use the following steps, reflected in the table below.\n# Sort the data by the first column (<math>X_i</math>). Create a new column <math>x_i</math> and assign it the ranked values 1, 2, 3, ..., ''n''.\n# Next, sort the data by the second column (<math>Y_i</math>). Create a fourth column <math>y_i</math> and similarly assign it the ranked values 1, 2, 3, ..., ''n''.\n# Create a fifth column <math>d_i</math> to hold the differences between the two rank columns (<math>x_i</math> and <math>y_i</math>).\n# Create one final column <math>d^2_i</math> to hold the value of column <math>d_i</math> squared.\n\n{| class=\"wikitable sortable\" style=\"text-align:right;\"\n|-\n![[IQ]], <math>X_i</math>\n!Hours of [[TV]] per week, <math>Y_i</math>\n!rank <math>x_i</math>\n!rank <math>y_i</math>\n!<math>d_i</math>\n!<math>d^2_i</math>\n|-\n|86\n|0\n|1\n|1\n|0\n|0\n|-\n|97\n|20\n|2\n|6\n| \u22124\n|16\n|-\n|99\n|28\n|3\n|8\n| \u22125\n|25\n|-\n|100\n|27\n|4\n|7\n| \u22123\n|9\n|-\n|101\n|50\n|5\n|10\n| \u22125\n|25\n|-\n|103\n|29\n|6\n|9\n| \u22123\n|9\n|-\n|106\n|7\n|7\n|3\n|4\n|16\n|-\n|110\n|17\n|8\n|5\n|3\n|9\n|-\n|112\n|6\n|9\n|2\n|7\n|49\n|-\n|113\n|12\n|10\n|4\n|6\n|36\n|}\n\nWith <math>d^2_i</math> found, add them to find <math>\\sum d_i^2 = 194</math>. The value of ''n'' is 10. These values can now be substituted back into the equation\n\n: <math>\\rho = 1 - \\frac{6 \\sum d_i^2}{n(n^2 - 1)}</math>\n\nto give\n\n: <math>\\rho = 1 - \\frac{6 \\times 194}{10(10^2 - 1)},</math>\n\nwhich evaluates to {{math|1=''\u03c1'' = \u221229/165 = \u22120.175757575...}} with a [[p-value|''p''-value]] = 0.627188 (using the [[Student's t-distribution|''t''-distribution]]).\n\n[[File:Spearman's Rank chart.png|thumb|Chart of the data presented. It can be seen that there might be a negative correlation, but that the relationship does not appear definitive.]]\nThat the value is close to zero shows that the correlation between IQ and hours spent watching TV is very low, although the negative value suggests that the longer the time spent watching television the lower the IQ. In the case of ties in the original values, this formula should not be used; instead, the Pearson correlation coefficient should be calculated on the ranks (where ties are given ranks, as described above {{Where|date=February 2018}}).\n\n==Determining significance==\nOne approach to test whether an observed value of ''\u03c1'' is significantly different from zero (''r'' will always maintain {{nowrap|\u22121 \u2264 ''r'' \u2264 1}}) is to calculate the probability that it would be greater than or equal to the observed ''r'', given the [[null hypothesis]], by using a [[Resampling (statistics)#Permutation tests|permutation test]]. An advantage of this approach is that it automatically takes into account the number of tied data values in the sample and the way they are treated in computing the rank correlation.\n\nAnother approach parallels the use of the [[Fisher transformation]] in the case of the Pearson product-moment correlation coefficient. That is, [[confidence intervals]] and [[hypothesis test]]s relating to the population value ''\u03c1'' can be carried out using the Fisher transformation:\n\n: <math>F(r) = \\frac{1}{2} \\ln\\frac{1 + r}{1 - r} = \\operatorname{arctanh} r.</math>\n\nIf ''F''(''r'') is the Fisher transformation of ''r'', the sample Spearman rank correlation coefficient, and ''n'' is the sample size, then\n\n: <math>z = \\sqrt{\\frac{n - 3}{1.06}} F(r)</math>\n\nis a [[standard score|''z''-score]] for ''r'', which approximately follows a standard [[normal distribution]] under the [[null hypothesis]] of [[statistical independence]] ({{math|1=''\u03c1'' = 0}}).<ref>{{cite journal |last=Choi |first=S. C. |year=1977 |title=Tests of Equality of Dependent Correlation Coefficients |journal=[[Biometrika]] |volume=64 |issue=3 |pages=645\u2013647 |doi=10.1093/biomet/64.3.645 }}</ref><ref>{{cite journal |last=Fieller |first=E. C. |last2=Hartley |first2=H. O. |last3=Pearson |first3=E. S. |year=1957 |title=Tests for rank correlation coefficients. I |journal=Biometrika |volume=44 |issue=3\u20134 |pages=470\u2013481 |doi=10.1093/biomet/44.3-4.470 |citeseerx=10.1.1.474.9634 }}</ref>\n\nOne can also test for significance using\n\n: <math>t = r \\sqrt{\\frac{n - 2}{1 - r^2}},</math>\n\nwhich is distributed approximately as [[Student's t-distribution|Student's ''t''-distribution]] with {{math|''n'' \u2212 2}} degrees of freedom under the [[null hypothesis]].<ref>{{cite book |last=Press |last2=Vettering |last3=Teukolsky |last4=Flannery |year=1992 |title=Numerical Recipes in C: The Art of Scientific Computing |url=https://archive.org/details/numericalrecipes00pres_0 |url-access=registration |edition=2nd |page=640 |publisher=Cambridge University Press }}</ref> A justification for this result relies on a permutation argument.<ref>{{cite book |last=Kendall |first=M. G. |last2=Stuart |first2=A. |year=1973 |title=The Advanced Theory of Statistics, Volume 2: Inference and Relationship |publisher=Griffin |isbn=978-0-85264-215-3 |url-access=registration |url=https://archive.org/details/advancedtheoryof0001kend |section=Sections 31.19, 31.21}}</ref>\n\nA generalization of the Spearman coefficient is useful in the situation where there are three or more conditions, a number of subjects are all observed in each of them, and it is predicted that the observations will have a particular order.  For example, a number of subjects might each be given three trials at the same task, and it is predicted that performance will improve from trial to trial.  A test of the significance of the trend between conditions in this situation was developed by E. B. Page<ref>{{cite journal |author=Page, E. B. |title=Ordered hypotheses for multiple treatments: A significance test for linear ranks |journal=Journal of the American Statistical Association |volume=58 |pages=216\u2013230 |year=1963 |doi=10.2307/2282965 |issue=301 |jstor=2282965 }}</ref> and is usually referred to as [[Page's trend test]] for ordered alternatives.\n\n==Correspondence analysis based on Spearman's \u03c1==\nClassic [[correspondence analysis]] is a statistical method that gives a score to every value of two nominal variables. In this way the Pearson [[Pearson product-moment correlation coefficient|correlation coefficient]] between them is maximized.\n\nThere exists an equivalent of this method, called [[grade correspondence analysis]], which maximizes Spearman's \u03c1 or [[Kendall's \u03c4]].<ref>{{cite book |editor1-last=Kowalczyk |editor1-first=T. |editor2-last=Pleszczy\u0144ska |editor2-first=E. |editor3-last=Ruland |editor3-first=F. | year=2004 |title=Grade Models and Methods for Data Analysis with Applications for the Analysis of Data Populations |series=Studies in Fuzziness and Soft Computing |volume=151 |publisher=Springer Verlag |location=Berlin Heidelberg New York |isbn=978-3-540-21120-4}}</ref>\n\n==Software Implementations==\n\n* [[R (programming language)|R]]'s statistics base-package implements the test [http://stat.ethz.ch/R-manual/R-patched/library/stats/html/cor.test.html <code>cor.test(x, y, method = \"spearman\")</code>] in its \"stats\" package (also <code>cor(x, y, method = \"spearman\")</code> will work, but without returning the ''p''-value).\n\n==See also==\n{{Portal|Mathematics}}\n* [[Kendall tau rank correlation coefficient]]\n* [[Chebyshev's sum inequality]], [[rearrangement inequality]] (These two articles may shed light on the mathematical properties of Spearman's \u03c1.)\n*[[Distance correlation]]\n* [[Polychoric correlation]]\n\n==References==\n{{Reflist|30em}}\n\n==Further reading==\n* Corder, G.\u00a0W. & Foreman, D.\u00a0I. (2014). Nonparametric Statistics: A Step-by-Step Approach, Wiley. {{isbn|978-1118840313}}.\n* {{cite book |last=Daniel |first=Wayne W. |chapter=Spearman rank correlation coefficient |title=Applied Nonparametric Statistics |location=Boston |publisher=PWS-Kent |edition=2nd |year=1990 |isbn=978-0-534-91976-4 |pages=358\u2013365 |chapterurl=https://books.google.com/books?id=0hPvAAAAMAAJ&pg=PA358 }}\n* {{Cite journal |author=Spearman C. |title=The proof and measurement of association between two things |journal=American Journal of Psychology |volume=15 |issue=1 |year=1904 |pages=72\u2013101 |doi=10.2307/1412159 |jstor=1412159 }}\n* {{Cite journal |author=Bonett D.\u00a0G., Wright, T.\u00a0A. |title=Sample size requirements for Pearson, Kendall, and Spearman correlations |journal=Psychometrika |volume=65 |year=2000 |pages=23\u201328 |doi=10.1007/bf02294183}}\n* {{Cite book |author=Kendall M.\u00a0G. |title=Rank correlation methods |location=London |publisher=Griffin |year=1970 |edition=4th |isbn=978-0-852-6419-96 |oclc=136868}}\n* {{Cite book |authors=Hollander M., Wolfe D.\u00a0A. |title=Nonparametric statistical methods |location=New York |publisher=Wiley |year=1973 |isbn=978-0-471-40635-8 |oclc=520735 |url-access=registration |url=https://archive.org/details/nonparametricsta00holl }}\n* {{Cite journal |authors=Caruso J.\u00a0C., Cliff N. |title=Empirical size, coverage, and power of confidence intervals for Spearman's Rho |journal=Educational and Psychological Measurement |volume=57 |issue=4 |year=1997 |pages=637\u2013654 |doi=10.1177/0013164497057004009}}\n\n==External links==\n{{Wikiversity}}\n*[http://www.sussex.ac.uk/Users/grahamh/RM1web/Rhotable.htm Table of critical values of \u03c1 for significance with small samples]\n* [https://www.rgs.org/NR/rdonlyres/4844E3AB-B36D-4B14-8A20-3A3C28FAC087/0/OASpearmansRankExcelGuidePDF.pdf Spearman\u2019s Rank Correlation Coefficient \u2013 Excel Guide]: sample data and formulae for Excel, developed by the [[Royal Geographical Society]].\n{{Statistics|descriptive}}\n\n{{DEFAULTSORT:Spearman's Rank Correlation Coefficient}}\n[[Category:Covariance and correlation]]\n[[Category:Information retrieval evaluation]]\n[[Category:Nonparametric statistics]]\n[[Category:Statistical tests]]\n", "name_user": "K.roditakis", "label": "safe", "comment": "days shoulde be 2 not zero, as shown in previous table", "url_page": "//en.wikipedia.org/wiki/Spearman%27s_rank_correlation_coefficient"}
