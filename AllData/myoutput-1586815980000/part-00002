{"title_page": "Phonetics", "text_new": "{{short description|Branch of linguistics that comprises the study of the sounds of human speech}}\n{{other uses}}\n{{Linguistics|Subfields}}\n'''Phonetics''' is a branch of [[linguistics]] that studies the sounds of human speech, or in the case of [[sign language]]s, the equivalent aspects of sign.{{sfn|O'Grady|2005|p=15}} Phoneticians\u2014linguists who specialize in phonetics\u2014study the physical properties of speech. The field of phonetics is traditionally divided into three subdisciplines based on the research questions involved such as how humans plan and execute movements to produce speech ([[articulatory phonetics]]), how different movements affect the properties of the resulting sound ([[acoustic phonetics]]), or how humans convert sound waves to linguistic information ([[auditory phonetics]]). Traditionally, the minimal linguistic unit of phonetics is the [[phone (phonetics)|phone]]\u2014a speech sound in a language\u2014which differs from the phonological unit of [[phoneme]]; the phoneme is an abstract categorization of phones.\n\nPhonetics broadly deals with two aspects of human speech: production\u2014the ways humans make sounds\u2014and perception\u2014the way speech is understood. The [[linguistic modality|modality]] of a language describes the method by which a language produces and perceives languages. Languages with oral-aural modalities such as English produce speech orally (using the mouth) and perceive speech aurally (using the ears). Many sign languages such as [[Auslan]] have a manual-visual modality and produce speech manually (using the hands) and perceive speech visually (using the eyes), while some languages like American Sign have manual-manual dialect for use in [[tactile signing]] by deafblind speakers where signs are produced with the hands and perceived with the hands as well.\n\nLanguage production consists of several interdependent processes which transform a nonlinguistic message into a spoken or signed linguistic signal. After identifying a message to be linguistically encoded, a speaker must select the individual words\u2014known as [[lexical item]]s\u2014to represent that message in a process called lexical selection. During phonological encoding, the mental representation of the words are assigned their phonological content as a sequence of [[phoneme]]s to be produced. The phonemes are specified for articulatory features which denote particular goals such as closed lips or the tongue in a particular location. These phonemes are then coordinated into a sequence of muscle commands that can be sent to the muscles, and when these commands are executed properly the intended sounds are produced.\n\nThese movements disrupt and modify an airstream which results in a sound wave. The modification is done by the articulators, with different places and manners of articulation producing different acoustic results. For example, the words ''tack'' and ''sack'' both begin with alveolar sounds in English, but differ in how far the tongue is from the alveolar ridge. This difference has large effects on the air stream and thus the sound that is produced. Similarly, the direction and source of the airstream can affect the sound. The most common airstream mechanism is pulmonic\u2014using the lungs\u2014but the glottis and tongue can also be used to produce airstreams.\n\nLanguage perception is the process by which a linguistic signal is decoded and understood by a listener. In order to perceive speech the continuous acoustic signal must be converted into discrete linguistic units such as [[phonemes]], [[morphemes]], and [[words]]. In order to correctly identify and categorize sounds, listeners prioritize certain aspects of the signal that can reliably distinguish between linguistic categories. While certain cues are prioritized over others, many aspects of the signal can contribute to perception. For example, though oral languages prioritize acoustic information, the [[McGurk effect]] shows that visual information is used to distinguish ambiguous information when the acoustic cues are unreliable.\n\nModern phonetics has three main branches:\n\n*[[Articulatory phonetics]] which studies the way sounds are made with the articulators\n*[[Acoustic phonetics]] which studies the acoustic results of different articulations\n*[[Auditory phonetics]] which studies the way listeners perceive and understand linguistic signals\n\nThe first known phonetic studies occurred in the [[Indic subcontinent]] during the 6th century BCE, among which was Hindu scholar [[P\u0101\u1e47ini]]'s articulatory description of [[voicing (phonetics)|voicing]], though this pioneering work was primarily concerned with the relationship between written Vedic texts and spoken vernacular languages. With the advent of modern phonetics in the 19th century CE, the focus of scholarship shifted to the physical properties of speech itself. Before the widespread availability of recording devices, phoneticians relied upon [[#Transcription|phonetic transcription systems]] to collect and share data. Some systems, such as the [[International Phonetic Alphabet]] are still in wide use among phoneticians.\n\n{{TOC limit|4}}\n\n== Production ==\n{{main|Language production}}\n<!--[[File:Places of articulation.svg|thumb|Passive and active places of articulation: (1) ''Exo-labial''; (2) ''Endo-labial''; (3) ''Dental''; (4) ''Alveolar''; (5) ''Post-alveolar''; (6) ''Pre-palatal''; (7) ''Palatal''; (8) ''Velar''; (9) ''Uvular''; (10) ''Pharyngeal''; (11) ''Glottal''; (12) ''Epiglottal''; (13) ''Radical''; (14) ''Postero-dorsal''; (15) ''Antero-dorsal''; (16) ''Laminal''; (17) ''Apical''; (18) ''Sub-apical'' or ''sub-laminal''.|alt=A midsagittal view of the mouth with numbers marking places of articulation.]] Need to find a place to put this -->\nLanguage production consists of several interdependent processes which transform a nonlinguistic message into a spoken or signed linguistic signal. Linguists debate whether the process of language production occurs in a series of stages (serial processing) or whether production processes occur in parallel.  After identifying a message to be linguistically encoded, a speaker must select the individual words\u2014known as [[lexical item]]s\u2014to represent that message in a process called lexical selection. The words are selected based on their meaning, which in linguistics is called [[Semantics|semantic]] information. Lexical selection activates the word's [[Lemma (psycholinguistics)|lemma]], which contains both semantic and grammatical information about the word.{{sfn|Dell|O'Seaghdha|1992}}{{efn|Linguists debate whether these stages can interact or whether they occur serially (compare {{harvtxt|Dell|Reich|1981}} and {{harvtxt|Motley|Camden|Baars|1982}}). For ease of description, the language production process is described as a series of independent stages, though recent evidence shows this is inaccurate.{{sfn|Sedivy|2019|p=439}} For further descriptions of interactive activation models see {{harvtxt|Jaeger|Furth|Hilliard|2012}}.}} \n\nAfter an utterance has been planned,{{efn|or after part of an utterance has been planned; see {{harvtxt|Gleitman|January|Nappa|Trueswell|2007}} for evidence of production before a message has been completely planned}} it then goes through phonological encoding. In this stage of language production, the mental representation of the words are assigned their phonological content as a sequence of [[phoneme]]s to be produced. The phonemes are specified for articulatory features which denote particular goals such as closed lips or the tongue in a particular location. These phonemes are then coordinated into a sequence of muscle commands that can be sent to the muscles, and when these commands are executed properly the intended sounds are produced.{{sfn|Boersma|1998}} Thus the process of production from message to sound can be summarized as the following sequence:{{efn|adapted from {{harvtxt|Sedivy|2019|p=411}} and {{harvtxt|Boersma|1998|p=11}}}}\n\n* Message planning\n* Lemma selection\n* Retrieval and assignment of phonological word forms\n* Articulatory specification\n* Muscle commands\n* Articulation\n* Speech sounds\n\n=== Place of articulation ===\n{{main|Place of articulation}}\n{{Places of articulation mini navbox}}\nSounds which are made by a full or partial construction of the vocal tract are called [[consonants]]. Consonants are pronounced in the vocal tract, usually in the mouth, and the location of this construction affects the resulting sound. Because of the close connection between the position of the tongue and the resulting sound, the place of articulation is an important concept in many subdisciplines of phonetics.\n\nSounds are partly categorized by the location of a construction as well as the part of the body doing the constricting. For example, in English the words ''fought'' and ''thought'' are a [[minimal pair]] differing only in the organ making the construction rather than the location of the construction. The \"f\" in ''fought'' is a labiodental articulation made with the bottom lip against the teeth. The \"th\" in ''thought'' is a linguodental articulation made with the tongue against the teeth. Constrictions made by the lips are called [[Labialized|labial]]s while those made with the tongue are called lingual. \n\nConstrictions made with the tongue can be made in several parts of the vocal tract, broadly classified into coronal, dorsal and radical places of articulation. [[Coronal consonant|Coronal]] articulations are made with the front of the tongue, [[Dorsal consonant|dorsal]] articulations are made with the back of the tongue, and [[Radical consonant|radical]] articulations are made in the [[pharynx]].{{sfn|Ladefoged|2001|p=5}} These divisions are not sufficient for distinguishing and describing all speech sounds.{{sfn|Ladefoged|2001|p=5}} For example, in English the sounds {{ipa|[s]}} and {{ipa|[\u0283]}} are both coronal, but they are produced in different places of the mouth. To account for this, more detailed places of articulation are needed based upon the area of the mouth in which the constriction occurs.{{sfn|Ladefoged|Maddieson|1996|p=9}}\n\n==== Labial ====\nArticulations involving the lips can be made in three different ways: with both lips (bilabial), with one lip and the teeth (labiodental), and with the tongue and the upper lip (linguolabial).{{sfn|Ladefoged|Maddieson|1996|p=16}} Depending on the definition used, some or all of these kinds of articulations may be categorized into the class of [[Labial consonant|labial articulation]]s. [[Bilabial consonant]]s are made with both lips. In producing these sounds the lower lip moves farthest to meet the upper lip, which also moves down slightly,{{sfn|Maddieson|1993}} though in some cases the force from air moving through the aperture (opening between the lips) may cause the lips to separate faster than they can come together.{{sfn|Fujimura|1961}} Unlike most other articulations, both articulators are made from soft tissue, and so bilabial stops are more likely to be produced with incomplete closures than articulations involving hard surfaces like the teeth or palate. Bilabial stops are also unusual in that an articulator in the upper section of the vocal tract actively moves downwards, as the upper lip shows some active downward movement.{{sfn|Ladefoged|Maddieson|1996|pp=16\u201317}} [[Linguolabial consonant]]s are made with the blade of the tongue approaching or contacting the upper lip. Like in bilabial articulations, the upper lip moves slightly towards the more active articulator. Articulations in this group do not have their own symbols in the International Phonetic Alphabet, rather, they are formed by combining an apical symbol with a diacritic implicitly placing them in the coronal category.{{sfn|International Phonetic Association|2015}}{{sfn|Ladefoged|Maddieson|1996|p=18}} They exist in a number of languages indigenous to [[Vanuatu]] such as [[Tangoa language|Tangoa]].\n\n[[Labiodental consonant]]s are made by the lower lip rising to the upper teeth. Labiodental consonants are most often [[fricative]]s while labiodental nasals are also typologically common.{{sfn|Ladefoged|Maddieson|1996|pp=17\u201318}} There is debate as to whether true labiodental [[plosive]]s occur in any natural language,{{sfn|Ladefoged|Maddieson|1996|p=17}} though a number of languages are reported to have labiodental plosives including [[Zulu language|Zulu]],{{sfn|Doke|1926}} [[Tonga language (Zambia and Zimbabwe)|Tonga]],{{sfn|Guthrie|1948|p=61}} and [[Shubi language|Shubi]].{{sfn|Ladefoged|Maddieson|1996|p=17}} \n\n==== Coronal ====\nCoronal consonants are made with the tip or blade of the tongue and, because of the agility of the front of the tongue, represent a variety not only in place but in the posture of the tongue. The coronal places of articulation represent the areas of the mouth where the tongue contacts or makes a constriction, and include dental, alveolar, and post-alveolar locations. Tongue postures using the tip of the tongue can be [[Apical consonant|apical]] if using the top of the tongue tip, [[laminal consonant|laminal]] if made with the blade of the tongue, or [[Retroflex consonant|sub-apical]] if the tongue tip is curled back and the bottom of the tongue is used. Coronals are unique as a group in that every [[manner of articulation]] is attested.{{sfn|International Phonetic Association|2015}}{{sfn|Ladefoged|Maddieson|1996|pp=19\u201331}} [[Australian languages]] are well known for the large number of coronal contrasts exhibited within and across languages in the region.{{sfn|Ladefoged|Maddieson|1996|p=28}} [[Dental consonant]]s are made with the tip or blade of the tongue and the upper teeth. They are divided into two groups based upon the part of the tongue used to produce them: apical dental consonants are produced with the tongue tip touching the teeth; interdental consonants are produced with the blade of the tongue as the tip of the tongue sticks out in front of the teeth. No language is known to use both contrastively though they may exist [[allophone|allophonically]]. [[Alveolar consonant]]s are made with the tip or blade of the tongue at the alveolar ridge just behind the teeth and can similarly be apical or laminal.{{sfn|Ladefoged|Maddieson|1996|pp=19\u201325}}\n\nCrosslinguistically, dental consonants and alveolar consonants are frequently contrasted leading to a number of generalizations of crosslinguistic patterns. The different places of articulation tend to also be contrasted in the part of the tongue used to produce them: most languages with dental stops have laminal dentals, while languages with apical stops usually have apical stops. Languages rarely have two consonants in the same place with a contrast in laminality, though [[Taa]] (\u01c3X\u00f3\u00f5) is a counterexample to this pattern.{{sfn|Ladefoged|Maddieson|1996|pp=20, 40\u20131}} If a language has only one of a dental stop or an alveolar stop, it will usually be laminal if it is a dental stop, and the stop will usually be apical if it is an alveolar stop, though for example [[Temne language|Temne]] and [[Bulgarian language|Bulgarian]]{{sfn|Scatton|1984|p=60}} do not follow this pattern.{{sfn|Ladefoged|Maddieson|1996|p=23}} If a language has both an apical and laminal stop, then the laminal stop is more likely to be affricated like in [[Isoko language|Isoko]], though [[Dahalo language|Dahalo]] show the opposite pattern with alveolar stops being more affricated.{{sfn|Ladefoged|Maddieson|1996|pp=23\u20135}}\n\n[[Retroflex consonant]]s have several different definitions depending on whether the position of the tongue or the position on the roof of the mouth is given prominence. In general, they represent a group of articulations in which the tip of the tongue is curled upwards to some degree. In this way, retroflex articulations can occur in several different locations on the roof of the mouth including alveolar, post-alveolar, and palatal regions. If the underside of the tongue tip makes contact with the roof of the mouth, it is sub-apical though apical post-alveolar sounds are also described as retroflex.{{sfn|Ladefoged|Maddieson|1996|pp=25, 27\u20138}} Typical examples of sub-apical retroflex stops are commonly found in [[Dravidian languages]], and in some [[Indigenous languages of the Americas|languages indigenous to the southwest United States]] the contrastive difference between dental and alveolar stops is a slight retroflexion of the alveolar stop.{{sfn|Ladefoged|Maddieson|1996|p=27}} Acoustically, retroflexion tends to affect the higher formants.{{sfn|Ladefoged|Maddieson|1996|p=27}}\n\nArticulations taking place just behind the alveolar ridge, known as [[post-alveolar consonant]]s, have been referred to using a number of different terms. Apical post-alveolar consonants are often called retroflex, while laminal articulations are sometimes called palato-alveolar;{{sfn|Ladefoged|Maddieson|1996|pp=27\u20138}} in the Australianist literature, these laminal stops are often described as 'palatal' though they are produced further forward than the palate region typically described as palatal.{{sfn|Ladefoged|Maddieson|1996|p=28}} Because of individual anatomical variation, the precise articulation of palato-alveolar stops (and coronals in general) can vary widely within a speech community.{{sfn|Ladefoged|Maddieson|1996|p=32}}\n\n==== Dorsal ====\nDorsal consonants are those consonants made using the tongue body rather than the tip or blade and are typically produced at the palate, velum or uvula. [[Palatal consonants]] are made using the tongue body against the hard palate on the roof of the mouth. They are frequently contrasted with velar or uvular consonants, though it is rare for a language to contrast all three simultaneously, with [[Jaqaru]] as a possible example of a three-way contrast.{{sfn|Ladefoged|Maddieson|1996|p=35}} [[Velar consonants]] are made using the tongue body against the [[Soft palate|velum]]. They are incredibly common cross-linguistically; almost all languages have a velar stop. Because both velars and vowels are made using the tongue body, they are highly affected by [[coarticulation]] with vowels and can be produced as far forward as the hard palate or as far back as the uvula. These variations are typically divided into front, central, and back velars in parallel with the vowel space.{{sfn|Ladefoged|Maddieson|1996|pp=33\u201334}} They can be hard to distinguish phonetically from palatal consonants, though are produced slightly behind the area of prototypical palatal consonants.{{sfn|Keating|Lahiri|1993|p=89}} [[Uvular consonants]] are made by the tongue body contacting or approaching the uvula. They are rare, occurring in an estimated 19 percent of languages, and large regions of the Americas and Africa have no languages with uvular consonants. In languages with uvular consonants, stops are most frequent followed by [[continuant]]s (including nasals).{{sfn|Maddieson|2013}}\n\n==== Pharyngeal and laryngeal ====\nConsonants made by constrictions of the throat are pharyngeals, and those made by a constriction in the larynx are laryngeal. Laryngeals are made using the vocal folds as the larynx is too far down the throat to reach with the tongue. Pharyngeals however are close enough to the mouth that parts of the tongue can reach them.\n\nRadical consonants either use the root of the tongue or the [[epiglottis]] during production and are produced very far back in the vocal tract.{{Sfn|Ladefoged|Maddieson|5=1996|p=11}} [[Pharyngeal consonant]]s are made by retracting the root of the tongue far enough to almost touch the wall of the [[pharynx]]. Due to production difficulties, only fricatives and approximants can produced this way.{{Sfn|Lodge|2009|p=33}}{{Sfn|Ladefoged|Maddieson|1996|p=37}} [[Epiglottal consonant]]s are made with the epiglottis and the back wall of the pharynx. Epiglottal stops have been recorded in [[Dahalo language|Dahalo]].{{Sfn|Ladefoged|Maddieson|p=37}} Voiced epiglottal consonants are not deemed possible due to the cavity between the [[glottis]] and epiglottis being too small to permit voicing.{{Sfn|Ladefoged|Maddieson|1996|p=38}}\n\nGlottal consonants are those produced using the vocal folds in the larynx. Because the vocal folds are the source of phonation and below the oro-nasal vocal tract, a number of glottal consonants are impossible such as a voiced glottal stop. Three glottal consonants are possible, a voiceless glottal stop and two glottal fricatives, and all are attested in natural languages.{{sfn|International Phonetic Association|2015}} [[Glottal stop]]s, produced by closing the [[Vocal cords|vocal folds]], are notably common in the world's languages.{{Sfn|Ladefoged|Maddieson|1996|p=38}} While many languages use them to demarcate phrase boundaries, some languages like [[Huautla Mazatec|Huatla Mazatec]] have them as contrastive phonemes. Additionally, glottal stops can be realized as [[laryngealization]] of the following vowel in this language.{{Sfn|Ladefoged|Maddieson|1996|p=74}} Glottal stops, especially between vowels, do usually not form a complete closure. True glottal stops normally occur only when they're [[Gemination|geminated]].{{Sfn|Ladefoged|Maddieson|1996|p=75}}\n\n===The larynx===\n{{further|Larynx}}\n[[File:Larynx (top view).jpg|thumb|A top-down view of the larynx.|alt=See caption]]\n<!--This text previously a part of the voicing section below-->\nThe larynx, commonly known as the \"voice box\", is a cartilaginous structure in the [[trachea]] responsible for [[phonation]]. The vocal folds (chords) are held together so that they vibrate, or held apart so that they do not. The positions of the vocal folds are achieved by movement of the [[arytenoid cartilage]]s.{{sfn|Ladefoged|2001|p=123}} The [[Larynx#Intrinsic|intrinsic laryngeal muscles]] are responsible for moving the arytenoid cartilages as well as modulating the tension of the vocal folds.{{sfn|Seikel|Drumright|King|2016|p=222}} If the vocal folds are not close or tense enough, they will either vibrate sporadically or not at all. If they vibrate sporadically it will result in either creaky or breathy voice, depending on the degree; if don't vibrate at all, the result will be [[voicelessness]]. \n\nIn addition to correctly positioning the vocal folds, there must also be air flowing across them or they will not vibrate. The difference in pressure across the glottis required for voicing is estimated at 1 \u2013 2 [[Centimetre of water|cm H<sub>2</sub>0]] (98.0665 \u2013 196.133 pascals<!--this conversion is based on page 47 of https://physics.nist.gov/cuu/pdf/sp811.pdf-->).{{sfn|Ohala|1997|p=1}} The pressure differential can fall below levels required for phonation either because of an increase in pressure above the glottis (superglottal pressure) or a decrease in pressure below the glottis (subglottal pressure). The subglottal pressure is maintained by the [[respiratory muscles]]. Supraglottal pressure, with no constrictions or articulations, is equal to about [[atmospheric pressure]]. However, because articulations\u2014especially consonants\u2014represent constrictions of the airflow, the pressure in the cavity behind those constrictions can increase resulting in a higher supraglottal pressure.{{sfn|Chomsky|Halle|1968|pp=300\u2013301}}\n\n=== Lexical access ===\nAccording to the lexical access model two different stages of cognition are employed; thus, this concept is known as the two-stage theory of lexical access. The first stage, lexical selection provides information about lexical items required to construct the functional level representation. These items are retrieved according to their specific semantic and syntactic properties, but phonological forms are not yet made available at this stage. The second stage, retrieval of wordforms, provides information required for building the positional level representation.{{sfn|Altmann|2002}}\n\n===Articulatory models===\nWhen producing speech, the articulators move through and contact particular locations in space resulting in changes to the acoustic signal. Some models of speech production take this as the basis for modeling articulation in a coordinate system that may be internal to the body (intrinsic) or external (extrinsic). Intrinsic coordinate systems model the movement of articulators as positions and angles of joints in the body. Intrinsic coordinate models of the jaw often use two to three degrees of freedom representing translation and rotation. These face issues with modeling the tongue which, unlike joints of the jaw and arms, is a [[muscular hydrostat]]\u2014like an elephant trunk\u2014which lacks joints.{{sfn|L\u00f6fqvist|2010|p=359}} Because of the different physiological structures, movement paths of the jaw are relatively straight lines during speech and mastication, while movements of the tongue follow curves.{{sfn|Munhall|Ostry|Flanagan|1991|p=299|ps=, ''et seq.''}}\n\nStraight-line movements have been used to argue articulations as planned in extrinsic rather than intrinsic space, though extrinsic coordinate systems also include acoustic coordinate spaces, not just physical coordinate spaces.{{sfn|L\u00f6fqvist|2010|p=359}} Models that assume movements are planned in extrinsic space run into an [[inverse problem]] of explaining the muscle and joint locations which produce the observed path or acoustic signal. The arm, for example, has seven degrees of freedom and 22 muscles, so multiple different joint and muscle configurations can lead to the same final position. For models of planning in extrinsic acoustic space, the same one-to-many mapping problem applies as well, with no unique mapping from physical or acoustic targets to the muscle movements required to achieve them. Concerns about the inverse problem may be exaggerated, however, as speech is a highly learned skill using neurological structures which evolved for the purpose.{{sfn|L\u00f6fqvist|2010|p=360}}\n\n<!-- See Bizzi et al 1992 on equilibrium-point model -->\nThe equilibrium-point model proposes a resolution to the inverse problem by arguing that movement targets be represented as the position of the muscle pairs acting on a joint.{{efn|See {{harvtxt|Feldman|1966}} for the original proposal.}} Importantly, muscles are modeled as springs, and the target is the equilibrium point for the modeled spring-mass system. By using springs, the equilibrium point model can easily account for compensation and response when movements are disrupted. They are considered a coordinate model because they assume that these muscle positions are represented as points in space, equilibrium points, where the spring-like action of the muscles converges.{{sfn|Bizzi|Hogan|Mussa-Ivaldi|Giszter|1992}}{{sfn|L\u00f6fqvist|2010|p=361}}\n\nGestural approaches to speech production propose that articulations are represented as movement patterns rather than particular coordinates to hit. The minimal unit is a gesture that represents a group of \"functionally equivalent articulatory movement patterns that are actively controlled with reference to a given speech-relevant goal (e.g., a bilabial closure).\"{{sfn|Saltzman|Munhall|1989}} These groups represent coordinative structures or \"synergies\" which view movements not as individual muscle movements but as task-dependent groupings of muscles which work together as a single unit.{{sfn|Mattingly|1990}}{{sfn|L\u00f6fqvist|2010|pp=362\u20134}} This reduces the degrees of freedom in articulation planning, a problem especially in intrinsic coordinate models, which allows for any movement that achieves the speech goal, rather than encoding the particular movements in the abstract representation. Coarticulation is well described by gestural models as the articulations at faster speech rates can be explained as composites of the independent gestures at slower speech rates.{{sfn|L\u00f6fqvist|2010|p=364}}\n\n== Acoustics ==\n[[File:Waveform spectrogram and transcription of wikipedia in praat.png|thumb|upright=1.5|A waveform (top), spectrogram (middle), and transcription (bottom) of a woman saying \"Wikipedia\" displayed using the [[Praat]] software for linguistic analysis. {{listen |filename=En-us-Wikipedia.ogg |title=Listen|description=The accompanying audio.|plain=yes}}<!--Aesthetics could be better for the audio-->]]\nSpeech sounds are created by the modification of an airstream which results in a sound wave. The modification is done by the articulators, with different places and manners of articulation producing different acoustic results. Because the posture of the vocal tract, not just the position of the tongue can affect the resulting sound, the [[manner of articulation]] is important for describing the speech sound. The words ''tack'' and ''sack'' both begin with alveolar sounds in English, but differ in how far the tongue is from the alveolar ridge. This difference has large affects on the air stream and thus the sound that is produced. Similarly, the direction and source of the airstream can affect the sound. The most common airstream mechanism is pulmonic\u2014using the lungs\u2014but the glottis and tongue can also be used to produce airstreams.\n\n===Voicing and phonation types===\n<!--{{Further|#Models of phonation|#The larynx}}-->\nA major distinction between speech sounds is whether they are voiced. Sounds are voiced when the vocal folds begin to vibrate in the process of phonation. Many sounds can be produced with or without phonation, though physical constraints may make phonation difficult or impossible for some articulations. When articulations are voiced, the main source of noise is the periodic vibration of the vocal folds. Articulations like voiceless plosives have no acoustic source and are noticeable by their silence, but other voiceless sounds like fricatives create their own acoustic source regardless of phonation.\n\nPhonation is controlled by the muscles of the larynx, and languages make use of more acoustic detail than binary voicing. During phonation, the vocal folds vibrate at a certain rate. This vibration results in a periodic acoustic waveform comprising a [[fundamental frequency]] and its harmonics. The fundamental frequency of the acoustic wave can be controlled by adjusting the muscles of the larynx, and listeners perceive this fundamental frequency as pitch. Languages use pitch manipulation to convey lexical information in tonal languages, and many languages use pitch to mark prosodic or pragmatic information.\n\nFor the vocal folds to vibrate, they must be in the proper position and there must be air flowing through the glottis.{{sfn|Ohala|1997|p=1}} Phonation types are modeled on a continuum of glottal states from completely open (voiceless) to completely closed (glottal stop). The optimal position for vibration, and the phonation type most used in speech, modal voice, exists in the middle of these two extremes. If the glottis is slightly wider, breathy voice occurs, while bringing the vocal folds closer together results in creaky voice.{{sfn|Gordon|Ladefoged|2001}}\n\nThe normal phonation pattern used in typical speech is modal voice, where the vocal folds are held close together with moderate tension. The vocal folds vibrate as a single unit periodically and efficiently with a full glottal closure and no aspiration.{{Sfn|Gobl|N\u00ed Chasaide|2010|p=399}} If they are pulled farther apart, they do not vibrate and so produce voiceless phones. If they are held firmly together they produce a glottal stop.{{sfn|Gordon|Ladefoged|2001}}<!--I think, double check-->\n\nIf the vocal folds are held slightly further apart than in modal voicing, they produce phonation types like breathy voice (or murmur) and whispery voice. The tension across the vocal ligaments ([[vocal cords]]) is less than in modal voicing allowing for air to flow more freely. Both breathy voice and whispery voice exist on a continuum loosely characterized as going from the more periodic waveform of breathy voice to the more noisy waveform of whispery voice. Acoustically, both tend to dampen the first formant with whispery voice showing more extreme deviations. {{sfn|Gobl|N\u00ed Chasaide|2010|p=400-401}}\n\nHolding the vocal folds more tightly together results in a creaky voice. The tension across the vocal folds is less than in modal voice, but they are held tightly together resulting in only the ligaments of the vocal folds vibrating.{{efn|See [[#The larynx]] for further information on the anatomy of phonation.}} The pulses are highly irregular, with low pitch and frequency amplitude.{{sfn|Gobl|N\u00ed Chasaide|2010|p=401}}\n<!--Gobl and N\u00ed Chasaide has coverage of tense and lax, but I think that might be too much here-->\n\nSome languages do not maintain a voicing distinction for some consonants,{{efn|Hawaiian, for example, does not contrast voiced and voiceless plosives.}} but all languages use voicing to some degree. For example, no language is known to have a phonemic voicing contrast for vowels with all known vowels canonically voiced.{{efn|There are languages, like [[Japanese language|Japanese]], where vowels are produced as voiceless in certain contexts.}} Other positions of the glottis, such as breathy and creaky voice, are used in a number of languages, like [[Jalapa Mazatec]], to contrast [[phonemes]] while in other languages, like English, they exist allophonically. \n\nThere are several ways to determine if a segment is voiced or not, the simplest being to feel the larynx during speech and note when vibrations are felt. More precise measurements can be obtained through acoustic analysis of a spectrogram or spectral slice. In a spectrographic analysis, voiced segments show a voicing bar, a region of high acoustic energy, in the low frequencies of voiced segments.{{sfn|Dawson|Phelan|2016}} In examining a spectral splice, the acoustic spectrum at a given point in time a model of the vowel pronounced reverses the filtering of the mouth producing the spectrum of the glottis. A computational model of the unfiltered glottal signal is then fitted to the inverse filtered acoustic signal to determine the characteristics of the glottis.{{sfn|Gobl|N\u00ed Chasaide|2010|pp=388, ''et seq''}} Visual analysis is also available using specialized medical equipment such as ultrasound and endoscopy.{{sfn|Dawson|Phelan|2016}}{{efn|See [[#Articulatory models]] for further information on acoustic modeling.}}\n\n=== Vowels ===\n{{IPA vowels|class=floatright}}\nVowels are broadly categorized by the area of the mouth in which they are produced, but because they are produced without a constriction in the vocal tract their precise description relies on measuring acoustic correlates of tongue position. The location of the tongue during vowel production changes the frequencies at which the cavity resonates, and it is these resonances\u2014known as [[formants]]\u2014which are measured and used to characterize vowels.\n\nVowel height traditionally refers to the highest point of the tongue during articulation.{{Sfn|Ladefoged|Maddieson|1996|p=282}} The height parameter is divided into four primary levels: high (close), close-mid, open-mid and low (open). Vowels whose height are in the middle are referred to as mid. Slightly opened close vowels and slightly closed open vowels are referred to as near-close and near-open respectively. The lowest vowels are not just articulated with a lowered tongue, but also by lowering the jaw.{{Sfn|Lodge|2009|p=39}}\n\nWhile the IPA implies that there are seven levels of vowel height, it is unlikely that a given language can minimally contrast all seven levels. [[Noam Chomsky|Chomsky]] and [[Morris Halle|Halle]] suggest that there are only three levels,{{Sfn|Chomsky|Halle|1968|p=}} although four levels of vowel height seem to be needed to describe [[Danish language|Danish]] and it's possible that some languages might even need five.{{Sfn|Ladefoged|Maddieson|1996|p=289}}\n\nVowel backness is dividing into three levels: front, central and back. Languages usually do not minimally contrast more than two levels of vowel backness. Some languages claimed to have a three-way backness distinction include [[Nimboran language|Nimboran]] and [[Norwegian language|Norwegian]].{{Sfn|Ladefoged|Maddieson|p=290}}\n\nIn most languages, the lips during vowel production can be classified as either rounded or unrounded (spread), although other types of lip positions, such as compression and protrusion, have been described. Lip position is correlated with height and backness: front and low vowels tend to be unrounded whereas back and high vowels are usually rounded.{{Sfn|Ladefoged|Maddieson|p=292-295}} Paired vowels on the IPA chart have the spread vowel on the left and the rounded vowel on the right.{{Sfn|Lodge|2009|p=40}}\n\nTogether with the universal vowel features described above, some languages have additional features such as [[Nasal vowel|nasality]], [[Vowel length|length]] and different types of phonation such as [[Voiceless vowel|voiceless]] or [[Creaky voice|creaky]]. Sometimes more specialized tongue gestures such as [[Rhotic vowel|rhoticity]], [[Advanced and retracted tongue root|advanced tongue root]], [[pharyngealization]], [[Strident vowel|stridency]] and frication are required to describe a certain vowel.{{Sfn|Ladefoged|Maddieson|p=298}}\n\n=== Manner of articulation ===\n{{main|Manner of articulation}}\nKnowing the place of articulation is not enough to fully describe a consonant, the way in which the stricture happens is equally important. Manners of articulation describe how exactly the active articulator modifies, narrows or closes off the vocal tract.{{Sfn|Ladefoged|Johnson|2011|p=14}}\n\n[[Stop consonant|Stops]] (also referred to as plosives) are consonants where the airstream is completely obstructed. Pressure builds up in the mouth during the stricture, which is then released as a small burst of sound when the articulators move apart. The velum is raised so that air cannot flow through the nasal cavity. If the velum is lowered and allows for air to flow through the nose, the result in a nasal stop. However, phoneticians almost always refer to nasal stops as just \"nasals\".{{Sfn|Ladefoged|Johnson|2011|p=14}}[[Affricate consonant|Affricates]] are a sequence of stops followed by a fricative in the same place.{{Sfn|Ladefoged|Johnson|2011|p=67}}\n\n[[Fricative consonant|Fricatives]] are consonants where the airstream is made turbulent by partially, but not completely, obstructing part of the vocal tract.{{Sfn|Ladefoged|Johnson|2011|p=14}} [[Sibilant]]s are a special type of fricative where the turbulent airstream is directed towards the teeth,{{Sfn|Ladefoged|Maddieson|1996|p=145}} creating a high-pitched hissing sound.{{Sfn|Ladefoged|Johnson|2011|p=15}}\n\n[[Nasal consonant|Nasals]] (sometimes referred to as nasal stops) are consonants in which there's a closure in the oral cavity and the velum is lowered, allowing air to flow through the nose.{{Sfn|Ladefoged|Maddieson|1996|p=102}}\n\nIn an [[Approximant consonant|approximant]], the articulators come close together, but not to such an extent that allows a turbulent airstream.{{Sfn|Ladefoged|Johnson|2011|p=15}}\n\n[[Lateral consonant|Laterals]] are consonants in which the airstream is obstructed along the center of the vocal tract, allowing the airstream to flow freely on one or both sides.{{Sfn|Ladefoged|Johnson|2011|p=15}} Laterals have also been defined as consonants in which the tongue is contracted in such a way that the airstream is greater around the sides than over the center of the tongue.{{Sfn|Ladefoged|Maddieson|1996|p=182}} The first definition does not allow for air to flow over the tongue.\n\n[[Trill consonant|Trills]] are consonants in which the tongue or lips are set in motion by the airstream.{{Sfn|Ladefoged|Johnson|2011|p=175}} The stricture is formed in such a way that the airstream causes a repeating pattern of opening and closing of the soft articulator(s).{{Sfn|Ladefoged|Maddieson|1996|p=217}} Apical trills typically consist of two or three periods of vibration.{{Sfn|Ladefoged|Maddieson|1996|p=218}}\n\n[[Flap consonant|Taps]] and [[Flap consonant|flaps]] are single, rapid, usually [[Apical consonant|apical]] gestures where the tongue is thrown against the roof of the mouth, comparable to a very rapid stop.{{Sfn|Ladefoged|Johnson|2011|p=175}} These terms are sometimes used interchangeably, but some phoneticians make a distinction.{{Sfn|Ladefoged|Maddieson|1996|p=230-231}} In a tap, the tongue contacts the roof in a single motion whereas in a flap the tongue moves tangentially to the roof of the mouth, striking it in passing.\n\nDuring a [[glottalic airstream mechanism]], the glottis is closed, trapping a body of air. This allows for the remaining air in the vocal tract to be moved separately. An upward movement of the closed glottis will move this air out, resulting in it an [[Ejective consonant|ejective]] consonant. Alternatively, the glottis can lower, sucking more air into the mouth, which results in an [[Implosive consonant|implosive]] consonant.{{Sfn|Ladefoged|Johnson|2011|p=137}}\n\n[[Click consonant|Clicks]] are stops in which tongue movement causes air to be sucked in the mouth, this is referred to as a [[velaric airstream]].{{Sfn|Ladefoged|Maddieson|1996|p=78}} During the click, the air becomes [[Rarefaction|rarefied]] between two articulatory closures, producing a loud 'click' sound when the anterior closure is released. The release of the anterior closure is referred to as the click influx. The release of the posterior closure, which can be velar or uvular, is the click efflux. Clicks are used in several African language families, such as the [[Khoisan languages|Khoisan]] and [[Bantu languages|Bantu]] languages.{{Sfn|Ladefoged|Maddieson|1996|p=246-247}}\n\n===Pulmonary and subglottal system===\n{{see|Breathing}}\n<!--[[File:Respiratory system.svg|thumb]]-->\nThe lungs drive nearly all speech production, and their importance in phonetics is due to their creation of pressure for pulmonic sounds. The most common kinds of sound across languages are pulmonic egress, where air is exhaled from the lungs.{{sfn|Ladefoged|2001|p=1}} The opposite is possible, though no language is known to  have pulmonic ingressive sounds as phonemes.{{sfn|Eklund|2008|p=237}} Many languages such as [[Swedish language|Swedish]] use them for [[paralinguistic]] articulations such as affirmations in a number of genetically and geographically diverse languages.{{sfn|Eklund|2008}} Both egressive and ingressive sounds rely on holding the vocal folds in a particular posture and using the lungs to draw air across the vocal folds so that they either vibrate (voiced) or do not vibrate (voiceless).{{sfn|Ladefoged|2001|p=1}} Pulmonic articulations are restricted by the volume of air able to be exhaled in a given respiratory cycle, known as the [[vital capacity]].\n\nThe lungs are used to maintain two kinds of pressure simultaneously in order to produce and modify phonation. To produce phonation at all, the lungs must maintain a pressure of 3\u20135&nbsp;cm H<sub>2</sub>0 higher than the pressure above the glottis. However small and fast adjustments are made to the subglottal pressure to modify speech for suprasegmental features like stress. A number of thoracic muscles are used to make these adjustments. Because the lungs and thorax stretch during inhalation, the elastic forces of the lungs alone can produce pressure differentials sufficient for phonation at lung volumes above 50 percent of vital capacity.{{sfn|Seikel|Drumright|King|2016|p=176}} Above 50 percent of vital capacity, the [[respiratory muscles]] are used to \"check\" the elastic forces of the thorax to maintain a stable pressure differential. Below that volume, they are used to increase the subglottal pressure by actively exhaling air.\n\nDuring speech, the respiratory cycle is modified to accommodate both linguistic and biological needs. Exhalation, usually about 60 percent of the respiratory cycle at rest, is increased to about 90 percent of the respiratory cycle. Because metabolic needs are relatively stable, the total volume of air moved in most cases of speech remains about the same as quiet tidal breathing.{{sfn|Seikel|Drumright|King|2016|p=171}} Increases in speech intensity of 18&nbsp;dB (a loud conversation) has relatively little impact on the volume of air moved. Because their respiratory systems are not as developed as adults, children tend to use a larger proportion of their vital capacity compared to adults, with more deep inhales.{{sfn|Seikel|Drumright|King|2016|pp=168\u201377}}\n\n=== Source\u2013filter theory ===\n{{main|Source\u2013filter model}}\n{{Expand section|date=February 2020}}\nThe source\u2013filter model of speech is a theory of speech production which explains the link between vocal tract posture and the acoustic consequences. Under this model, the vocal tract can be modeled as a noise source coupled onto an [[acoustic filter]].{{sfn|Johnson|2008|p=83\u20135}} The noise source in many cases is the larynx during the process of voicing, though other noise sources can be modeled in the same way. The shape of the supraglottal vocal tract acts as the filter, and different configurations of the articulators result in different acoustic patterns. These changes are predictable. The vocal tract can be modeled as a sequence of tubes, closed at one end, with varying diameters, and by using equations for [[acoustic resonance]] the acoustic effect of an articulatory posture can be derived.{{sfn|Johnson|2008|p=104\u20135}} The process of inverse filtering uses this principle to analyze the source spectrum produced by the vocal folds during voicing. By taking the inverse of a predicted filter, the acoustic effect of the supraglottal vocal tract can be undone giving the acoustic spectrum produced by the vocal folds.{{sfn|Johnson|2008|p=157}} This allows quantitative study of the various phonation types.\n\n== Perception ==\n{{main|Speech perception}}\nLanguage perception is the process by which a linguistic signal is decoded and understood by a listener.{{efn|As with speech production, the nature of the linguistic signal varies depending on the [[language modality]]. The signal can be acoustic for oral speech, visual for signed languages, or tactile for manual-tactile sign languages. For simplicity acoustic speech is described here; for sign language perception specifically, see [[Sign language#Sign perception]].}} In order to perceive speech the continuous acoustic signal must be converted into discrete linguistic units such as [[phonemes]], [[morphemes]], and [[words]].{{sfn|Sedivy|2019|p=259\u201360}} In order to correctly identify and categorize sounds, listeners prioritize certain aspects of the signal that can reliably distinguish between linguistic categories.{{sfn|Sedivy|2019|p=269}} While certain cues are prioritized over others, many aspects of the signal can contribute to perception. For example, though oral languages prioritize acoustic information, the [[McGurk effect]] shows that visual information is used to distinguish ambiguous information when the acoustic cues are unreliable.{{sfn|Sedivy|2019|p=273}}\n\nWhile listeners can use a variety of information to segment the speech signal, the relationship between acoustic signal and category perception is not a perfect mapping. Because of [[coarticulation]], noisy environments, and individual differences, there is a high degree of acoustic variability within categories.{{sfn|Sedivy|2019|p=259}} Known as the problem of '''perceptual invariance'''<!--redirects here-->, listeners are able to reliably perceive categories despite the variability in acoustic instantiation.{{sfn|Sedivy|2019|p=260}} In order to do this, listeners rapidly accommodate to new speakers and will shift their boundaries between categories to match the acoustic distinctions their conversational partner is making.{{sfn|Sedivy|2019|p=274\u201385}}\n\n=== Audition ===\n{{hatnote|Main article: [[Hearing]]; for further information see [[Neuronal encoding of sound]]}}\n[[File:Journey of Sound to the Brain.ogg|thumb|How sounds make their way from the source to the brain|300x300px]]\nAudition, the process of hearing sounds, is the first stage of perceiving speech. Articulators cause systematic changes in air pressure which travel as sound waves to the listener's ear. The sound waves then hit the listener's [[ear drum]] causing it to vibrate. The vibration of the ear drum is transmitted by the [[ossicles]]\u2014three small bones of the middle ear\u2014to the [[cochlea]].{{sfn|Johnson|2003|p=46\u20137}} The cochlea is a spiral-shaped, fluid-filled tube divided lengthwise by the [[organ of Corti]] which contains the [[basilar membrane]]. The basilar membrane increases in thickness as it travels through the cochlea causing different frequencies to resonate at different locations. This [[tonotopic]] design allows for the ear to analyze sound in a manner similar to a [[Fourier transform]].{{sfn|Johnson|2003|p=47}}\n\nThe differential vibration of the basilar causes the [[hair cells]] within the organ of Corti to move. This causes [[depolarization]] of the hair cells and ultimately a conversion of the acoustic signal into a neuronal signal.{{sfn|Schacter|Gilbert|Wegner|2011|p=158\u20139}} While the hair cells do not produce [[action potential]]s themselves, they release neurotransmitter at synapses with the fibers of the [[auditory nerve]], which does produce action potentials. In this way, the patterns of oscillations on the basilar membrane are converted to [[spatiotemporal pattern]]s of firings which transmit information about the sound to the [[brainstem]].{{sfn|Yost|2003|p=130}}\n\n=== Prosody ===\n{{Main|Prosody (linguistics)}}\nBesides consonants and vowels, phonetics also describes the properties of speech that are not localized to [[Segment (linguistics)|segments]] but greater units of speech, such as [[Syllable|syllables]] and [[Phrase|phrases]]. Prosody includes [[Auditory phonetics|auditory characteristics]] such as [[Pitch (music)|pitch]], [[Isochrony|speech rate]], [[Duration (music)|duration]], and [[loudness]]. Languages use these properties to different degrees to implement [[Stress (linguistics)|stress]], [[Pitch accent (intonation)|pitch accents]], and [[Intonation (linguistics)|intonation]] \u2014 for example, [[Stress and vowel reduction in English|stress in English]] and [[Stress in Spanish|Spanish]] is correlated with changes in pitch and duration, whereas [[Welsh phonology|stress in Welsh]] is more consistently correlated with pitch than duration and stress in Thai is only correlated with duration.{{sfn|Cutler|2005}}\n\n=== Theories of speech perception ===\nEarly theories of speech perception such as [[motor theory]] attempted to solve the problem of perceptual invariance by arguing that speech perception and production were closely linked. In its strongest form, motor theory argues that speech perception ''requires'' the listener to access the articulatory representation of sounds;{{sfn|Sedivy|2019|p=289}} in order to properly categorize a sound, a listener reverse engineers the articulation which would produce that sound and by identifying these gestures is able to retrieve the intended linguistic category.{{sfn|Galantucci|Fowler|Turvey|2006}} While findings such as the McGurk effect and case studies from patients with neurological injuries have provided support for motor theory, further experiments have not supported the strong form of motor theory, though there is some support for weaker forms of motor theory which claim a non-deterministic relationship between production and perception.{{sfn|Galantucci|Fowler|Turvey|2006}}{{sfn|Sedivy|2019|p=292\u20133}}{{sfn|Skipper|Devlin|Lametti|2017}}\n\nSuccessor theories of speech perception place the focus on acoustic cues to sound categories and can be grouped into two broad categories: abstractionist theories and episodic theories.{{sfn|Goldinger|1996}} In abstractionist theories, speech perception involves the identification of an idealized lexical object based on a signal reduced to its necessary components and normalizing the signal to counteract speaker variability. Episodic theories such as the [[exemplar model]] argue that speech perception involves accessing detailed memories (i.e., [[episodic memories]]) of previously heard tokens. The problem of perceptual invariance is explained by episodic theories as an issue of familiarity: normalization is a byproduct of exposure to more variable distributions rather than a discrete process as abstractionist theories claim.{{sfn|Goldinger|1996}}\n\n==Development of the field==\n{{anchor|History}} <!--'History of phonetics' redirects here-->\nThe first known phonetic studies were carried out as early as the 6th century BCE by [[Sanskrit]] grammarians.{{sfn|Caffrey|2017}} The Hindu scholar [[P\u0101\u1e47ini]] is among the most well known of these early investigators, whose four-part grammar, written around 350 BCE, is influential in modern linguistics and still represents \"the most complete generative grammar of any language yet written\".{{sfn|Kiparsky|1993|p=2918}} His grammar formed the basis of modern linguistics and described several important phonetic principles, including voicing. This early account described resonance as being produced either by tone, when vocal folds are closed, or noise, when vocal folds are open. The phonetic principles in the grammar are considered \"primitives\" in that they are the basis for his theoretical analysis rather than the objects of theoretical analysis themselves, and the principles can be inferred from his system of phonology.{{sfn|Kiparsky|1993|pp=2922\u20133}}\n\nAdvancements in phonetics after P\u0101\u1e47ini and his contemporaries were limited until the modern era, save some limited investigations by Greek and Roman grammarians. In the millennia between Indic grammarians and modern phonetics, the focus shifted from the difference between spoken and written language, which was the driving force behind P\u0101\u1e47ini's account, and began to focus on the physical properties of speech alone. Sustained interest in phonetics began again around 1800 CE with the term \"phonetics\" being first used in the present sense in 1841.{{sfn|Oxford English Dictionary|2018}}{{sfn|Caffrey|2017}} With new developments in medicine and the development of audio and visual recording devices, phonetic insights were able to use and review new and more detailed data. This early period of modern phonetics included the development of an influential phonetic alphabet based on articulatory positions by [[Alexander Melville Bell]]. Known as [[visible speech]], it gained prominence as a tool in the [[Oralism|oral education of deaf children]].{{sfn|Caffrey|2017}}\n\nBefore the widespread availability of audio recording equipment, phoneticians relied heavily on a tradition of practical phonetics to ensure that transcriptions and findings were able to be consistent across phoneticians. This training involved both ear training\u2014the recognition of speech sounds\u2014as well as production training\u2014the ability to produce sounds. Phoneticians were expected to learn to recognize by ear the various sounds on the [[International Phonetic Alphabet]] and the IPA still tests and certifies speakers on their ability to accurately produce the phonetic patterns of English (though they have discontinued this practice for other languages).{{sfn|Roach|n.d.}} As a revision of his visible speech method, Melville Bell developed a description of vowels by height and backness resulting in 9 [[cardinal vowel]]s.{{sfn|Ladefoged|1960|p=388}} As part of their training in practical phonetics, phoneticians were expected to learn to produce these cardinal vowels in order to anchor their perception and transcription of these phones during fieldwork.{{sfn|Roach|n.d.}} This approach was critiqued by [[Peter Ladefoged]] in the 1960s based on experimental evidence where he found that cardinal vowels were auditory rather than articulatory targets, challenging the claim that they represented articulatory anchors by which phoneticians could judge other articulations.{{sfn|Ladefoged|1960}}\n\n== Subdisciplines ==\n===Acoustic phonetics===\n{{main|Acoustic phonetics}}\nAcoustic phonetics deals with the [[Acoustics|acoustic]] properties of speech sounds. The [[Sensation (psychology)|sensation]] of sound is caused by pressure fluctuations which cause the [[eardrum]] to move. The ear transforms this movement into neural signals that the brain registers as sound. Acoustic waveforms are records that measure these pressure fluctuations.{{Sfn|Johnson|2003|p=1}}\n\n===Articulatory phonetics===\n{{main|Articulatory phonetics}}\nArticulatory phonetics deals with the ways in which speech sounds are made.\n\n===Auditory phonetics===\n{{main|Auditory phonetics}}\nAuditory phonetics studies how humans perceive speech sounds. Due to the anatomical features of the auditory system distorting the speech signal, humans do not experience speech sounds as perfect acoustic records. For example, the auditory impressions of [[Loudness|volume]], measured in decibels (dB), does not linearly match the difference in sound pressure.{{Sfn|Johnson|2003|p=46-49}}\n\nThe mismatch between acoustic analyses and what the listener hears is especially noticeable in speech sounds that have a lot of high-frequency energy, such as certain fricatives. To reconcile this mismatch, functional models of the auditory system have been developed.{{Sfn|Johnson|2003|p=53}}\n\n==Describing sounds==\n{{anchor|Consonants}}\n<!-- See Language Files 12th ed. -->\n<!-- See A Course in Phonetics 4th ed. -->\n<!--[[File:Phonological anatomy 1.png|thumb|A diagram of anatomical locations in the vocal tract. (A) Nasal cavity; (B) alveolar ridge; (C) lips; (D) teeth; (E) tongue tip; (F) larynx; (G) glottis; (H) palate; (I) tongue body; (J) velum; (K) uvula; (L) trachea; (M) esophagus.|alt=See caption.]]\n[[File:Diagram showing the parts of the pharynx CRUK 334.svg|thumb]]-->\nHuman languages use many different sounds and in order to compare them linguists must be able to describe sounds in a way that is language independent. Speech sounds can be described in a number of ways. Most commonly speech sounds are referred to by the mouth movements needed to produce them. [[Consonant]]s and [[vowel]]s are two gross categories that phoneticians define by the movements in a speech sound. More fine-grained descriptors are parameters such as place of articulation. [[Place of articulation]], [[manner of articulation]], and [[voicing (phonetics)|voicing]]<!--Maybe these should go to the relevant sections instead of separate pages--> are used to describe consonants and are the main divisions of the [[International Phonetic Alphabet]] consonant chart. Vowels are described by their height, backness, and rounding. Sign language are described using a similar but [[sign language parameters|distinct set of parameters]] to describe signs: location, movement, hand shape, palm orientation, and non-manual features. In addition to articulatory descriptions, sounds used in oral languages can be described using their acoustics. Because the acoustics are a consequence of the articulation, both methods of description are sufficient to distinguish sounds with the choice between systems dependent on the phonetic feature being investigated.\n\nConsonants are speech sounds that are articulated with a complete or partial closure of the [[vocal tract]]. They are generally produced by the modification of an [[Airstream mechanism|airstream]] exhaled from the lungs. The respiratory organs used to create and modify airflow are divided into three regions: the vocal tract (supralaryngeal), the [[larynx]], and the subglottal system. The airstream can be either [[Egressive sound|egressive]] (out of the vocal tract) or [[Ingressive sound|ingressive]] (into the vocal tract). In pulmonic sounds, the airstream is produced by the lungs in the subglottal system and passes through the larynx and vocal tract. [[Glottalic consonant|Glottalic]] sounds use an airstream created by movements of the larynx without airflow from the lungs. [[Click consonant|Click]] consonants are articulated through the [[rarefaction]] of air using the tongue, followed by releasing the forward closure of the tongue.\n\nVowels are [[Syllable|syllabic]] speech sounds that are pronounced without any obstruction in the vocal tract.{{Sfn|Ladefoged|Maddieson|1996|p=281}} Unlike consonants, which usually have definite places of articulation, vowels are defined in relation to a set of reference vowels called [[cardinal vowels]]. Three properties are needed to define vowels: tongue height, tongue backness and lip roundedness. Vowels that are articulated with a stable quality are called [[monophthong]]s; a combination of two separate vowels in the same syllable is a [[diphthong]].{{Sfn|Gussenhoven|Jacobs|p=26-27|2017}} In the [[International Phonetic Alphabet|IPA]], the vowels are represented on a trapezoid shape representing the human mouth: the vertical axis representing the mouth from floor to roof and the horizontal axis represents the front-back dimension.{{Sfn|Lodge|2009|p=38}}\n\n===Transcription===\n{{Main|Phonetic transcription}}\n[[Phonetic transcription]] is a system for transcribing [[Phone (phonetics)|phones]] that occur in a language, whether [[oral language|oral]] or [[sign language|sign]]. The most widely known system of phonetic transcription, the [[International Phonetic Alphabet]] (IPA), provides a standardized set of symbols for oral phones.{{sfn|O'Grady|2005|p=17}}{{sfn|International Phonetic Association|1999}} The standardized nature of the IPA enables its users to transcribe accurately and consistently the phones of different languages, [[dialect]]s, and [[idiolect]]s.{{sfn|O'Grady|2005|p=17}}{{sfn|Ladefoged|2005}}{{sfn|Ladefoged|Maddieson|1996}} The IPA is a useful tool not only for the study of phonetics, but also for language teaching, professional acting, and [[speech pathology]].{{sfn|Ladefoged|2005}}\n\nWhile no sign language has a standardized writing system, linguists have developed their own notation systems that describe the handshape, location and movement. The [[Hamburg Notation System]] (HamNoSys) is similar to the IPA in that it allows for varying levels of detail. Some notation systems such as KOMVA and the [[Stokoe notation|Stokoe system]] were designed for use in dictionaries; they also make use of alphabetic letters in the local language for handshapes whereas HamNoSys represents the handshape directly. [[SignWriting]] aims to be an easy-to-learn writing system for sign languages, although it has not been officially adopted by any deaf community yet.{{Sfn|Baker|van den Bogarde|Pfau|p=242-244|Schermer|2016}}\n\n== Sign languages ==\nUnlike spoken languages, words in [[sign language]]s are perceived with the eyes instead of the ears. Signs are articulated with the hands, upper body and head. Various factors \u2013 such as muscle flexibility or being considered [[taboo]] \u2013 restrict what can be considered a sign.{{Sfn|Baker|van den Bogarde|Pfau|p=229-235|Schermer|2016}}\n\nThe main articulators are the hands and arms. Relative parts of the arm are described with the terms [[proximal]] and [[distal]]. Proximal refers to a part closer to the torso whereas a distal part is further away from it. For example, a wrist movement is distal compared to an elbow movement. Due to requiring less energy, distal movements are generally easier to produce.{{Sfn|Baker|van den Bogarde|Pfau|p=229-235|Schermer|2016}}\n\nUnlike spoken languages, sign languages have two identical articulators: the hands. Signers may use whichever hand they prefer with no disruption in communication. Due to universal neurological limitations, two-handed signs generally have the same kind of articulation in both hands; this is referred to as the Symmetry Condition.{{Sfn|Baker|van den Bogarde|Pfau|p=229-235|Schermer|2016}} The second universal constraint is the Dominance Condition, which holds that when two handshapes are involved, one hand will remain stationary and have a more limited set handshapes compared to the dominant, moving hand.{{Sfn|Baker|van den Bogarde|Pfau|p=286|Schermer|2016}} Additionally, it is common for one hand in a two-handed sign to be dropped during informal conversations, a process referred to as weak drop.{{Sfn|Baker|van den Bogarde|Pfau|p=229-235|Schermer|2016}}\n\nJust like words in spoken languages, coarticulation may cause signs to influence each other's form. Examples include the handshapes of neighboring signs becoming more similar to each other ([[Assimilation (phonology)|assimilation]]) or weak drop (an instance of [[Deletion (phonology)|deletion]]).{{Sfn|Baker|van den Bogarde|Pfau|p=239|Schermer|2016}}\n\nNative signers do not look at their conversation partner's hands. Instead, their eye gaze is fixated on the face. Because [[peripheral vision]] is not as focused as the center of the visual field, signs articulated near the face allow for more subtle differences in finger movement and location to be perceived.{{Sfn|Baker|van den Bogarde|Pfau|p=236|Schermer|2016}}\n\n==See also==\n* [[Motor theory of speech perception]]\n* [[Exemplar theory]]\n* [[Articulatory phonology]]\n\n== References ==\n=== Notes ===\n{{notelist}}\n\n=== Citations ===\n{{reflist}}\n\n=== Works cited ===\n{{refbegin|30em}}\n* {{cite book\n|last=Abercrombie\n|first=D.\n|year=1967\n|isbn=\n|title=Elements of General Phonetics\n|url=https://archive.org/details/elementsofgenera0000aber\n|url-access=registration\n|place=Edinburgh\n|pages=\n|ref=harv\n|publisher=Chicago, Aldine Pub. Co\n}}\n* {{cite book\n|title=Psycholinguistics : critical concepts in psychology\n|date=2002\n|publisher=Routledge\n|last=Altmann\n|first=Gerry\n|isbn=978-0415229906\n|location=London\n|oclc=48014482\n|ref=harv\n}}\n* {{cite book\n|last1=Baker\n|first1=Anne\n|last2=van den Bogaerde\n|first2=Beppie\n|last3=Pfau\n|first3=Roland \n|last4=Schermer\n|first4=Trude\n|title=The Linguistics of Sign Languages\n|date=2016\n|publisher=John Benjamins Publishing Company\n|location=Amsterdam/Philadelphia\n|isbn=978-90-272-1230-6\n|ref=harv\n}}\n* {{cite book\n|last=Baumbach\n|first=E. J. M\n|year=1987\n|isbn=\n|title=Analytical Tsonga Grammar\n|place=Pretoria\n|pages=\n|publisher=University of South Africa\n|ref=harv\n}}\n* {{cite journal\n|last=Bizzi\n|first=E.\n|last2=Hogan\n|first2=N.\n|last3=Mussa-Ivaldi\n|first3=F.\n|last4=Giszter\n|first4=S.\n|year=1992\n|title=Does the nervous system use equilibrium-point control to guide single and multiple joint movements?\n|url=\n|journal=Behavioral and Brain Sciences\n|volume=15\n|issue=4\n|pages=603\u201313\n|ref=harv\n|doi=10.1017/S0140525X00072538\n|pmid=23302290\n}}\n* {{cite book\n|title=Psycholinguistics: Critical Concepts in Psychology\n|last=Bock\n|first=Kathryn\n|last2=Levelt\n|first2=Willem\n|publisher=Routledge\n|year=2002\n|isbn=978-0-415-26701-4\n|editor-last=Atlmann\n|editor-first=Gerry\n|volume=5\n|location=New York\n|pages=405\u2013407\n|ref=harv\n}}\n* {{cite book\n|last=Boersma\n|first=Paul\n|title=Functional phonology: Formalizing the interactions between articulatory and perceptual drives\n|year=1998\n|publisher=Holland Academic Graphics\n|place=The Hague\n|oclc=40563066\n|isbn=9055690546\n|ref=harv\n}}\n* {{cite book\n|last=Caffrey\n|first=Cait\n|year=2017\n|isbn=\n|location=\n|pages=\n|chapter=Phonetics\n|title=Salem Press Encyclopedia\n|publisher=Salem Press\n|ref=harv\n}}\n* {{cite book\n|last=Catford\n|first=J. C.\n|title=A Practical Introduction to Phonetics\n|url=https://archive.org/details/practicalintrodu00catf\n|url-access=registration\n|year=2001\n|publisher=Oxford University Press\n|edition= 2nd\n|location=\n|pages=\n|isbn=978-0-19-924635-9\n|ref=harv\n}}\n* {{cite book\n|last=Chomsky\n|first=Noam\n|last2=Halle\n|first2=Morris\n|year=1968\n|isbn=\n|location=\n|pages=\n|title=Sound Pattern of English\n|publisher=Harper and Row\n|ref=harv\n}}\n* {{cite book\n|last=Cutler\n|first=Anne\n|date=2005\n|doi=10.1002/9780470757024.ch11\n|chapter=Lexical Stress\n|title=The Handbook of Speech Perception\n|pages=264\u2013289\n|editor-last=Pisoni\n|editor-first=David B.\n|editor2-last=Remez\n|editor2-first=Robert\n|chapter-url=https://repository.ubn.ru.nl/bitstream/handle/2066/56545/56545.pdf\n|publisher=Blackwell\n|isbn=978-0-631-22927-8\n|oclc=749782145\n|access-date=2019-12-29\n|ref=harv\n}}\n* {{cite book\n|editor-last=Dawson\n|editor-first=Hope\n|editor2-last=Phelan\n|editor2-first=Michael\n|year=2016\n|title=Language Files: Materials for an Introduction to Linguistics\n|last=\n|first=\n|publisher=The Ohio State University Press\n|edition= 12th\n|location=\n|pages=\n|isbn=978-0-8142-5270-3\n|ref=harv\n}}\n* {{cite journal\n|last=Dell\n|first=Gary\n|last2=O'Seaghdha\n|first2=Padraig\n|date=1992\n|title=Stages of lexical access in language production\n|url=\n|journal=Cognition\n|volume=42\n|issue=1\u20133\n|pages=287\u2013314\n|doi=10.1016/0010-0277(92)90046-k\n|pmid=1582160\n|ref=harv\n}}\n* {{cite journal\n|last=Dell\n|first=Gary\n|first2=Peter\n|last2=Reich\n|title=Stages in sentence production: An analysis of speech error data\n|journal=Journal of Memory and Language\n|volume=20\n|issue=6\n|year=1981\n|pages=611\u2013629\n|doi=10.1016/S0022-5371(81)90202-4\n|ref=harv\n}}\n* {{cite book\n|last=Doke\n|first=Clement M\n|year=1926\n|isbn=\n|title=The Phonetics of the Zulu Language\n|series=Bantu Studies\n|place=Johannesburg\n|pages=\n|publisher=Wiwatersrand University Press\n|ref=harv\n}}\n* {{cite journal\n|last=Eklund\n|first=Robert\n|year=2008\n|title=Pulmonic ingressive phonation: Diachronic and synchronic characteristics, distribution and function in animal and human sound production and in human speech\n|url=\n|journal=Journal of the International Phonetic Association\n|volume=38\n|issue=3\n|pages=235\u2013324\n|doi=10.1017/S0025100308003563\n|ref=harv\n}}\n* {{cite journal\n|last=Feldman\n|first=Anatol G.\n|year=1966\n|title=Functional tuning of the nervous system with control of movement or maintenance of a steady posture, III: Mechanographic analysis of the execution by man of the simplest motor task\n|url=\n|journal=Biophysics\n|volume=11\n|pages=565\u2013578\n|ref=harv\n|via=\n}}\n* {{cite journal\n|last=Fujimura\n|first=Osamu\n|year=1961\n|title=Bilabial stop and nasal consonants: A motion picture study and its acoustical implications\n|url=\n|journal=Journal of Speech and Hearing Research\n|volume=4\n|issue=3\n|pages=233\u201347\n|ref=harv\n|pmid=13702471\n|doi=10.1044/jshr.0403.233\n}}\n* {{cite journal\n|last1=Galantucci\n|first1=Bruno\n|first2=Carol\n|last2=Fowler\n|first3=Michael\n|last3=Turvey\n|title=The motor theory of speech perception reviewed\n|journal=Psychonomic Bulletin & Review\n|volume=13\n|issue=3\n|year=2006\n|pages=361\u2013377\n|ref=harv\n|doi=10.3758/BF03193857\n|pmid=17048719\n|pmc=2746041\n}}\n* {{cite journal\n|last=Gleitman\n|first=Lila\n|first2=David\n|last2=January\n|first3=Rebecca\n|last3=Nappa\n|first4=John\n|last4=Trueswell\n|title=On the give and take between event apprehension and utterance formulation \n|journal=Journal of Memory and Language\n|volume=57\n|issue=4\n|year=2007\n|pages=544\u2013569\n|doi=10.1016/j.jml.2007.01.007\n|pmid=18978929\n|pmc=2151743\n|ref=harv\n}}\n* {{cite book\n|first=Christer\n|last=Gobl\n|first2=Ailbhe\n|last2=N\u00ed Chasaide\n|year=2010\n|isbn=\n|chapter=Voice source variation and its communicative functions\n|title=The Handbook of Phonetic Sciences\n|edition= 2nd\n|location=\n|pages=378\u2013424\n|ref=harv\n}}\n* {{cite journal\n|last=Goldinger\n|first=Stephen\n|title=Words and voices: episodic traces in spoken word identification and recognition memory\n|journal=Journal of Experimental Psychology: Learning, Memory, and Cognition\n|volume=22\n|issue=5\n|year=1996\n|pages=1166\u201383\n|ref=harv\n|doi=10.1037/0278-7393.22.5.1166\n}}\n* {{cite journal\n|last=Gordon\n|first=Matthew\n|first2=Peter\n|last2=Ladefoged\n|year=2001\n|title=Phonation types: a cross-linguistic overview\n|url=\n|journal=Journal of Phonetics\n|volume=29\n|number=4\n|pages=383\u2013406\n|ref=harv\n|doi=10.1006/jpho.2001.0147\n}}\n* {{cite book\n|last1=Guthrie\n|first1=Malcolm\n|title=The classification of the Bantu languages\n|date=1948\n|publisher=Oxford University Press\n|isbn=\n|location=London\n|pages=\n|ref=harv\n}}\n* {{cite book \n|title=Understanding phonology \n|last=Gussenhoven \n|first=Carlos \n|last2=Jacobs \n|first2=Haike \n|date=2017 \n|publisher=Routledge \n|isbn=9781138961418 \n|edition= Fourth \n|location=London and New York \n|oclc=958066102\n|ref=harv\n}}\n* {{cite book\n|last=Hall\n|first=Tracy Alan\n|year=2001\n|isbn=\n|chapter=Introduction: Phonological representations and phonetic implementation of distinctive features\n|title=Distinctive Feature Theory\n|editor-last=Hall\n|editor-first=Tracy Alan\n|location=\n|pages=1\u201340\n|publisher=de Gruyter\n|ref=harv\n}}\n* {{cite journal\n|last=Halle\n|first=Morris\n|year=1983\n|title=On Distinctive Features and their articulatory implementation\n|url=\n|journal=Natural Language and Linguistic Theory\n|volume=1\n|number=1\n|pages=91\u2013105\n|ref=harv\n|doi=10.1007/BF00210377\n}}\n* {{cite book\n|editor1-last=Hardcastle\n|editor1-first=William\n|editor2-last=Laver\n|editor2-first=John\n|editor3-last=Gibbon\n|editor3-first=Fiona\n|year=2010\n|title=The Handbook of Phonetic Sciences\n|last=\n|first=\n|edition= 2nd\n|location=\n|pages=\n|publisher=Wiley-Blackwell\n|isbn=978-1-405-14590-9\n|ref=harv\n}}\n* {{cite book\n|author=International Phonetic Association\n|year=1999\n|isbn=\n|location=\n|pages=\n|title=Handbook of the International Phonetic Association\n|publisher=Cambridge University Press\n|ref=harv\n}}\n* {{cite book\n|author=International Phonetic Association\n|year=2015\n|isbn=\n|location=\n|pages=\n|title=International Phonetic Alphabet\n|publisher=International Phonetic Association\n|ref=harv\n}}\n* {{cite journal\n|last=Jaeger\n|first=Florian\n|first2=Katrina\n|last2=Furth\n|first3=Caitlin\n|last3=Hilliard\n|title=Phonological overlap affects lexical selection during sentence production\n|journal=Journal of Experimental Psychology: Learning, Memory, and Cognition\n|volume=38\n|issue=5\n|year=2012\n|pages=1439\u20131449\n|doi=10.1037/a0027862\n|pmid=22468803\n|ref=harv\n}}\n* {{cite book\n|last1=Jakobson\n|first1=Roman\n|first2=Gunnar\n|last2=Fant\n|first3=Morris\n|last3=Halle\n|year=1976\n|title=Preliminaries to Speech Analysis: The Distinctive Features and their Correlates\n|publisher=MIT Press\n|isbn=978-0-262-60001-9\n|location=\n|pages=\n|ref={{sfnref|Jakobson, Fant, and Halle|1976}}\n}}\n* {{cite book \n|title=Acoustic and auditory phonetics \n|last=Johnson \n|first=Keith \n|date=2003 \n|publisher=Blackwell Pub \n|isbn=1405101229 \n|edition= 2nd \n|oclc=50198698\n|ref=harv\n}}\n* {{cite book\n|last=Johnson\n|first=Keith\n|year=2011\n|title=Acoustic and Auditory Phonetics\n|edition= 3rd\n|location=\n|pages=\n|publisher=Wiley-Blackwell\n|isbn=978-1-444-34308-3\n|ref=harv\n}}\n* {{cite journal\n|last=Jones\n|first=Daniel\n|year=1948\n|title=The London school of phonetics\n|url=\n|journal=Zeitschrift f\u00fcr Phonetik\n|volume=11\n|number=3/4\n|pages=127\u2013135\n|ref=harv\n|via=}} (Reprinted in {{cite book|editor1-first=W. E.|editor1-last=Jones|editor2-first=J.|editor2-last=Laver|title=Phonetics in Linguistics|last=|first=|publisher=Longman|year=1973|isbn=|location=|pages=180\u2013186|ref=harv}})\n* {{cite journal\n|last=Keating\n|first=Patricia\n|last2=Lahiri\n|first2=Aditi\n|year=1993\n|title=Fronted Velars, Palatalized Velars, and Palatals\n|url=\n|journal=Phonetica\n|volume=50\n|issue=2\n|pages=73\u2013101\n|doi=10.1159/000261928\n|pmid=8316582\n|ref=harv\n}}\n* {{cite book\n|last=Kingston\n|first=John\n|year=2007\n|pages=\n|chapter=The Phonetics-Phonology Interface\n|title=The Cambridge Handbook of Phonology\n|editor-first=Paul\n|location=\n|editor-last=DeLacy\n|publisher=Cambridge University Press\n|isbn=978-0-521-84879-4\n|ref=harv\n}}\n* {{cite book\n|last=Kiparsky\n|first=Paul\n|year=1993\n|isbn=\n|chapter=P\u0101\u1e47inian linguistics\n|title=Encyclopedia of Languages and Linguistics\n|editor-last=Asher\n|editor-first=R.E.\n|place=Oxford\n|pages=\n|publisher=Pergamon\n|ref=harv\n}}\n* {{cite journal\n|last=Ladefoged\n|first=Peter\n|year=1960\n|title=The Value of Phonetic Statements\n|url=\n|journal=Language\n|volume=36\n|number=3\n|pages=387\u201396\n|jstor=410966\n|ref=harv\n|doi=10.2307/410966\n}}\n* {{cite book\n|last=Ladefoged\n|first=Peter\n|year=2001\n|title=A Course in Phonetics\n|place=Boston\n|pages=\n|publisher=[[Thomson/Wadsworth]]\n|edition=4th\n|isbn=978-1-413-00688-9\n|ref=harv\n|url=https://archive.org/details/courseinphonetic00lade_0\n}}\n* {{cite book\n|last=Ladefoged\n|first=Peter\n|year=2005\n|title=A Course in Phonetics\n|place=Boston\n|pages=\n|publisher=[[Thomson/Wadsworth]]\n|edition=5th\n|isbn=978-1-413-00688-9\n|ref=harv\n|url=https://archive.org/details/courseinphonetic00lade_0\n}}\n* {{cite book\n|last1=Ladefoged\n|first1=Peter\n|authorlink1=Peter Ladefoged\n|last2=Johnson\n|first2=Keith\n|year=2011\n|title=A Course in Phonetics\n|edition= 6th\n|publisher=Wadsworth\n|isbn=978-1-42823126-9\n|ref=harv\n}}\n* {{cite book\n|last=Ladefoged\n|first=Peter\n|first2=Ian\n|last2=Maddieson\n|year=1996\n|title=The Sounds of the World's Languages\n|place=Oxford\n|pages=\n|publisher=Blackwell\n|isbn=978-0-631-19815-4\n|ref=harv\n}}\n* {{cite journal\n|last=Levelt\n|first=Willem\n|title=A theory of lexical access in speech production\n|journal=Behavioral and Brain Sciences\n|volume=22\n|issue=1\n|pages=3\u20136\n| pmid = 11301520 \n| doi=10.1017/s0140525x99001776\n|year=1999\n|hdl=11858/00-001M-0000-0013-3E7A-A\n|ref=harv\n}}\n* {{cite book\n|title=A Critical Introduction to Phonetics\n|last=Lodge\n|first=Ken\n|publisher=Continuum International Publishing Group\n|year=2009\n|isbn=978-0-8264-8873-2\n|location=New York\n|pages=\n|ref=harv\n}}\n* {{cite book\n|last=L\u00f6fqvist\n|first=Anders\n|year=2010\n|isbn=\n|chapter=Theories and Models of Speech Production\n|title=Handbook of Phonetic Sciences\n|edition= 2nd\n|location=\n|pages=353\u201378\n|ref=harv\n}}\n* {{cite journal\n|last=Maddieson\n|first=Ian\n|year=1993\n|title=Investigating Ewe articulations with electromagnetic articulography\n|url=\n|journal=Forschungberichte des Intituts f\u00fcr Phonetik und Sprachliche Kommunikation der Universit\u00e4t M\u00fcnchen\n|volume=31\n|pages=181\u2013214\n|ref=harv\n|via=\n}}\n* {{cite book\n|last=Maddieson\n|first=Ian\n|year=2013\n|isbn=\n|chapter=Uvular Consonants\n|title=The World Atlas of Language Structures Online\n|editor1-last=Dryer\n|editor1-first=Matthew S.\n|editor2-last=Haspelmath\n|editor2-first=Martin\n|place=Leipzig\n|pages=\n|publisher=Max Planck Institute for Evolutionary Anthropology\n|chapter-url=http://wals.info/chapter/6\n|ref=harv\n|title-link=World Atlas of Language Structures\n}}\n* {{cite journal\n|last=Mattingly\n|first=Ignatius\n|year=1990\n|title=The global character of phonetic gestures\n|journal=Journal of Phonetics\n|volume=18\n|issue=3\n|pages=445\u201352\n|url=http://www.haskins.yale.edu/Reprints/HL0739.pdf\n|ref=harv\n|via=\n|doi=10.1016/S0095-4470(19)30372-9\n}}\n* {{cite journal\n|last=Motley\n|first=Michael\n|first2=Carl\n|last2=Camden\n|first3=Bernard\n|last3=Baars\n|title=Covert formulation and editing of anomalies in speech production: Evidence from experimentally elicited slips of the tongue\n|journal=Journal of Verbal Learning and Verbal Behavior\n|volume=21\n|number=5\n|year=1982\n|pages=578\u2013594\n|doi=10.1016/S0022-5371(82)90791-5\n|ref=harv\n}}\n* {{cite journal\n|last1=Munhall\n|first1=K.\n|last2=Ostry\n|first2=D\n|last3=Flanagan\n|first3=J.\n|year=1991\n|title=Coordinate spaces in speech planning\n|url=\n|journal=Journal of Phonetics\n|volume=19\n|issue=3\u20134\n|pages=293\u2013307\n|ref=harv\n|doi=10.1016/S0095-4470(19)30346-8\n}}\n* {{cite book\n|last1=O'Connor\n|first1=J.D.\n|title=Phonetics\n|date=1973\n|publisher=Pelican\n|isbn=978-0140215601\n|location=\n|pages=16\u201317\n|ref=harv\n}}\n* {{cite book\n|last=O'Grady\n|first=William\n|title=Contemporary Linguistics: An Introduction\n|edition= 5th\n|location=\n|pages=\n|publisher=Bedford/St. Martin's\n|year=2005\n|isbn=978-0-312-41936-3\n|ref=harv\n}}\n* {{cite journal\n|last1=Ohala\n|first1=John\n|year=1997\n|title=Aerodynamics of phonology\n|journal=Proceedings of the Seoul Internation Conference on Linguistics\n|volume=92\n|pages=\n|url=https://www.researchgate.net/publication/242446947\n|ref=harv\n|via=\n}}\n* {{cite book\n|location=\n|pages=\n|chapter=Phonetics, n.\n|title=Oxford English Dictionary Online\n|last=\n|first=\n|year=2018\n|isbn=\n|publisher=Oxford University Press\n|ref={{sfnref|Oxford English dictionary|2018}}\n}} <!-- No author so ordered by title-->\n* {{cite web\n|last=Roach\n|first=Peter\n|year=n.d.\n|title=Practical Phonetic Training\n|website=\n|publisher=Peter Roach\n|url=https://www.peterroach.net/practical-phonetic-training.html\n|archive-url=\n|archive-date=\n|access-date=10 May 2019\n|ref={{sfnref|Roach|n.d.}}\n}}\n* {{cite journal\n|last=Saltzman\n|first=Elliot\n|last2=Munhall\n|first2=Kevin\n|year=1989\n|title=Dynamical Approach to Gestural Patterning in Speech Production\n|journal=Ecological Psychology\n|volume=1\n|number=4\n|pages=333\u201382\n|url=http://www.haskins.yale.edu/sr/SR099/SR099_03.pdf\n|ref=harv\n|via=\n|doi=10.1207/s15326969eco0104_2\n}}\n* {{cite book\n|last=Scatton\n|first=Ernest\n|year=1984\n|title=A reference grammar of modern Bulgarian\n|publisher=Slavica\n|isbn=978-0893571238\n|location=\n|pages=\n|ref=harv\n}}\n* {{cite book\n |last=Schacter\n |first=Daniel\n |last2=Gilbert\n |first2=Daniel\n |last3=Wegner\n |first3=Daniel\n | chapter = Sensation and Perception\n | editor = Charles Linsmeiser\n | title = Psychology\n | chapter-url = https://archive.org/details/psychology0000scha\n | chapter-url-access = registration\n | publisher = Worth Publishers\n | year = 2011\n | isbn = 978-1-4292-3719-2\n | ref=harv\n}}\n* {{cite journal\n|last=Schiller\n|first=Niels\n|last2=Bles\n|first2=Mart\n|last3=Jansma\n|first3=Bernadette\n|date=2003\n|title=Tracking the time course of phonological encoding in speech production: an event-related brain potential study\n|url=\n|journal=Cognitive Brain Research\n|volume=17\n|issue=3\n|pages=819\u2013831\n|doi=10.1016/s0926-6410(03)00204-0\n|pmid=14561465\n|ref=harv\n}}\n* {{cite book\n|last=Sedivy\n|first=Julie\n|year=2019\n|title=Language in Mind: An Introduction to Psycholinguistics\n|edition=2nd\n|isbn=978-1605357058\n|ref=harv\n}}\n* {{cite book\n|last=Seikel\n|first=J. Anthony\n|last2=Drumright\n|first2=David\n|last3=King\n|first3=Douglas\n|year=2016\n|title=Anatomy and Physiology for Speech, Language, and Hearing\n|edition= 5th\n|location=\n|pages=\n|publisher=Cengage\n|isbn=978-1-285-19824-8\n|ref=harv\n}}\n* {{cite journal\n|last1=Skipper\n|first1=Jeremy\n|first2=Joseph\n|last2=Devlin\n|first3=Daniel\n|last3=Lametti\n|title=The hearing ear is always found close to the speaking tongue: Review of the role of the motor system in speech perception\n|journal=Brain and Language\n|volume=164\n|year=2017\n|pages=77\u2013105\n|ref=harv\n|doi=10.1016/j.bandl.2016.10.004\n|pmid=27821280\n}}\n* {{cite book\n|last=Stearns\n|first=Peter\n|title=World Civilizations\n|url=https://archive.org/details/worldcivilizatio00stea_1\n|url-access=registration\n|year=2001\n|publisher=Longman\n|location=New York\n|pages=\n|isbn=978-0-321-04479-2\n|edition= 3rd\n|author2=Adas, Michael\n|author3=Schwartz, Stuart\n|author4=Gilbert, Marc Jason\n|ref=harv\n}}\n* {{cite book\n|last=Trask\n|first=R.L.\n|authorlink=Larry Trask\n|year=1996\n|title=A Dictionary of Phonetics and Phonology\n|place=Abingdon\n|pages=\n|publisher=Routledge\n|isbn=978-0-415-11261-1\n|ref=harv}}\n* {{cite book\n | last = Yost\n | first = William\n | chapter = Audition\n |editor1=Alice F. Healy |editor2=Robert W. Proctor | title = Handbook of Psychology: Experimental psychology\n | publisher = John Wiley and Sons\n | year = 2003\n | isbn = 978-0-471-39262-0\n | page = 130\n | chapter-url = https://books.google.com/books?id=sPkIn4sUyXEC&pg=PA130\n | ref=harv\n}}\n{{refend}}\n\n==External links==\n{{NSRW Poster}}\n*[https://users.castle.unc.edu/~jlsmith/phonetics-resources.html Collection of phonetics resources] by the University of North Carolina\n*[https://www.peterroach.net/uploads/3/6/5/8/3658625/english-phonetics-and-phonology4-glossary.pdf \"A Little Encyclopedia of Phonetics\"] by [[Peter Roach (phonetician)|Peter Roach]].\n*[https://dood.al/pinktrombone/ Pink Trombone], an interactive articulation simulator by Neil Thapen.\n\n{{Authority control}}\n\n[[Category:Phonetics| ]]\n", "text_old": "{{short description|Branch of linguistics that comprises the study of the sounds of human speech}}\n{{other uses}}\n{{Linguistics|Subfields}}\n'''Phonetics''' is a branch of [[linguistics]] that studies the sounds of human speech, or in the case of [[sign language]]s, the equivalent aspects of sign.{{sfn|O'Grady|2005|p=15}} Phoneticians\u2014linguists who specialize in phonetics\u2014study the physical properties of speech. The field of phonetics is traditionally divided into three subdisciplines based on the research questions involved such as how humans plan and execute movements to produce speech ([[articulatory phonetics]]), how different movements affect the properties of the resulting sound ([[acoustic phonetics]]), or how humans convert sound waves to linguistic information ([[auditory phonetics]]). Traditionally, the minimal linguistic unit of phonetics is the [[phone (phonetics)|phone]]\u2014a speech sound in a language\u2014which differs from the phonological unit of [[phoneme]]; the phoneme is an abstract categorization of phones.\n\nPhonetics broadly deals with two aspects of human speech: production\u2014the ways humans make sounds\u2014and perception\u2014the way speech is understood. The [[linguistic modality|modality]] of a language describes the method by which a language produces and perceives languages. Languages with oral-aural modalities such as English produce speech orally (using the mouth) and perceive speech aurally (using the ears). Many sign languages such as [[Auslan]] have a manual-visual modality and produce speech manually (using the hands) and perceive speech visually (using the eyes), while some languages like American Sign have manual-manual dialect for use in [[tactile signing]] by deafblind speakers where signs are produced with the hands and perceived with the hands as well.\n\nLanguage production consists of several interdependent processes which transform a nonlinguistic message into a spoken or signed linguistic signal. After identifying a message to be linguistically encoded, a speaker must select the individual words\u2014known as [[lexical item]]s\u2014to represent that message in a process called lexical selection. During phonological encoding, the mental representation of the words are assigned their phonological content as a sequence of [[phoneme]]s to be produced. The phonemes are specified for articulatory features which denote particular goals such as closed lips or the tongue in a particular location. These phonemes are then coordinated into a sequence of muscle commands that can be sent to the muscles, and when these commands are executed properly the intended sounds are produced.\n\nThese movements disrupt and modify an airstream which results in a sound wave. The modification is done by the articulators, with different places and manners of articulation producing different acoustic results. For example, the words ''tack'' and ''sack'' both begin with alveolar sounds in English, but differ in how far the tongue is from the alveolar ridge. This difference has large effects on the air stream and thus the sound that is produced. Similarly, the direction and source of the airstream can affect the sound. The most common airstream mechanism is pulmonic\u2014using the lungs\u2014but the glottis and tongue can also be used to produce airstreams.\n\nLanguage perception is the process by which a linguistic signal is decoded and understood by a listener. In order to perceive speech the continuous acoustic signal must be converted into discrete linguistic units such as [[phonemes]], [[morphemes]], and [[words]]. In order to correctly identify and categorize sounds, listeners prioritize certain aspects of the signal that can reliably distinguish between linguistic categories. While certain cues are prioritized over others, many aspects of the signal can contribute to perception. For example, though oral languages prioritize acoustic information, the [[McGurk effect]] shows that visual information is used to distinguish ambiguous information when the acoustic cues are unreliable.\n\nModern phonetics has three main branches:\n\n*[[Articulatory phonetics]] which studies the way sounds are made with the articulators\n*[[Acoustic phonetics]] which studies the acoustic results of different articulations\n*[[Auditory phonetics]] which studies the way listeners perceive and understand linguistic signals\n\nThe first known phonetic studies occurred in the [[Indic subcontinent]] during the 6th century BCE, among which was Hindu scholar [[P\u0101\u1e47ini]]'s articulatory description of [[voicing (phonetics)|voicing]], though this pioneering work was primarily concerned with the relationship between written Vedic texts and spoken vernacular languages. With the advent of modern phonetics in the 19th century CE, the focus of scholarship shifted to the physical properties of speech itself. Before the widespread availability of recording devices, phoneticians relied upon [[#Transcription|phonetic transcription systems]] to collect and share data. Some systems, such as the [[International Phonetic Alphabet]] are still in wide use among phoneticians.\n\n{{TOC limit|4}}\n\n== Production ==\n{{main|Language production}}\n<!--[[File:Places of articulation.svg|thumb|Passive and active places of articulation: (1) ''Exo-labial''; (2) ''Endo-labial''; (3) ''Dental''; (4) ''Alveolar''; (5) ''Post-alveolar''; (6) ''Pre-palatal''; (7) ''Palatal''; (8) ''Velar''; (9) ''Uvular''; (10) ''Pharyngeal''; (11) ''Glottal''; (12) ''Epiglottal''; (13) ''Radical''; (14) ''Postero-dorsal''; (15) ''Antero-dorsal''; (16) ''Laminal''; (17) ''Apical''; (18) ''Sub-apical'' or ''sub-laminal''.|alt=A midsagittal view of the mouth with numbers marking places of articulation.]] Need to find a place to put this -->\nLanguage production consists of several interdependent processes which transform a nonlinguistic message into a spoken or signed linguistic signal. Linguists debate whether the process of language production occurs in a series of stages (serial processing) or whether production processes occur in parallel.  After identifying a message to be linguistically encoded, a speaker must select the individual words\u2014known as [[lexical item]]s\u2014to represent that message in a process called lexical selection. The words are selected based on their meaning, which in linguistics is called [[Semantics|semantic]] information. Lexical selection activates the word's [[Lemma (psycholinguistics)|lemma]], which contains both semantic and grammatical information about the word.{{sfn|Dell|O'Seaghdha|1992}}{{efn|Linguists debate whether these stages can interact or whether they occur serially (compare {{harvtxt|Dell|Reich|1981}} and {{harvtxt|Motley|Camden|Baars|1982}}). For ease of description, the language production process is described as a series of independent stages, though recent evidence shows this is inaccurate.{{sfn|Sedivy|2019|p=439}} For further descriptions of interactive activation models see {{harvtxt|Jaeger|Furth|Hilliard|2012}}.}} \n\nAfter an utterance has been planned,{{efn|or after part of an utterance has been planned; see {{harvtxt|Gleitman|January|Nappa|Trueswell|2007}} for evidence of production before a message has been completely planned}} it then goes through phonological encoding. In this stage of language production, the mental representation of the words are assigned their phonological content as a sequence of [[phoneme]]s to be produced. The phonemes are specified for articulatory features which denote particular goals such as closed lips or the tongue in a particular location. These phonemes are then coordinated into a sequence of muscle commands that can be sent to the muscles, and when these commands are executed properly the intended sounds are produced.{{sfn|Boersma|1998}} Thus the process of production from message to sound can be summarized as the following sequence:{{efn|adapted from {{harvtxt|Sedivy|2019|p=411}} and {{harvtxt|Boersma|1998|p=11}}}}\n\n* Message planning\n* Lemma selection\n* Retrieval and assignment of phonological word forms\n* Articulatory specification\n* Muscle commands\n* Articulation\n* Speech sounds\n\n=== Place of articulation ===\n{{main|Place of articulation}}\n{{Places of articulation mini navbox}}\nSounds which are made by a full or partial construction of the vocal tract are called [[consonants]]. Consonants are pronounced in the vocal tract, usually in the mouth, and the location of this construction affects the resulting sound. Because of the close connection between the position of the tongue and the resulting sound, the place of articulation is an important concept in many subdisciplines of phonetics.\n\nSounds are partly categorized by the location of a construction as well as the part of the body doing the constricting. For example, in English the words ''fought'' and ''thought'' are a [[minimal pair]] differing only in the organ making the construction rather than the location of the construction. The \"f\" in ''fought'' is a labiodental articulation made with the bottom lip against the teeth. The \"th\" in ''thought'' is a linguodental articulation made with the tongue against the teeth. Constrictions made by the lips are called [[Labialized|labial]]s while those made with the tongue are called lingual. \n\nConstrictions made with the tongue can be made in several parts of the vocal tract, broadly classified into coronal, dorsal and radical places of articulation. [[Coronal consonant|Coronal]] articulations are made with the front of the tongue, [[Dorsal consonant|dorsal]] articulations are made with the back of the tongue, and [[Radical consonant|radical]] articulations are made in the [[pharynx]].{{sfn|Ladefoged|2001|p=5}} These divisions are not sufficient for distinguishing and describing all speech sounds.{{sfn|Ladefoged|2001|p=5}} For example, in English the sounds {{ipa|[s]}} and {{ipa|[\u0283]}} are both coronal, but they are produced in different places of the mouth. To account for this, more detailed places of articulation are needed based upon the area of the mouth in which the constriction occurs.{{sfn|Ladefoged|Maddieson|1996|p=9}}\n\n==== Labial ====\nArticulations involving the lips can be made in three different ways: with both lips (bilabial), with one lip and the teeth (labiodental), and with the tongue and the upper lip (linguolabial).{{sfn|Ladefoged|Maddieson|1996|p=16}} Depending on the definition used, some or all of these kinds of articulations may be categorized into the class of [[Labial consonant|labial articulation]]s. [[Bilabial consonant]]s are made with both lips. In producing these sounds the lower lip moves farthest to meet the upper lip, which also moves down slightly,{{sfn|Maddieson|1993}} though in some cases the force from air moving through the aperture (opening between the lips) may cause the lips to separate faster than they can come together.{{sfn|Fujimura|1961}} Unlike most other articulations, both articulators are made from soft tissue, and so bilabial stops are more likely to be produced with incomplete closures than articulations involving hard surfaces like the teeth or palate. Bilabial stops are also unusual in that an articulator in the upper section of the vocal tract actively moves downwards, as the upper lip shows some active downward movement.{{sfn|Ladefoged|Maddieson|1996|pp=16\u201317}} [[Linguolabial consonant]]s are made with the blade of the tongue approaching or contacting the upper lip. Like in bilabial articulations, the upper lip moves slightly towards the more active articulator. Articulations in this group do not have their own symbols in the International Phonetic Alphabet, rather, they are formed by combining an apical symbol with a diacritic implicitly placing them in the coronal category.{{sfn|International Phonetic Association|2015}}{{sfn|Ladefoged|Maddieson|1996|p=18}} They exist in a number of languages indigenous to [[Vanuatu]] such as [[Tangoa language|Tangoa]].\n\n[[Labiodental consonant]]s are made by the lower lip rising to the upper teeth. Labiodental consonants are most often [[fricative]]s while labiodental nasals are also typologically common.{{sfn|Ladefoged|Maddieson|1996|pp=17\u201318}} There is debate as to whether true labiodental [[plosive]]s occur in any natural language,{{sfn|Ladefoged|Maddieson|1996|p=17}} though a number of languages are reported to have labiodental plosives including [[Zulu language|Zulu]],{{sfn|Doke|1926}} [[Tonga language (Zambia and Zimbabwe)|Tonga]],{{sfn|Guthrie|1948|p=61}} and [[Shubi language|Shubi]].{{sfn|Ladefoged|Maddieson|1996|p=17}} \n\n==== Coronal ====\nCoronal consonants are made with the tip or blade of the tongue and, because of the agility of the front of the tongue, represent a variety not only in place but in the posture of the tongue. The coronal places of articulation represent the areas of the mouth where the tongue contacts or makes a constriction, and include dental, alveolar, and post-alveolar locations. Tongue postures using the tip of the tongue can be [[Apical consonant|apical]] if using the top of the tongue tip, [[laminal consonant|laminal]] if made with the blade of the tongue, or [[Retroflex consonant|sub-apical]] if the tongue tip is curled back and the bottom of the tongue is used. Coronals are unique as a group in that every [[manner of articulation]] is attested.{{sfn|International Phonetic Association|2015}}{{sfn|Ladefoged|Maddieson|1996|pp=19\u201331}} [[Australian languages]] are well known for the large number of coronal contrasts exhibited within and across languages in the region.{{sfn|Ladefoged|Maddieson|1996|p=28}} [[Dental consonant]]s are made with the tip or blade of the tongue and the upper teeth. They are divided into two groups based upon the part of the tongue used to produce them: apical dental consonants are produced with the tongue tip touching the teeth; interdental consonants are produced with the blade of the tongue as the tip of the tongue sticks out in front of the teeth. No language is known to use both contrastively though they may exist [[allophone|allophonically]]. [[Alveolar consonant]]s are made with the tip or blade of the tongue at the alveolar ridge just behind the teeth and can similarly be apical or laminal.{{sfn|Ladefoged|Maddieson|1996|pp=19\u201325}}\n\nCrosslinguistically, dental consonants and alveolar consonants are frequently contrasted leading to a number of generalizations of crosslinguistic patterns. The different places of articulation tend to also be contrasted in the part of the tongue used to produce them: most languages with dental stops have laminal dentals, while languages with apical stops usually have apical stops. Languages rarely have two consonants in the same place with a contrast in laminality, though [[Taa]] (\u01c3X\u00f3\u00f5) is a counterexample to this pattern.{{sfn|Ladefoged|Maddieson|1996|pp=20, 40\u20131}} If a language has only one of a dental stop or an alveolar stop, it will usually be laminal if it is a dental stop, and the stop will usually be apical if it is an alveolar stop, though for example [[Temne language|Temne]] and [[Bulgarian language|Bulgarian]]{{sfn|Scatton|1984|p=60}} do not follow this pattern.{{sfn|Ladefoged|Maddieson|1996|p=23}} If a language has both an apical and laminal stop, then the laminal stop is more likely to be affricated like in [[Isoko language|Isoko]], though [[Dahalo language|Dahalo]] show the opposite pattern with alveolar stops being more affricated.{{sfn|Ladefoged|Maddieson|1996|pp=23\u20135}}\n\n[[Retroflex consonant]]s have several different definitions depending on whether the position of the tongue or the position on the roof of the mouth is given prominence. In general, they represent a group of articulations in which the tip of the tongue is curled upwards to some degree. In this way, retroflex articulations can occur in several different locations on the roof of the mouth including alveolar, post-alveolar, and palatal regions. If the underside of the tongue tip makes contact with the roof of the mouth, it is sub-apical though apical post-alveolar sounds are also described as retroflex.{{sfn|Ladefoged|Maddieson|1996|pp=25, 27\u20138}} Typical examples of sub-apical retroflex stops are commonly found in [[Dravidian languages]], and in some [[Indigenous languages of the Americas|languages indigenous to the southwest United States]] the contrastive difference between dental and alveolar stops is a slight retroflexion of the alveolar stop.{{sfn|Ladefoged|Maddieson|1996|p=27}} Acoustically, retroflexion tends to affect the higher formants.{{sfn|Ladefoged|Maddieson|1996|p=27}}\n\nArticulations taking place just behind the alveolar ridge, known as [[post-alveolar consonant]]s, have been referred to using a number of different terms. Apical post-alveolar consonants are often called retroflex, while laminal articulations are sometimes called palato-alveolar;{{sfn|Ladefoged|Maddieson|1996|pp=27\u20138}} in the Australianist literature, these laminal stops are often described as 'palatal' though they are produced further forward than the palate region typically described as palatal.{{sfn|Ladefoged|Maddieson|1996|p=28}} Because of individual anatomical variation, the precise articulation of palato-alveolar stops (and coronals in general) can vary widely within a speech community.{{sfn|Ladefoged|Maddieson|1996|p=32}}\n\n==== Dorsal ====\nDorsal consonants are those consonants made using the tongue body rather than the tip or blade and are typically produced at the palate, velum or uvula. [[Palatal consonants]] are made using the tongue body against the hard palate on the roof of the mouth. They are frequently contrasted with velar or uvular consonants, though it is rare for a language to contrast all three simultaneously, with [[Jaqaru]] as a possible example of a three-way contrast.{{sfn|Ladefoged|Maddieson|1996|p=35}} [[Velar consonants]] are made using the tongue body against the [[Soft palate|velum]]. They are incredibly common cross-linguistically; almost all languages have a velar stop. Because both velars and vowels are made using the tongue body, they are highly affected by [[coarticulation]] with vowels and can be produced as far forward as the hard palate or as far back as the uvula. These variations are typically divided into front, central, and back velars in parallel with the vowel space.{{sfn|Ladefoged|Maddieson|1996|pp=33\u201334}} They can be hard to distinguish phonetically from palatal consonants, though are produced slightly behind the area of prototypical palatal consonants.{{sfn|Keating|Lahiri|1993|p=89}} [[Uvular consonants]] are made by the tongue body contacting or approaching the uvula. They are rare, occurring in an estimated 19 percent of languages, and large regions of the Americas and Africa have no languages with uvular consonants. In languages with uvular consonants, stops are most frequent followed by [[continuant]]s (including nasals).{{sfn|Maddieson|2013}}\n\n==== Pharyngeal and laryngeal ====\nConsonants made by constrictions of the throat are pharyngeals, and those made by a constriction in the larynx are laryngeal. Laryngeals are made using the vocal folds as the larynx is too far down the throat to reach with the tongue. Pharyngeals however are close enough to the mouth that parts of the tongue can reach them.\n\nRadical consonants either use the root of the tongue or the [[epiglottis]] during production and are produced very far back in the vocal tract.{{Sfn|Ladefoged|Maddieson|5=1996|p=11}} [[Pharyngeal consonant]]s are made by retracting the root of the tongue far enough to almost touch the wall of the [[pharynx]]. Due to production difficulties, only fricatives and approximants can produced this way.{{Sfn|Lodge|2009|p=33}}{{Sfn|Ladefoged|Maddieson|1996|p=37}} [[Epiglottal consonant]]s are made with the epiglottis and the back wall of the pharynx. Epiglottal stops have been recorded in [[Dahalo language|Dahalo]].{{Sfn|Ladefoged|Maddieson|p=37}} Voiced epiglottal consonants are not deemed possible due to the cavity between the [[glottis]] and epiglottis being too small to permit voicing.{{Sfn|Ladefoged|Maddieson|1996|p=38}}\n\nGlottal consonants are those produced using the vocal folds in the larynx. Because the vocal folds are the source of phonation and below the oro-nasal vocal tract, a number of glottal consonants are impossible such as a voiced glottal stop. Three glottal consonants are possible, a voiceless glottal stop and two glottal fricatives, and all are attested in natural languages.{{sfn|International Phonetic Association|2015}} [[Glottal stop]]s, produced by closing the [[Vocal cords|vocal folds]], are notably common in the world's languages.{{Sfn|Ladefoged|Maddieson|1996|p=38}} While many languages use them to demarcate phrase boundaries, some languages like [[Huautla Mazatec|Huatla Mazatec]] have them as contrastive phonemes. Additionally, glottal stops can be realized as [[laryngealization]] of the following vowel in this language.{{Sfn|Ladefoged|Maddieson|1996|p=74}} Glottal stops, especially between vowels, do usually not form a complete closure. True glottal stops normally occur only when they're [[Gemination|geminated]].{{Sfn|Ladefoged|Maddieson|1996|p=75}}\n\n===The larynx===\n{{further|Larynx}}\n[[File:Larynx (top view).jpg|thumb|A top-down view of the larynx.|alt=See caption]]\n<!--This text previously a part of the voicing section below-->\nThe larynx, commonly known as the \"voice box\", is a cartilaginous structure in the [[trachea]] responsible for [[phonation]]. The vocal folds (chords) are held together so that they vibrate, or held apart so that they do not. The positions of the vocal folds are achieved by movement of the [[arytenoid cartilage]]s.{{sfn|Ladefoged|2001|p=123}} The [[Larynx#Intrinsic|intrinsic laryngeal muscles]] are responsible for moving the arytenoid cartilages as well as modulating the tension of the vocal folds.{{sfn|Seikel|Drumright|King|2016|p=222}} If the vocal folds are not close or tense enough, they will either vibrate sporadically or not at all. If they vibrate sporadically it will result in either creaky or breathy voice, depending on the degree; if don't vibrate at all, the result will be [[voicelessness]]. \n\nIn addition to correctly positioning the vocal folds, there must also be air flowing across them or they will not vibrate. The difference in pressure across the glottis required for voicing is estimated at 1 \u2013 2 [[Centimetre of water|cm H<sub>2</sub>0]] (98.0665 \u2013 196.133 pascals<!--this conversion is based on page 47 of https://physics.nist.gov/cuu/pdf/sp811.pdf-->).{{sfn|Ohala|1997|p=1}} The pressure differential can fall below levels required for phonation either because of an increase in pressure above the glottis (superglottal pressure) or a decrease in pressure below the glottis (subglottal pressure). The subglottal pressure is maintained by the [[respiratory muscles]]. Supraglottal pressure, with no constrictions or articulations, is equal to about [[atmospheric pressure]]. However, because articulations\u2014especially consonants\u2014represent constrictions of the airflow, the pressure in the cavity behind those constrictions can increase resulting in a higher supraglottal pressure.{{sfn|Chomsky|Halle|1968|pp=300\u2013301}}\n\n=== Lexical access ===\nAccording to the lexical access model two different stages of cognition are employed; thus, this concept is known as the two-stage theory of lexical access. The first stage, lexical selection provides information about lexical items required to construct the functional level representation. These items are retrieved according to their specific semantic and syntactic properties, but phonological forms are not yet made available at this stage. The second stage, retrieval of wordforms, provides information required for building the positional level representation.{{sfn|Altmann|2002}}\n\n===Articulatory models===\nWhen producing speech, the articulators move through and contact particular locations in space resulting in changes to the acoustic signal. Some models of speech production take this as the basis for modeling articulation in a coordinate system that may be internal to the body (intrinsic) or external (extrinsic). Intrinsic coordinate systems model the movement of articulators as positions and angles of joints in the body. Intrinsic coordinate models of the jaw often use two to three degrees of freedom representing translation and rotation. These face issues with modeling the tongue which, unlike joints of the jaw and arms, is a [[muscular hydrostat]]\u2014like an elephant trunk\u2014which lacks joints.{{sfn|L\u00f6fqvist|2010|p=359}} Because of the different physiological structures, movement paths of the jaw are relatively straight lines during speech and mastication, while movements of the tongue follow curves.{{sfn|Munhall|Ostry|Flanagan|1991|p=299|ps=, ''et seq.''}}\n\nStraight-line movements have been used to argue articulations as planned in extrinsic rather than intrinsic space, though extrinsic coordinate systems also include acoustic coordinate spaces, not just physical coordinate spaces.{{sfn|L\u00f6fqvist|2010|p=359}} Models that assume movements are planned in extrinsic space run into an [[inverse problem]] of explaining the muscle and joint locations which produce the observed path or acoustic signal. The arm, for example, has seven degrees of freedom and 22 muscles, so multiple different joint and muscle configurations can lead to the same final position. For models of planning in extrinsic acoustic space, the same one-to-many mapping problem applies as well, with no unique mapping from physical or acoustic targets to the muscle movements required to achieve them. Concerns about the inverse problem may be exaggerated, however, as speech is a highly learned skill using neurological structures which evolved for the purpose.{{sfn|L\u00f6fqvist|2010|p=360}}\n\n<!-- See Bizzi et al 1992 on equilibrium-point model -->\nThe equilibrium-point model proposes a resolution to the inverse problem by arguing that movement targets be represented as the position of the muscle pairs acting on a joint.{{efn|See {{harvtxt|Feldman|1966}} for the original proposal.}} Importantly, muscles are modeled as springs, and the target is the equilibrium point for the modeled spring-mass system. By using springs, the equilibrium point model can easily account for compensation and response when movements are disrupted. They are considered a coordinate model because they assume that these muscle positions are represented as points in space, equilibrium points, where the spring-like action of the muscles converges.{{sfn|Bizzi|Hogan|Mussa-Ivaldi|Giszter|1992}}{{sfn|L\u00f6fqvist|2010|p=361}}\n\nGestural approaches to speech production propose that articulations are represented as movement patterns rather than particular coordinates to hit. The minimal unit is a gesture that represents a group of \"functionally equivalent articulatory movement patterns that are actively controlled with reference to a given speech-relevant goal (e.g., a bilabial closure).\"{{sfn|Saltzman|Munhall|1989}} These groups represent coordinative structures or \"synergies\" which view movements not as individual muscle movements but as task-dependent groupings of muscles which work together as a single unit.{{sfn|Mattingly|1990}}{{sfn|L\u00f6fqvist|2010|pp=362\u20134}} This reduces the degrees of freedom in articulation planning, a problem especially in intrinsic coordinate models, which allows for any movement that achieves the speech goal, rather than encoding the particular movements in the abstract representation. Coarticulation is well described by gestural models as the articulations at faster speech rates can be explained as composites of the independent gestures at slower speech rates.{{sfn|L\u00f6fqvist|2010|p=364}}\n\n== Acoustics ==\n[[File:Waveform spectrogram and transcription of wikipedia in praat.png|thumb|upright=1.5|A waveform (top), spectrogram (middle), and transcription (bottom) of a woman saying \"Wikipedia\" displayed using the [[Praat]] software for linguistic analysis. {{listen |filename=En-us-Wikipedia.ogg |title=Listen|description=The accompanying audio.|plain=yes}}<!--Aesthetics could be better for the audio-->]]\nSpeech sounds are created by the modification of an airstream which results in a sound wave. The modification is done by the articulators, with different places and manners of articulation producing different acoustic results. Because the posture of the vocal tract, not just the position of the tongue can affect the resulting sound, the [[manner of articulation]] is important for describing the speech sound. The words ''tack'' and ''sack'' both begin with alveolar sounds in English, but differ in how far the tongue is from the alveolar ridge. This difference has large affects on the air stream and thus the sound that is produced. Similarly, the direction and source of the airstream can affect the sound. The most common airstream mechanism is pulmonic\u2014using the lungs\u2014but the glottis and tongue can also be used to produce airstreams.\n\n===Voicing and phonation types===\n<!--{{Further|#Models of phonation|#The larynx}}-->\nA major distinction between speech sounds is whether they are voiced. Sounds are voiced when the vocal folds begin to vibrate in the process of phonation. Many sounds can be produced with or without phonation, though physical constraints may make phonation difficult or impossible for some articulations. When articulations are voiced, the main source of noise is the periodic vibration of the vocal folds. Articulations like voiceless plosives have no acoustic source and are noticeable by their silence, but other voiceless sounds like fricatives create their own acoustic source regardless of phonation.\n\nPhonation is controlled by the muscles of the larynx, and languages make use of more acoustic detail than binary voicing. During phonation, the vocal folds vibrate at a certain rate. This vibration results in a periodic acoustic waveform comprising a [[fundamental frequency]] and its harmonics. The fundamental frequency of the acoustic wave can be controlled by adjusting the muscles of the larynx, and listeners perceive this fundamental frequency as pitch. Languages use pitch manipulation to convey lexical information in tonal languages, and many languages use pitch to mark prosodic or pragmatic information.\n\nFor the vocal folds to vibrate, they must be in the proper position and there must be air flowing through the glottis.{{sfn|Ohala|1997|p=1}} Phonation types are modeled on a continuum of glottal states from completely open (voiceless) to completely closed (glottal stop). The optimal position for vibration, and the phonation type most used in speech, modal voice, exists in the middle of these two extremes. If the glottis is slightly wider, breathy voice occurs, while bringing the vocal folds closer together results in creaky voice.{{sfn|Gordon|Ladefoged|2001}}\n\nThe normal phonation pattern used in typical speech is modal voice, where the vocal folds are held close together with moderate tension. The vocal folds vibrate as a single unit periodically and efficiently with a full glottal closure and no aspiration.{{Sfn|Gobl|N\u00ed Chasaide|2010|p=399}} If they are pulled farther apart, they do not vibrate and so produce voiceless phones. If they are held firmly together they produce a glottal stop.{{sfn|Gordon|Ladefoged|2001}}<!--I think, double check-->\n\nIf the vocal folds are held slightly further apart than in modal voicing, they produce phonation types like breathy voice (or murmur) and whispery voice. The tension across the vocal ligaments ([[vocal cords]]) is less than in modal voicing allowing for air to flow more freely. Both breathy voice and whispery voice exist on a continuum loosely characterized as going from the more periodic waveform of breathy voice to the more noisy waveform of whispery voice. Acoustically, both tend to dampen the first formant with whispery voice showing more extreme deviations. {{sfn|Gobl|N\u00ed Chasaide|2010|p=400-401}}\n\nHolding the vocal folds more tightly together results in a creaky voice. The tension across the vocal folds is less than in modal voice, but they are held tightly together resulting in only the ligaments of the vocal folds vibrating.{{efn|See [[#The larynx]] for further information on the anatomy of phonation.}} The pulses are highly irregular, with low pitch and frequency amplitude.{{sfn|Gobl|N\u00ed Chasaide|2010|p=401}}\n<!--Gobl and N\u00ed Chasaide has coverage of tense and lax, but I think that might be too much here-->\n\nSome languages do not maintain a voicing distinction for some consonants,{{efn|Hawaiian, for example, does not contrast voiced and voiceless plosives.}} but all languages use voicing to some degree. For example, no language is known to have a phonemic voicing contrast for vowels with all known vowels canonically voiced.{{efn|There are languages, like [[Japanese language|Japanese]], where vowels are produced as voiceless in certain contexts.}} Other positions of the glottis, such as breathy and creaky voice, are used in a number of languages, like [[Jalapa Mazatec]], to contrast [[phonemes]] while in other languages, like English, they exist allophonically. \n\nThere are several ways to determine if a segment is voiced or not, the simplest being to feel the larynx during speech and note when vibrations are felt. More precise measurements can be obtained through acoustic analysis of a spectrogram or spectral slice. In a spectrographic analysis, voiced segments show a voicing bar, a region of high acoustic energy, in the low frequencies of voiced segments.{{sfn|Dawson|Phelan|2016}} In examining a spectral splice, the acoustic spectrum at a given point in time a model of the vowel pronounced reverses the filtering of the mouth producing the spectrum of the glottis. A computational model of the unfiltered glottal signal is then fitted to the inverse filtered acoustic signal to determine the characteristics of the glottis.{{sfn|Gobl|N\u00ed Chasaide|2010|pp=388, ''et seq''}} Visual analysis is also available using specialized medical equipment such as ultrasound and endoscopy.{{sfn|Dawson|Phelan|2016}}{{efn|See [[#Articulatory models]] for further information on acoustic modeling.}}\n\n=== Vowels ===\n{{IPA vowels|class=floatright}}\nVowels are broadly categorized by the area of the mouth in which they are produced, but because they are produced without a constriction in the vocal tract their precise description relies on measuring acoustic correlates of tongue position. The location of the tongue during vowel production changes the frequencies at which the cavity resonates, and it is these resonances\u2014known as [[formants]]\u2014which are measured and used to characterize vowels.\n\nVowel height traditionally refers to the highest point of the tongue during articulation.{{Sfn|Ladefoged|Maddieson|1996|p=282}} The height parameter is divided into four primary levels: high (close), close-mid, open-mid and low (open). Vowels whose height are in the middle are referred to as mid. Slightly opened close vowels and slightly closed open vowels are referred to as near-close and near-open respectively. The lowest vowels are not just articulated with a lowered tongue, but also by lowering the jaw.{{Sfn|Lodge|2009|p=39}}\n\nWhile the IPA implies that there are seven levels of vowel height, it is unlikely that a given language can minimally contrast all seven levels. [[Noam Chomsky|Chomsky]] and [[Morris Halle|Halle]] suggest that there are only three levels,{{Sfn|Chomsky|Halle|1968|p=}} although four levels of vowel height seem to be needed to describe [[Danish language|Danish]] and it's possible that some languages might even need five.{{Sfn|Ladefoged|Maddieson|1996|p=289}}\n\nVowel backness is dividing into three levels: front, central and back. Languages usually do not minimally contrast more than two levels of vowel backness. Some languages claimed to have a three-way backness distinction include [[Nimboran language|Nimboran]] and [[Norwegian language|Norwegian]].{{Sfn|Ladefoged|Maddieson|p=290}}\n\nIn most languages, the lips during vowel production can be classified as either rounded or unrounded (spread), although other types of lip positions, such as compression and protrusion, have been described. Lip position is correlated with height and backness: front and low vowels tend to be unrounded whereas back and high vowels are usually rounded.{{Sfn|Ladefoged|Maddieson|p=292-295}} Paired vowels on the IPA chart have the spread vowel on the left and the rounded vowel on the right.{{Sfn|Lodge|2009|p=40}}\n\nTogether with the universal vowel features described above, some languages have additional features such as [[Nasal vowel|nasality]], [[Vowel length|length]] and different types of phonation such as [[Voiceless vowel|voiceless]] or [[Creaky voice|creaky]]. Sometimes more specialized tongue gestures such as [[Rhotic vowel|rhoticity]], [[Advanced and retracted tongue root|advanced tongue root]], [[pharyngealization]], [[Strident vowel|stridency]] and frication are required to describe a certain vowel.{{Sfn|Ladefoged|Maddieson|p=298}}\n\n=== Manner of articulation ===\n{{main|Manner of articulation}}\nKnowing the place of articulation is not enough to fully describe a consonant, the way in which the stricture happens is equally important. Manners of articulation describe how exactly the active articulator modifies, narrows or closes off the vocal tract.{{Sfn|Ladefoged|Johnson|2011|p=14}}\n\n[[Stop consonant|Stops]] (also referred to as plosives) are consonants where the airstream is completely obstructed. Pressure builds up in the mouth during the stricture, which is then released as a small burst of sound when the articulators move apart. The velum is raised so that air cannot flow through the nasal cavity. If the velum is lowered and allows for air to flow through the nose, the result in a nasal stop. However, phoneticians almost always refer to nasal stops as just \"nasals\".{{Sfn|Ladefoged|Johnson|2011|p=14}}[[Affricate consonant|Affricates]] are a sequence of stops followed by a fricative in the same place.{{Sfn|Ladefoged|Johnson|2011|p=67}}\n\n[[Fricative consonant|Fricatives]] are consonants where the airstream is made turbulent by partially, but not completely, obstructing part of the vocal tract.{{Sfn|Ladefoged|Johnson|2011|p=14}} [[Sibilant]]s are a special type of fricative where the turbulent airstream is directed towards the teeth,{{Sfn|Ladefoged|Maddieson|1996|p=145}} creating a high-pitched hissing sound.{{Sfn|Ladefoged|Johnson|2011|p=15}}\n\n[[Nasal consonant|Nasals]] (sometimes referred to as nasal stops) are consonants in which there's a closure in the oral cavity and the velum is lowered, allowing air to flow through the nose.{{Sfn|Ladefoged|Maddieson|1996|p=102}}\n\nIn an [[Approximant consonant|approximant]], the articulators come close together, but not to such an extent that allows a turbulent airstream.{{Sfn|Ladefoged|Johnson|2011|p=15}}\n\n[[Lateral consonant|Laterals]] are consonants in which the airstream is obstructed along the center of the vocal tract, allowing the airstream to flow freely on one or both sides.{{Sfn|Ladefoged|Johnson|2011|p=15}} Laterals have also been defined as consonants in which the tongue is contracted in such a way that the airstream is greater around the sides than over the center of the tongue.{{Sfn|Ladefoged|Maddieson|1996|p=182}} The first definition does not allow for air to flow over the tongue.\n\n[[Trill consonant|Trills]] are consonants in which the tongue or lips are set in motion by the airstream.{{Sfn|Ladefoged|Johnson|2011|p=175}} The stricture is formed in such a way that the airstream causes a repeating pattern of opening and closing of the soft articulator(s).{{Sfn|Ladefoged|Maddieson|1996|p=217}} Apical trills typically consist of two or three periods of vibration.{{Sfn|Ladefoged|Maddieson|1996|p=218}}\n\n[[Flap consonant|Taps]] and [[Flap consonant|flaps]] are single, rapid, usually [[Apical consonant|apical]] gestures where the tongue is thrown against the roof of the mouth, comparable to a very rapid stop.{{Sfn|Ladefoged|Johnson|2011|p=175}} These terms are sometimes used interchangeably, but some phoneticians make a distinction.{{Sfn|Ladefoged|Maddieson|1996|p=230-231}} In a tap, the tongue contacts the roof in a single motion whereas in a flap the tongue moves tangentially to the roof of the mouth, striking it in passing.\n\nDuring a [[glottalic airstream mechanism]], the glottis is closed, trapping a body of air. This allows for the remaining air in the vocal tract to be moved separately. An upward movement of the closed glottis will move this air out, resulting in it an [[Ejective consonant|ejective]] consonant. Alternatively, the glottis can lower, sucking more air into the mouth, which results in an [[Implosive consonant|implosive]] consonant.{{Sfn|Ladefoged|Johnson|2011|p=137}}\n\n[[Click consonant|Clicks]] are stops in which tongue movement causes air to be sucked in the mouth, this is referred to as a [[velaric airstream]].{{Sfn|Ladefoged|Maddieson|1996|p=78}} During the click, the air becomes [[Rarefaction|rarefied]] between two articulatory closures, producing a loud 'click' sound when the anterior closure is released. The release of the anterior closure is referred to as the click influx. The release of the posterior closure, which can be velar or uvular, is the click efflux. Clicks are used in several African language families, such as the [[Khoisan languages|Khoisan]] and [[Bantu languages|Bantu]] languages.{{Sfn|Ladefoged|Maddieson|1996|p=246-247}}\n\n===Pulmonary and subglottal system===\n{{see|Breathing}}\n<!--[[File:Respiratory system.svg|thumb]]-->\nThe lungs drive nearly all speech production, and their importance in phonetics is due to their creation of pressure for pulmonic sounds. The most common kinds of sound across languages are pulmonic egress, where air is exhaled from the lungs.{{sfn|Ladefoged|2001|p=1}} The opposite is possible, though no language is known to  have pulmonic ingressive sounds as phonemes.{{sfn|Eklund|2008|p=237}} Many languages such as [[Swedish language|Swedish]] use them for [[paralinguistic]] articulations such as affirmations in a number of genetically and geographically diverse languages.{{sfn|Eklund|2008}} Both egressive and ingressive sounds rely on holding the vocal folds in a particular posture and using the lungs to draw air across the vocal folds so that they either vibrate (voiced) or do not vibrate (voiceless).{{sfn|Ladefoged|2001|p=1}} Pulmonic articulations are restricted by the volume of air able to be exhaled in a given respiratory cycle, known as the [[vital capacity]].\n\nThe lungs are used to maintain two kinds of pressure simultaneously in order to produce and modify phonation. To produce phonation at all, the lungs must maintain a pressure of 3\u20135&nbsp;cm H<sub>2</sub>0 higher than the pressure above the glottis. However small and fast adjustments are made to the subglottal pressure to modify speech for suprasegmental features like stress. A number of thoracic muscles are used to make these adjustments. Because the lungs and thorax stretch during inhalation, the elastic forces of the lungs alone can produce pressure differentials sufficient for phonation at lung volumes above 50 percent of vital capacity.{{sfn|Seikel|Drumright|King|2016|p=176}} Above 50 percent of vital capacity, the [[respiratory muscles]] are used to \"check\" the elastic forces of the thorax to maintain a stable pressure differential. Below that volume, they are used to increase the subglottal pressure by actively exhaling air.\n\nDuring speech, the respiratory cycle is modified to accommodate both linguistic and biological needs. Exhalation, usually about 60 percent of the respiratory cycle at rest, is increased to about 90 percent of the respiratory cycle. Because metabolic needs are relatively stable, the total volume of air moved in most cases of speech remains about the same as quiet tidal breathing.{{sfn|Seikel|Drumright|King|2016|p=171}} Increases in speech intensity of 18&nbsp;dB (a loud conversation) has relatively little impact on the volume of air moved. Because their respiratory systems are not as developed as adults, children tend to use a larger proportion of their vital capacity compared to adults, with more deep inhales.{{sfn|Seikel|Drumright|King|2016|pp=168\u201377}}\n\n=== Source\u2013filter theory ===\n{{main|Source\u2013filter model}}\n{{Expand section|date=February 2020}}\nThe source\u2013filter model of speech is a theory of speech production which explains the link between vocal tract posture and the acoustic consequences. Under this model, the vocal tract can be modeled as a noise source coupled onto an [[acoustic filter]].{{sfn|Johnson|2008|p=83\u20135}} The noise source in many cases is the larynx during the process of voicing, though other noise sources can be modeled in the same way. The shape of the supraglottal vocal tract acts as the filter, and different configurations of the articulators result in different acoustic patterns. These changes are predictable. The vocal tract can be modeled as a sequence of tubes, closed at one end, with varying diameters, and by using equations for [[acoustic resonance]] the acoustic effect of an articulatory posture can be derived.{{sfn|Johnson|2008|p=104\u20135}} The process of inverse filtering uses this principle to analyze the source spectrum produced by the vocal folds during voicing. By taking the inverse of a predicted filter, the acoustic effect of the supraglottal vocal tract can be undone giving the acoustic spectrum produced by the vocal folds.{{sfn|Johnson|2008|p=157}} This allows quantitative study of the various phonation types.\n\n== Perception ==\n{{main|Speech perception}}\nLanguage perception is the process by which a linguistic signal is decoded and understood by a listener.{{efn|As with speech production, the nature of the linguistic signal varies depending on the [[language modality]]. The signal can be acoustic for oral speech, visual for signed languages, or tactile for manual-tactile sign languages. For simplicity acoustic speech is described here; for sign language perception specifically, see [[Sign language#Sign perception]].}} In order to perceive speech the continuous acoustic signal must be converted into discrete linguistic units such as [[phonemes]], [[morphemes]], and [[words]].{{sfn|Sedivy|2019|p=259\u201360}} In order to correctly identify and categorize sounds, listeners prioritize certain aspects of the signal that can reliably distinguish between linguistic categories.{{sfn|Sedivy|2019|p=269}} While certain cues are prioritized over others, many aspects of the signal can contribute to perception. For example, though oral languages prioritize acoustic information, the [[McGurk effect]] shows that visual information is used to distinguish ambiguous information when the acoustic cues are unreliable.{{sfn|Sedivy|2019|p=273}}\n\nWhile listeners can use a variety of information to segment the speech signal, the relationship between acoustic signal and category perception is not a perfect mapping. Because of [[coarticulation]], noisy environments, and individual differences, there is a high degree of acoustic variability within categories.{{sfn|Sedivy|2019|p=259}} Known as the problem of '''perceptual invariance'''<!--redirects here-->, listeners are able to reliably perceive categories despite the variability in acoustic instantiation.{{sfn|Sedivy|2019|p=260}} In order to do this, listeners rapidly accommodate to new speakers and will shift their boundaries between categories to match the acoustic distinctions their conversational partner is making.{{sfn|Sedivy|2019|p=274\u201385}}\n\n=== Audition ===\n{{hatnote|Main article: [[Hearing]]; for further information see [[Neuronal encoding of sound]]}}\n[[File:Journey of Sound to the Brain.ogg|thumb|How sounds make their way from the source to your brain|300x300px]]\nAudition, the process of hearing sounds, is the first stage of perceiving speech. Articulators cause systematic changes in air pressure which travel as sound waves to the listener's ear. The sound waves then hit the listener's [[ear drum]] causing it to vibrate. The vibration of the ear drum is transmitted by the [[ossicles]]\u2014three small bones of the middle ear\u2014to the [[cochlea]].{{sfn|Johnson|2003|p=46\u20137}} The cochlea is a spiral-shaped, fluid-filled tube divided lengthwise by the [[organ of Corti]] which contains the [[basilar membrane]]. The basilar membrane increases in thickness as it travels through the cochlea causing different frequencies to resonate at different locations. This [[tonotopic]] design allows for the ear to analyze sound in a manner similar to a [[Fourier transform]].{{sfn|Johnson|2003|p=47}}\n\nThe differential vibration of the basilar causes the [[hair cells]] within the organ of Corti to move. This causes [[depolarization]] of the hair cells and ultimately a conversion of the acoustic signal into a neuronal signal.{{sfn|Schacter|Gilbert|Wegner|2011|p=158\u20139}} While the hair cells do not produce [[action potential]]s themselves, they release neurotransmitter at synapses with the fibers of the [[auditory nerve]], which does produce action potentials. In this way, the patterns of oscillations on the basilar membrane are converted to [[spatiotemporal pattern]]s of firings which transmit information about the sound to the [[brainstem]].{{sfn|Yost|2003|p=130}}\n\n=== Prosody ===\n{{Main|Prosody (linguistics)}}\nBesides consonants and vowels, phonetics also describes the properties of speech that are not localized to [[Segment (linguistics)|segments]] but greater units of speech, such as [[Syllable|syllables]] and [[Phrase|phrases]]. Prosody includes [[Auditory phonetics|auditory characteristics]] such as [[Pitch (music)|pitch]], [[Isochrony|speech rate]], [[Duration (music)|duration]], and [[loudness]]. Languages use these properties to different degrees to implement [[Stress (linguistics)|stress]], [[Pitch accent (intonation)|pitch accents]], and [[Intonation (linguistics)|intonation]] \u2014 for example, [[Stress and vowel reduction in English|stress in English]] and [[Stress in Spanish|Spanish]] is correlated with changes in pitch and duration, whereas [[Welsh phonology|stress in Welsh]] is more consistently correlated with pitch than duration and stress in Thai is only correlated with duration.{{sfn|Cutler|2005}}\n\n=== Theories of speech perception ===\nEarly theories of speech perception such as [[motor theory]] attempted to solve the problem of perceptual invariance by arguing that speech perception and production were closely linked. In its strongest form, motor theory argues that speech perception ''requires'' the listener to access the articulatory representation of sounds;{{sfn|Sedivy|2019|p=289}} in order to properly categorize a sound, a listener reverse engineers the articulation which would produce that sound and by identifying these gestures is able to retrieve the intended linguistic category.{{sfn|Galantucci|Fowler|Turvey|2006}} While findings such as the McGurk effect and case studies from patients with neurological injuries have provided support for motor theory, further experiments have not supported the strong form of motor theory, though there is some support for weaker forms of motor theory which claim a non-deterministic relationship between production and perception.{{sfn|Galantucci|Fowler|Turvey|2006}}{{sfn|Sedivy|2019|p=292\u20133}}{{sfn|Skipper|Devlin|Lametti|2017}}\n\nSuccessor theories of speech perception place the focus on acoustic cues to sound categories and can be grouped into two broad categories: abstractionist theories and episodic theories.{{sfn|Goldinger|1996}} In abstractionist theories, speech perception involves the identification of an idealized lexical object based on a signal reduced to its necessary components and normalizing the signal to counteract speaker variability. Episodic theories such as the [[exemplar model]] argue that speech perception involves accessing detailed memories (i.e., [[episodic memories]]) of previously heard tokens. The problem of perceptual invariance is explained by episodic theories as an issue of familiarity: normalization is a byproduct of exposure to more variable distributions rather than a discrete process as abstractionist theories claim.{{sfn|Goldinger|1996}}\n\n==Development of the field==\n{{anchor|History}} <!--'History of phonetics' redirects here-->\nThe first known phonetic studies were carried out as early as the 6th century BCE by [[Sanskrit]] grammarians.{{sfn|Caffrey|2017}} The Hindu scholar [[P\u0101\u1e47ini]] is among the most well known of these early investigators, whose four-part grammar, written around 350 BCE, is influential in modern linguistics and still represents \"the most complete generative grammar of any language yet written\".{{sfn|Kiparsky|1993|p=2918}} His grammar formed the basis of modern linguistics and described several important phonetic principles, including voicing. This early account described resonance as being produced either by tone, when vocal folds are closed, or noise, when vocal folds are open. The phonetic principles in the grammar are considered \"primitives\" in that they are the basis for his theoretical analysis rather than the objects of theoretical analysis themselves, and the principles can be inferred from his system of phonology.{{sfn|Kiparsky|1993|pp=2922\u20133}}\n\nAdvancements in phonetics after P\u0101\u1e47ini and his contemporaries were limited until the modern era, save some limited investigations by Greek and Roman grammarians. In the millennia between Indic grammarians and modern phonetics, the focus shifted from the difference between spoken and written language, which was the driving force behind P\u0101\u1e47ini's account, and began to focus on the physical properties of speech alone. Sustained interest in phonetics began again around 1800 CE with the term \"phonetics\" being first used in the present sense in 1841.{{sfn|Oxford English Dictionary|2018}}{{sfn|Caffrey|2017}} With new developments in medicine and the development of audio and visual recording devices, phonetic insights were able to use and review new and more detailed data. This early period of modern phonetics included the development of an influential phonetic alphabet based on articulatory positions by [[Alexander Melville Bell]]. Known as [[visible speech]], it gained prominence as a tool in the [[Oralism|oral education of deaf children]].{{sfn|Caffrey|2017}}\n\nBefore the widespread availability of audio recording equipment, phoneticians relied heavily on a tradition of practical phonetics to ensure that transcriptions and findings were able to be consistent across phoneticians. This training involved both ear training\u2014the recognition of speech sounds\u2014as well as production training\u2014the ability to produce sounds. Phoneticians were expected to learn to recognize by ear the various sounds on the [[International Phonetic Alphabet]] and the IPA still tests and certifies speakers on their ability to accurately produce the phonetic patterns of English (though they have discontinued this practice for other languages).{{sfn|Roach|n.d.}} As a revision of his visible speech method, Melville Bell developed a description of vowels by height and backness resulting in 9 [[cardinal vowel]]s.{{sfn|Ladefoged|1960|p=388}} As part of their training in practical phonetics, phoneticians were expected to learn to produce these cardinal vowels in order to anchor their perception and transcription of these phones during fieldwork.{{sfn|Roach|n.d.}} This approach was critiqued by [[Peter Ladefoged]] in the 1960s based on experimental evidence where he found that cardinal vowels were auditory rather than articulatory targets, challenging the claim that they represented articulatory anchors by which phoneticians could judge other articulations.{{sfn|Ladefoged|1960}}\n\n== Subdisciplines ==\n===Acoustic phonetics===\n{{main|Acoustic phonetics}}\nAcoustic phonetics deals with the [[Acoustics|acoustic]] properties of speech sounds. The [[Sensation (psychology)|sensation]] of sound is caused by pressure fluctuations which cause the [[eardrum]] to move. The ear transforms this movement into neural signals that the brain registers as sound. Acoustic waveforms are records that measure these pressure fluctuations.{{Sfn|Johnson|2003|p=1}}\n\n===Articulatory phonetics===\n{{main|Articulatory phonetics}}\nArticulatory phonetics deals with the ways in which speech sounds are made.\n\n===Auditory phonetics===\n{{main|Auditory phonetics}}\nAuditory phonetics studies how humans perceive speech sounds. Due to the anatomical features of the auditory system distorting the speech signal, humans do not experience speech sounds as perfect acoustic records. For example, the auditory impressions of [[Loudness|volume]], measured in decibels (dB), does not linearly match the difference in sound pressure.{{Sfn|Johnson|2003|p=46-49}}\n\nThe mismatch between acoustic analyses and what the listener hears is especially noticeable in speech sounds that have a lot of high-frequency energy, such as certain fricatives. To reconcile this mismatch, functional models of the auditory system have been developed.{{Sfn|Johnson|2003|p=53}}\n\n==Describing sounds==\n{{anchor|Consonants}}\n<!-- See Language Files 12th ed. -->\n<!-- See A Course in Phonetics 4th ed. -->\n<!--[[File:Phonological anatomy 1.png|thumb|A diagram of anatomical locations in the vocal tract. (A) Nasal cavity; (B) alveolar ridge; (C) lips; (D) teeth; (E) tongue tip; (F) larynx; (G) glottis; (H) palate; (I) tongue body; (J) velum; (K) uvula; (L) trachea; (M) esophagus.|alt=See caption.]]\n[[File:Diagram showing the parts of the pharynx CRUK 334.svg|thumb]]-->\nHuman languages use many different sounds and in order to compare them linguists must be able to describe sounds in a way that is language independent. Speech sounds can be described in a number of ways. Most commonly speech sounds are referred to by the mouth movements needed to produce them. [[Consonant]]s and [[vowel]]s are two gross categories that phoneticians define by the movements in a speech sound. More fine-grained descriptors are parameters such as place of articulation. [[Place of articulation]], [[manner of articulation]], and [[voicing (phonetics)|voicing]]<!--Maybe these should go to the relevant sections instead of separate pages--> are used to describe consonants and are the main divisions of the [[International Phonetic Alphabet]] consonant chart. Vowels are described by their height, backness, and rounding. Sign language are described using a similar but [[sign language parameters|distinct set of parameters]] to describe signs: location, movement, hand shape, palm orientation, and non-manual features. In addition to articulatory descriptions, sounds used in oral languages can be described using their acoustics. Because the acoustics are a consequence of the articulation, both methods of description are sufficient to distinguish sounds with the choice between systems dependent on the phonetic feature being investigated.\n\nConsonants are speech sounds that are articulated with a complete or partial closure of the [[vocal tract]]. They are generally produced by the modification of an [[Airstream mechanism|airstream]] exhaled from the lungs. The respiratory organs used to create and modify airflow are divided into three regions: the vocal tract (supralaryngeal), the [[larynx]], and the subglottal system. The airstream can be either [[Egressive sound|egressive]] (out of the vocal tract) or [[Ingressive sound|ingressive]] (into the vocal tract). In pulmonic sounds, the airstream is produced by the lungs in the subglottal system and passes through the larynx and vocal tract. [[Glottalic consonant|Glottalic]] sounds use an airstream created by movements of the larynx without airflow from the lungs. [[Click consonant|Click]] consonants are articulated through the [[rarefaction]] of air using the tongue, followed by releasing the forward closure of the tongue.\n\nVowels are [[Syllable|syllabic]] speech sounds that are pronounced without any obstruction in the vocal tract.{{Sfn|Ladefoged|Maddieson|1996|p=281}} Unlike consonants, which usually have definite places of articulation, vowels are defined in relation to a set of reference vowels called [[cardinal vowels]]. Three properties are needed to define vowels: tongue height, tongue backness and lip roundedness. Vowels that are articulated with a stable quality are called [[monophthong]]s; a combination of two separate vowels in the same syllable is a [[diphthong]].{{Sfn|Gussenhoven|Jacobs|p=26-27|2017}} In the [[International Phonetic Alphabet|IPA]], the vowels are represented on a trapezoid shape representing the human mouth: the vertical axis representing the mouth from floor to roof and the horizontal axis represents the front-back dimension.{{Sfn|Lodge|2009|p=38}}\n\n===Transcription===\n{{Main|Phonetic transcription}}\n[[Phonetic transcription]] is a system for transcribing [[Phone (phonetics)|phones]] that occur in a language, whether [[oral language|oral]] or [[sign language|sign]]. The most widely known system of phonetic transcription, the [[International Phonetic Alphabet]] (IPA), provides a standardized set of symbols for oral phones.{{sfn|O'Grady|2005|p=17}}{{sfn|International Phonetic Association|1999}} The standardized nature of the IPA enables its users to transcribe accurately and consistently the phones of different languages, [[dialect]]s, and [[idiolect]]s.{{sfn|O'Grady|2005|p=17}}{{sfn|Ladefoged|2005}}{{sfn|Ladefoged|Maddieson|1996}} The IPA is a useful tool not only for the study of phonetics, but also for language teaching, professional acting, and [[speech pathology]].{{sfn|Ladefoged|2005}}\n\nWhile no sign language has a standardized writing system, linguists have developed their own notation systems that describe the handshape, location and movement. The [[Hamburg Notation System]] (HamNoSys) is similar to the IPA in that it allows for varying levels of detail. Some notation systems such as KOMVA and the [[Stokoe notation|Stokoe system]] were designed for use in dictionaries; they also make use of alphabetic letters in the local language for handshapes whereas HamNoSys represents the handshape directly. [[SignWriting]] aims to be an easy-to-learn writing system for sign languages, although it has not been officially adopted by any deaf community yet.{{Sfn|Baker|van den Bogarde|Pfau|p=242-244|Schermer|2016}}\n\n== Sign languages ==\nUnlike spoken languages, words in [[sign language]]s are perceived with the eyes instead of the ears. Signs are articulated with the hands, upper body and head. Various factors \u2013 such as muscle flexibility or being considered [[taboo]] \u2013 restrict what can be considered a sign.{{Sfn|Baker|van den Bogarde|Pfau|p=229-235|Schermer|2016}}\n\nThe main articulators are the hands and arms. Relative parts of the arm are described with the terms [[proximal]] and [[distal]]. Proximal refers to a part closer to the torso whereas a distal part is further away from it. For example, a wrist movement is distal compared to an elbow movement. Due to requiring less energy, distal movements are generally easier to produce.{{Sfn|Baker|van den Bogarde|Pfau|p=229-235|Schermer|2016}}\n\nUnlike spoken languages, sign languages have two identical articulators: the hands. Signers may use whichever hand they prefer with no disruption in communication. Due to universal neurological limitations, two-handed signs generally have the same kind of articulation in both hands; this is referred to as the Symmetry Condition.{{Sfn|Baker|van den Bogarde|Pfau|p=229-235|Schermer|2016}} The second universal constraint is the Dominance Condition, which holds that when two handshapes are involved, one hand will remain stationary and have a more limited set handshapes compared to the dominant, moving hand.{{Sfn|Baker|van den Bogarde|Pfau|p=286|Schermer|2016}} Additionally, it is common for one hand in a two-handed sign to be dropped during informal conversations, a process referred to as weak drop.{{Sfn|Baker|van den Bogarde|Pfau|p=229-235|Schermer|2016}}\n\nJust like words in spoken languages, coarticulation may cause signs to influence each other's form. Examples include the handshapes of neighboring signs becoming more similar to each other ([[Assimilation (phonology)|assimilation]]) or weak drop (an instance of [[Deletion (phonology)|deletion]]).{{Sfn|Baker|van den Bogarde|Pfau|p=239|Schermer|2016}}\n\nNative signers do not look at their conversation partner's hands. Instead, their eye gaze is fixated on the face. Because [[peripheral vision]] is not as focused as the center of the visual field, signs articulated near the face allow for more subtle differences in finger movement and location to be perceived.{{Sfn|Baker|van den Bogarde|Pfau|p=236|Schermer|2016}}\n\n==See also==\n* [[Motor theory of speech perception]]\n* [[Exemplar theory]]\n* [[Articulatory phonology]]\n\n== References ==\n=== Notes ===\n{{notelist}}\n\n=== Citations ===\n{{reflist}}\n\n=== Works cited ===\n{{refbegin|30em}}\n* {{cite book\n|last=Abercrombie\n|first=D.\n|year=1967\n|isbn=\n|title=Elements of General Phonetics\n|url=https://archive.org/details/elementsofgenera0000aber\n|url-access=registration\n|place=Edinburgh\n|pages=\n|ref=harv\n|publisher=Chicago, Aldine Pub. Co\n}}\n* {{cite book\n|title=Psycholinguistics : critical concepts in psychology\n|date=2002\n|publisher=Routledge\n|last=Altmann\n|first=Gerry\n|isbn=978-0415229906\n|location=London\n|oclc=48014482\n|ref=harv\n}}\n* {{cite book\n|last1=Baker\n|first1=Anne\n|last2=van den Bogaerde\n|first2=Beppie\n|last3=Pfau\n|first3=Roland \n|last4=Schermer\n|first4=Trude\n|title=The Linguistics of Sign Languages\n|date=2016\n|publisher=John Benjamins Publishing Company\n|location=Amsterdam/Philadelphia\n|isbn=978-90-272-1230-6\n|ref=harv\n}}\n* {{cite book\n|last=Baumbach\n|first=E. J. M\n|year=1987\n|isbn=\n|title=Analytical Tsonga Grammar\n|place=Pretoria\n|pages=\n|publisher=University of South Africa\n|ref=harv\n}}\n* {{cite journal\n|last=Bizzi\n|first=E.\n|last2=Hogan\n|first2=N.\n|last3=Mussa-Ivaldi\n|first3=F.\n|last4=Giszter\n|first4=S.\n|year=1992\n|title=Does the nervous system use equilibrium-point control to guide single and multiple joint movements?\n|url=\n|journal=Behavioral and Brain Sciences\n|volume=15\n|issue=4\n|pages=603\u201313\n|ref=harv\n|doi=10.1017/S0140525X00072538\n|pmid=23302290\n}}\n* {{cite book\n|title=Psycholinguistics: Critical Concepts in Psychology\n|last=Bock\n|first=Kathryn\n|last2=Levelt\n|first2=Willem\n|publisher=Routledge\n|year=2002\n|isbn=978-0-415-26701-4\n|editor-last=Atlmann\n|editor-first=Gerry\n|volume=5\n|location=New York\n|pages=405\u2013407\n|ref=harv\n}}\n* {{cite book\n|last=Boersma\n|first=Paul\n|title=Functional phonology: Formalizing the interactions between articulatory and perceptual drives\n|year=1998\n|publisher=Holland Academic Graphics\n|place=The Hague\n|oclc=40563066\n|isbn=9055690546\n|ref=harv\n}}\n* {{cite book\n|last=Caffrey\n|first=Cait\n|year=2017\n|isbn=\n|location=\n|pages=\n|chapter=Phonetics\n|title=Salem Press Encyclopedia\n|publisher=Salem Press\n|ref=harv\n}}\n* {{cite book\n|last=Catford\n|first=J. C.\n|title=A Practical Introduction to Phonetics\n|url=https://archive.org/details/practicalintrodu00catf\n|url-access=registration\n|year=2001\n|publisher=Oxford University Press\n|edition= 2nd\n|location=\n|pages=\n|isbn=978-0-19-924635-9\n|ref=harv\n}}\n* {{cite book\n|last=Chomsky\n|first=Noam\n|last2=Halle\n|first2=Morris\n|year=1968\n|isbn=\n|location=\n|pages=\n|title=Sound Pattern of English\n|publisher=Harper and Row\n|ref=harv\n}}\n* {{cite book\n|last=Cutler\n|first=Anne\n|date=2005\n|doi=10.1002/9780470757024.ch11\n|chapter=Lexical Stress\n|title=The Handbook of Speech Perception\n|pages=264\u2013289\n|editor-last=Pisoni\n|editor-first=David B.\n|editor2-last=Remez\n|editor2-first=Robert\n|chapter-url=https://repository.ubn.ru.nl/bitstream/handle/2066/56545/56545.pdf\n|publisher=Blackwell\n|isbn=978-0-631-22927-8\n|oclc=749782145\n|access-date=2019-12-29\n|ref=harv\n}}\n* {{cite book\n|editor-last=Dawson\n|editor-first=Hope\n|editor2-last=Phelan\n|editor2-first=Michael\n|year=2016\n|title=Language Files: Materials for an Introduction to Linguistics\n|last=\n|first=\n|publisher=The Ohio State University Press\n|edition= 12th\n|location=\n|pages=\n|isbn=978-0-8142-5270-3\n|ref=harv\n}}\n* {{cite journal\n|last=Dell\n|first=Gary\n|last2=O'Seaghdha\n|first2=Padraig\n|date=1992\n|title=Stages of lexical access in language production\n|url=\n|journal=Cognition\n|volume=42\n|issue=1\u20133\n|pages=287\u2013314\n|doi=10.1016/0010-0277(92)90046-k\n|pmid=1582160\n|ref=harv\n}}\n* {{cite journal\n|last=Dell\n|first=Gary\n|first2=Peter\n|last2=Reich\n|title=Stages in sentence production: An analysis of speech error data\n|journal=Journal of Memory and Language\n|volume=20\n|issue=6\n|year=1981\n|pages=611\u2013629\n|doi=10.1016/S0022-5371(81)90202-4\n|ref=harv\n}}\n* {{cite book\n|last=Doke\n|first=Clement M\n|year=1926\n|isbn=\n|title=The Phonetics of the Zulu Language\n|series=Bantu Studies\n|place=Johannesburg\n|pages=\n|publisher=Wiwatersrand University Press\n|ref=harv\n}}\n* {{cite journal\n|last=Eklund\n|first=Robert\n|year=2008\n|title=Pulmonic ingressive phonation: Diachronic and synchronic characteristics, distribution and function in animal and human sound production and in human speech\n|url=\n|journal=Journal of the International Phonetic Association\n|volume=38\n|issue=3\n|pages=235\u2013324\n|doi=10.1017/S0025100308003563\n|ref=harv\n}}\n* {{cite journal\n|last=Feldman\n|first=Anatol G.\n|year=1966\n|title=Functional tuning of the nervous system with control of movement or maintenance of a steady posture, III: Mechanographic analysis of the execution by man of the simplest motor task\n|url=\n|journal=Biophysics\n|volume=11\n|pages=565\u2013578\n|ref=harv\n|via=\n}}\n* {{cite journal\n|last=Fujimura\n|first=Osamu\n|year=1961\n|title=Bilabial stop and nasal consonants: A motion picture study and its acoustical implications\n|url=\n|journal=Journal of Speech and Hearing Research\n|volume=4\n|issue=3\n|pages=233\u201347\n|ref=harv\n|pmid=13702471\n|doi=10.1044/jshr.0403.233\n}}\n* {{cite journal\n|last1=Galantucci\n|first1=Bruno\n|first2=Carol\n|last2=Fowler\n|first3=Michael\n|last3=Turvey\n|title=The motor theory of speech perception reviewed\n|journal=Psychonomic Bulletin & Review\n|volume=13\n|issue=3\n|year=2006\n|pages=361\u2013377\n|ref=harv\n|doi=10.3758/BF03193857\n|pmid=17048719\n|pmc=2746041\n}}\n* {{cite journal\n|last=Gleitman\n|first=Lila\n|first2=David\n|last2=January\n|first3=Rebecca\n|last3=Nappa\n|first4=John\n|last4=Trueswell\n|title=On the give and take between event apprehension and utterance formulation \n|journal=Journal of Memory and Language\n|volume=57\n|issue=4\n|year=2007\n|pages=544\u2013569\n|doi=10.1016/j.jml.2007.01.007\n|pmid=18978929\n|pmc=2151743\n|ref=harv\n}}\n* {{cite book\n|first=Christer\n|last=Gobl\n|first2=Ailbhe\n|last2=N\u00ed Chasaide\n|year=2010\n|isbn=\n|chapter=Voice source variation and its communicative functions\n|title=The Handbook of Phonetic Sciences\n|edition= 2nd\n|location=\n|pages=378\u2013424\n|ref=harv\n}}\n* {{cite journal\n|last=Goldinger\n|first=Stephen\n|title=Words and voices: episodic traces in spoken word identification and recognition memory\n|journal=Journal of Experimental Psychology: Learning, Memory, and Cognition\n|volume=22\n|issue=5\n|year=1996\n|pages=1166\u201383\n|ref=harv\n|doi=10.1037/0278-7393.22.5.1166\n}}\n* {{cite journal\n|last=Gordon\n|first=Matthew\n|first2=Peter\n|last2=Ladefoged\n|year=2001\n|title=Phonation types: a cross-linguistic overview\n|url=\n|journal=Journal of Phonetics\n|volume=29\n|number=4\n|pages=383\u2013406\n|ref=harv\n|doi=10.1006/jpho.2001.0147\n}}\n* {{cite book\n|last1=Guthrie\n|first1=Malcolm\n|title=The classification of the Bantu languages\n|date=1948\n|publisher=Oxford University Press\n|isbn=\n|location=London\n|pages=\n|ref=harv\n}}\n* {{cite book \n|title=Understanding phonology \n|last=Gussenhoven \n|first=Carlos \n|last2=Jacobs \n|first2=Haike \n|date=2017 \n|publisher=Routledge \n|isbn=9781138961418 \n|edition= Fourth \n|location=London and New York \n|oclc=958066102\n|ref=harv\n}}\n* {{cite book\n|last=Hall\n|first=Tracy Alan\n|year=2001\n|isbn=\n|chapter=Introduction: Phonological representations and phonetic implementation of distinctive features\n|title=Distinctive Feature Theory\n|editor-last=Hall\n|editor-first=Tracy Alan\n|location=\n|pages=1\u201340\n|publisher=de Gruyter\n|ref=harv\n}}\n* {{cite journal\n|last=Halle\n|first=Morris\n|year=1983\n|title=On Distinctive Features and their articulatory implementation\n|url=\n|journal=Natural Language and Linguistic Theory\n|volume=1\n|number=1\n|pages=91\u2013105\n|ref=harv\n|doi=10.1007/BF00210377\n}}\n* {{cite book\n|editor1-last=Hardcastle\n|editor1-first=William\n|editor2-last=Laver\n|editor2-first=John\n|editor3-last=Gibbon\n|editor3-first=Fiona\n|year=2010\n|title=The Handbook of Phonetic Sciences\n|last=\n|first=\n|edition= 2nd\n|location=\n|pages=\n|publisher=Wiley-Blackwell\n|isbn=978-1-405-14590-9\n|ref=harv\n}}\n* {{cite book\n|author=International Phonetic Association\n|year=1999\n|isbn=\n|location=\n|pages=\n|title=Handbook of the International Phonetic Association\n|publisher=Cambridge University Press\n|ref=harv\n}}\n* {{cite book\n|author=International Phonetic Association\n|year=2015\n|isbn=\n|location=\n|pages=\n|title=International Phonetic Alphabet\n|publisher=International Phonetic Association\n|ref=harv\n}}\n* {{cite journal\n|last=Jaeger\n|first=Florian\n|first2=Katrina\n|last2=Furth\n|first3=Caitlin\n|last3=Hilliard\n|title=Phonological overlap affects lexical selection during sentence production\n|journal=Journal of Experimental Psychology: Learning, Memory, and Cognition\n|volume=38\n|issue=5\n|year=2012\n|pages=1439\u20131449\n|doi=10.1037/a0027862\n|pmid=22468803\n|ref=harv\n}}\n* {{cite book\n|last1=Jakobson\n|first1=Roman\n|first2=Gunnar\n|last2=Fant\n|first3=Morris\n|last3=Halle\n|year=1976\n|title=Preliminaries to Speech Analysis: The Distinctive Features and their Correlates\n|publisher=MIT Press\n|isbn=978-0-262-60001-9\n|location=\n|pages=\n|ref={{sfnref|Jakobson, Fant, and Halle|1976}}\n}}\n* {{cite book \n|title=Acoustic and auditory phonetics \n|last=Johnson \n|first=Keith \n|date=2003 \n|publisher=Blackwell Pub \n|isbn=1405101229 \n|edition= 2nd \n|oclc=50198698\n|ref=harv\n}}\n* {{cite book\n|last=Johnson\n|first=Keith\n|year=2011\n|title=Acoustic and Auditory Phonetics\n|edition= 3rd\n|location=\n|pages=\n|publisher=Wiley-Blackwell\n|isbn=978-1-444-34308-3\n|ref=harv\n}}\n* {{cite journal\n|last=Jones\n|first=Daniel\n|year=1948\n|title=The London school of phonetics\n|url=\n|journal=Zeitschrift f\u00fcr Phonetik\n|volume=11\n|number=3/4\n|pages=127\u2013135\n|ref=harv\n|via=}} (Reprinted in {{cite book|editor1-first=W. E.|editor1-last=Jones|editor2-first=J.|editor2-last=Laver|title=Phonetics in Linguistics|last=|first=|publisher=Longman|year=1973|isbn=|location=|pages=180\u2013186|ref=harv}})\n* {{cite journal\n|last=Keating\n|first=Patricia\n|last2=Lahiri\n|first2=Aditi\n|year=1993\n|title=Fronted Velars, Palatalized Velars, and Palatals\n|url=\n|journal=Phonetica\n|volume=50\n|issue=2\n|pages=73\u2013101\n|doi=10.1159/000261928\n|pmid=8316582\n|ref=harv\n}}\n* {{cite book\n|last=Kingston\n|first=John\n|year=2007\n|pages=\n|chapter=The Phonetics-Phonology Interface\n|title=The Cambridge Handbook of Phonology\n|editor-first=Paul\n|location=\n|editor-last=DeLacy\n|publisher=Cambridge University Press\n|isbn=978-0-521-84879-4\n|ref=harv\n}}\n* {{cite book\n|last=Kiparsky\n|first=Paul\n|year=1993\n|isbn=\n|chapter=P\u0101\u1e47inian linguistics\n|title=Encyclopedia of Languages and Linguistics\n|editor-last=Asher\n|editor-first=R.E.\n|place=Oxford\n|pages=\n|publisher=Pergamon\n|ref=harv\n}}\n* {{cite journal\n|last=Ladefoged\n|first=Peter\n|year=1960\n|title=The Value of Phonetic Statements\n|url=\n|journal=Language\n|volume=36\n|number=3\n|pages=387\u201396\n|jstor=410966\n|ref=harv\n|doi=10.2307/410966\n}}\n* {{cite book\n|last=Ladefoged\n|first=Peter\n|year=2001\n|title=A Course in Phonetics\n|place=Boston\n|pages=\n|publisher=[[Thomson/Wadsworth]]\n|edition=4th\n|isbn=978-1-413-00688-9\n|ref=harv\n|url=https://archive.org/details/courseinphonetic00lade_0\n}}\n* {{cite book\n|last=Ladefoged\n|first=Peter\n|year=2005\n|title=A Course in Phonetics\n|place=Boston\n|pages=\n|publisher=[[Thomson/Wadsworth]]\n|edition=5th\n|isbn=978-1-413-00688-9\n|ref=harv\n|url=https://archive.org/details/courseinphonetic00lade_0\n}}\n* {{cite book\n|last1=Ladefoged\n|first1=Peter\n|authorlink1=Peter Ladefoged\n|last2=Johnson\n|first2=Keith\n|year=2011\n|title=A Course in Phonetics\n|edition= 6th\n|publisher=Wadsworth\n|isbn=978-1-42823126-9\n|ref=harv\n}}\n* {{cite book\n|last=Ladefoged\n|first=Peter\n|first2=Ian\n|last2=Maddieson\n|year=1996\n|title=The Sounds of the World's Languages\n|place=Oxford\n|pages=\n|publisher=Blackwell\n|isbn=978-0-631-19815-4\n|ref=harv\n}}\n* {{cite journal\n|last=Levelt\n|first=Willem\n|title=A theory of lexical access in speech production\n|journal=Behavioral and Brain Sciences\n|volume=22\n|issue=1\n|pages=3\u20136\n| pmid = 11301520 \n| doi=10.1017/s0140525x99001776\n|year=1999\n|hdl=11858/00-001M-0000-0013-3E7A-A\n|ref=harv\n}}\n* {{cite book\n|title=A Critical Introduction to Phonetics\n|last=Lodge\n|first=Ken\n|publisher=Continuum International Publishing Group\n|year=2009\n|isbn=978-0-8264-8873-2\n|location=New York\n|pages=\n|ref=harv\n}}\n* {{cite book\n|last=L\u00f6fqvist\n|first=Anders\n|year=2010\n|isbn=\n|chapter=Theories and Models of Speech Production\n|title=Handbook of Phonetic Sciences\n|edition= 2nd\n|location=\n|pages=353\u201378\n|ref=harv\n}}\n* {{cite journal\n|last=Maddieson\n|first=Ian\n|year=1993\n|title=Investigating Ewe articulations with electromagnetic articulography\n|url=\n|journal=Forschungberichte des Intituts f\u00fcr Phonetik und Sprachliche Kommunikation der Universit\u00e4t M\u00fcnchen\n|volume=31\n|pages=181\u2013214\n|ref=harv\n|via=\n}}\n* {{cite book\n|last=Maddieson\n|first=Ian\n|year=2013\n|isbn=\n|chapter=Uvular Consonants\n|title=The World Atlas of Language Structures Online\n|editor1-last=Dryer\n|editor1-first=Matthew S.\n|editor2-last=Haspelmath\n|editor2-first=Martin\n|place=Leipzig\n|pages=\n|publisher=Max Planck Institute for Evolutionary Anthropology\n|chapter-url=http://wals.info/chapter/6\n|ref=harv\n|title-link=World Atlas of Language Structures\n}}\n* {{cite journal\n|last=Mattingly\n|first=Ignatius\n|year=1990\n|title=The global character of phonetic gestures\n|journal=Journal of Phonetics\n|volume=18\n|issue=3\n|pages=445\u201352\n|url=http://www.haskins.yale.edu/Reprints/HL0739.pdf\n|ref=harv\n|via=\n|doi=10.1016/S0095-4470(19)30372-9\n}}\n* {{cite journal\n|last=Motley\n|first=Michael\n|first2=Carl\n|last2=Camden\n|first3=Bernard\n|last3=Baars\n|title=Covert formulation and editing of anomalies in speech production: Evidence from experimentally elicited slips of the tongue\n|journal=Journal of Verbal Learning and Verbal Behavior\n|volume=21\n|number=5\n|year=1982\n|pages=578\u2013594\n|doi=10.1016/S0022-5371(82)90791-5\n|ref=harv\n}}\n* {{cite journal\n|last1=Munhall\n|first1=K.\n|last2=Ostry\n|first2=D\n|last3=Flanagan\n|first3=J.\n|year=1991\n|title=Coordinate spaces in speech planning\n|url=\n|journal=Journal of Phonetics\n|volume=19\n|issue=3\u20134\n|pages=293\u2013307\n|ref=harv\n|doi=10.1016/S0095-4470(19)30346-8\n}}\n* {{cite book\n|last1=O'Connor\n|first1=J.D.\n|title=Phonetics\n|date=1973\n|publisher=Pelican\n|isbn=978-0140215601\n|location=\n|pages=16\u201317\n|ref=harv\n}}\n* {{cite book\n|last=O'Grady\n|first=William\n|title=Contemporary Linguistics: An Introduction\n|edition= 5th\n|location=\n|pages=\n|publisher=Bedford/St. Martin's\n|year=2005\n|isbn=978-0-312-41936-3\n|ref=harv\n}}\n* {{cite journal\n|last1=Ohala\n|first1=John\n|year=1997\n|title=Aerodynamics of phonology\n|journal=Proceedings of the Seoul Internation Conference on Linguistics\n|volume=92\n|pages=\n|url=https://www.researchgate.net/publication/242446947\n|ref=harv\n|via=\n}}\n* {{cite book\n|location=\n|pages=\n|chapter=Phonetics, n.\n|title=Oxford English Dictionary Online\n|last=\n|first=\n|year=2018\n|isbn=\n|publisher=Oxford University Press\n|ref={{sfnref|Oxford English dictionary|2018}}\n}} <!-- No author so ordered by title-->\n* {{cite web\n|last=Roach\n|first=Peter\n|year=n.d.\n|title=Practical Phonetic Training\n|website=\n|publisher=Peter Roach\n|url=https://www.peterroach.net/practical-phonetic-training.html\n|archive-url=\n|archive-date=\n|access-date=10 May 2019\n|ref={{sfnref|Roach|n.d.}}\n}}\n* {{cite journal\n|last=Saltzman\n|first=Elliot\n|last2=Munhall\n|first2=Kevin\n|year=1989\n|title=Dynamical Approach to Gestural Patterning in Speech Production\n|journal=Ecological Psychology\n|volume=1\n|number=4\n|pages=333\u201382\n|url=http://www.haskins.yale.edu/sr/SR099/SR099_03.pdf\n|ref=harv\n|via=\n|doi=10.1207/s15326969eco0104_2\n}}\n* {{cite book\n|last=Scatton\n|first=Ernest\n|year=1984\n|title=A reference grammar of modern Bulgarian\n|publisher=Slavica\n|isbn=978-0893571238\n|location=\n|pages=\n|ref=harv\n}}\n* {{cite book\n |last=Schacter\n |first=Daniel\n |last2=Gilbert\n |first2=Daniel\n |last3=Wegner\n |first3=Daniel\n | chapter = Sensation and Perception\n | editor = Charles Linsmeiser\n | title = Psychology\n | chapter-url = https://archive.org/details/psychology0000scha\n | chapter-url-access = registration\n | publisher = Worth Publishers\n | year = 2011\n | isbn = 978-1-4292-3719-2\n | ref=harv\n}}\n* {{cite journal\n|last=Schiller\n|first=Niels\n|last2=Bles\n|first2=Mart\n|last3=Jansma\n|first3=Bernadette\n|date=2003\n|title=Tracking the time course of phonological encoding in speech production: an event-related brain potential study\n|url=\n|journal=Cognitive Brain Research\n|volume=17\n|issue=3\n|pages=819\u2013831\n|doi=10.1016/s0926-6410(03)00204-0\n|pmid=14561465\n|ref=harv\n}}\n* {{cite book\n|last=Sedivy\n|first=Julie\n|year=2019\n|title=Language in Mind: An Introduction to Psycholinguistics\n|edition=2nd\n|isbn=978-1605357058\n|ref=harv\n}}\n* {{cite book\n|last=Seikel\n|first=J. Anthony\n|last2=Drumright\n|first2=David\n|last3=King\n|first3=Douglas\n|year=2016\n|title=Anatomy and Physiology for Speech, Language, and Hearing\n|edition= 5th\n|location=\n|pages=\n|publisher=Cengage\n|isbn=978-1-285-19824-8\n|ref=harv\n}}\n* {{cite journal\n|last1=Skipper\n|first1=Jeremy\n|first2=Joseph\n|last2=Devlin\n|first3=Daniel\n|last3=Lametti\n|title=The hearing ear is always found close to the speaking tongue: Review of the role of the motor system in speech perception\n|journal=Brain and Language\n|volume=164\n|year=2017\n|pages=77\u2013105\n|ref=harv\n|doi=10.1016/j.bandl.2016.10.004\n|pmid=27821280\n}}\n* {{cite book\n|last=Stearns\n|first=Peter\n|title=World Civilizations\n|url=https://archive.org/details/worldcivilizatio00stea_1\n|url-access=registration\n|year=2001\n|publisher=Longman\n|location=New York\n|pages=\n|isbn=978-0-321-04479-2\n|edition= 3rd\n|author2=Adas, Michael\n|author3=Schwartz, Stuart\n|author4=Gilbert, Marc Jason\n|ref=harv\n}}\n* {{cite book\n|last=Trask\n|first=R.L.\n|authorlink=Larry Trask\n|year=1996\n|title=A Dictionary of Phonetics and Phonology\n|place=Abingdon\n|pages=\n|publisher=Routledge\n|isbn=978-0-415-11261-1\n|ref=harv}}\n* {{cite book\n | last = Yost\n | first = William\n | chapter = Audition\n |editor1=Alice F. Healy |editor2=Robert W. Proctor | title = Handbook of Psychology: Experimental psychology\n | publisher = John Wiley and Sons\n | year = 2003\n | isbn = 978-0-471-39262-0\n | page = 130\n | chapter-url = https://books.google.com/books?id=sPkIn4sUyXEC&pg=PA130\n | ref=harv\n}}\n{{refend}}\n\n==External links==\n{{NSRW Poster}}\n*[https://users.castle.unc.edu/~jlsmith/phonetics-resources.html Collection of phonetics resources] by the University of North Carolina\n*[https://www.peterroach.net/uploads/3/6/5/8/3658625/english-phonetics-and-phonology4-glossary.pdf \"A Little Encyclopedia of Phonetics\"] by [[Peter Roach (phonetician)|Peter Roach]].\n*[https://dood.al/pinktrombone/ Pink Trombone], an interactive articulation simulator by Neil Thapen.\n\n{{Authority control}}\n\n[[Category:Phonetics| ]]\n", "name_user": "Megaman en m", "label": "safe", "comment": "\u2192\u200ePerception:changed image description, don't refer to the reader", "url_page": "//en.wikipedia.org/wiki/Phonetics"}
